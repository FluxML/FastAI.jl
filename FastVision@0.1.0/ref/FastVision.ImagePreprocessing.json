{"attributes":{"kind":"struct","backlinks":[{"tag":"documentation","title":"FastVision","docid":"FastVision@0.1.0/ref/FastVision"},{"tag":"document","title":"Saving and loading models for inference","docid":"FastAI@dev/doc/docs/notebooks/serialization.ipynb"},{"tag":"sourcefile","title":"FastVision/encodings/imagepreprocessing.jl","docid":"FastVision@0.1.0/src/encodings/imagepreprocessing.jl"},{"tag":"sourcefile","title":"FastVision/tasks/segmentation.jl","docid":"FastVision@0.1.0/src/tasks/segmentation.jl"},{"tag":"document","title":"Introduction","docid":"FastAI@dev/doc/docs/introduction.md"},{"tag":"documentation","title":"showblockinterpretable","docid":"FastAI@dev/ref/FastAI.showblockinterpretable"},{"tag":"document","title":"fastai API comparison","docid":"FastAI@dev/doc/docs/fastai_api_comparison.md"},{"tag":"document","title":"Keypoint regression","docid":"FastAI@dev/doc/docs/notebooks/keypointregression.ipynb"},{"tag":"sourcefile","title":"FastVision/tasks/utils.jl","docid":"FastVision@0.1.0/src/tasks/utils.jl"},{"tag":"document","title":"Image segmentation","docid":"FastAI@dev/doc/docs/notebooks/imagesegmentation.ipynb"},{"tag":"documentation","title":"ImageSegmentation","docid":"FastVision@0.1.0/ref/FastVision.ImageSegmentation"},{"tag":"document","title":"Siamese image similarity","docid":"FastAI@dev/doc/docs/notebooks/siamese.ipynb"},{"tag":"document","title":"Blocks and encodings","docid":"FastAI@dev/doc/docs/background/blocksencodings.md"},{"tag":"document","title":"Variational autoencoders","docid":"FastAI@dev/doc/docs/notebooks/vae.ipynb"},{"tag":"documentation","title":"AbstractBlockTask","docid":"FastAI@dev/ref/FastAI.AbstractBlockTask"},{"tag":"document","title":"How to augment vision data","docid":"FastAI@dev/doc/docs/howto/augmentvision.md"},{"tag":"documentation","title":"ImageClassificationMulti","docid":"FastVision@0.1.0/ref/FastVision.ImageClassificationMulti"},{"tag":"documentation","title":"ImageClassificationSingle","docid":"FastVision@0.1.0/ref/FastVision.ImageClassificationSingle"},{"tag":"documentation","title":"setup","docid":"FastAI@dev/ref/FastAI.setup"},{"tag":"sourcefile","title":"FastVision/FastVision.jl","docid":"FastVision@0.1.0/src/FastVision.jl"},{"tag":"sourcefile","title":"FastVision/tests.jl","docid":"FastVision@0.1.0/src/tests.jl"}],"methods":[{"symbol_id":"FastVision.ImagePreprocessing","module_id":"FastVision","file":"encodings/imagepreprocessing.jl","line":54,"signature":"(::Signature)"}],"package_id":"FastVision@0.1.0","title":"ImagePreprocessing","symbol_id":"FastVision.ImagePreprocessing","exported":true,"module_id":"FastVision"},"tag":"documentation","children":[{"attributes":{"symbol":"FastVision.ImagePreprocessing","line":26,"module":"FastVision","file":"encodings/imagepreprocessing.jl"},"tag":"docstring","children":[{"attributes":{},"tag":"md","children":[{"attributes":{"lang":""},"tag":"codeblock","children":["ImagePreprocessing([; kwargs...]) <: Encoding\n"],"type":"node"},{"attributes":{},"tag":"p","children":["Encodes ",{"attributes":{},"tag":"code","children":["Image"],"type":"node"},"s by converting them to a common color type ",{"attributes":{},"tag":"code","children":["C"],"type":"node"},", expanding the color channels and normalizing the channel values. Additionally, apply pixel-level augmentations passed in as ",{"attributes":{},"tag":"code","children":["augmentations"],"type":"node"}," during ",{"attributes":{},"tag":"code","children":["Training"],"type":"node"},"."],"type":"node"},{"attributes":{},"tag":"p","children":["Encodes"],"type":"node"},{"attributes":{},"tag":"ul","children":[{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"code","children":["Image{N}"],"type":"node"}," -> ",{"attributes":{},"tag":"code","children":["ImageTensor{N}"],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"},{"attributes":{},"tag":"h2","children":["Keyword arguments"],"type":"node"},{"attributes":{},"tag":"ul","children":[{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"code","children":["augmentations::"],"type":"node"},{"attributes":{"reftype":"symbol","href":"#","title":"","document_id":"DataAugmentation@0.2.10/ref/DataAugmentation.Transform"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["DataAugmentation.Transform"],"type":"node"}],"type":"node"},": Augmentation to apply to every image before preprocessing. See ",{"attributes":{"reftype":"symbol","href":"#","title":"","document_id":"FastVision@0.1.0/ref/FastVision.augs_lighting"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["augs_lighting"],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"},{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"code","children":["buffered = true"],"type":"node"},": Whether to use inplace transformations. Reduces memory usage."],"type":"node"}],"type":"node"},{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"code","children":["means::SVector = IMAGENET_MEANS"],"type":"node"},": mean value of each color channel."],"type":"node"}],"type":"node"},{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"code","children":["stds::SVector = IMAGENET_STDS"],"type":"node"},": standard deviation of each color channel."],"type":"node"}],"type":"node"},{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"code","children":["C::Type{<:Colorant} = RGB{N0f8}"],"type":"node"},": color type to convert images to."],"type":"node"}],"type":"node"},{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"code","children":["T::Type{<:Real} = Float32"],"type":"node"},": element type of output"],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}