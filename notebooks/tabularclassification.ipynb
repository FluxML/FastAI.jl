{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"/Users/manikyabardhan/.julia/dev/FastAI\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tabular Classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tabular Classification involves having a categorical column as the target. Here, we'll use the adult sample dataset from fastai and try to predict whether the salary is above 50K or not, making this a binary classification task. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "using Flux\n",
    "using FastAI\n",
    "using FastAI.Datasets\n",
    "using Tables\n",
    "using Statistics\n",
    "using FluxTraining\n",
    "using DataAugmentation"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "┌ Info: Precompiling FastAI [5d0beca9-ade8-49ae-ad0b-a3cf890e669f]\n",
      "└ @ Base loading.jl:1317\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can quickly download and get the path of any dataset from fastai by using `datasetpath`. Once we have the path, we'll load the data in a `TableContainer`. By default, if we pass in just the path to `TableContainer`, the data is loaded in a `DataFrame`, but we can use any package for accessing our data, and pass an object satisfying the Tables.jl interface to it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "data = TableDataset(joinpath(datasetpath(\"adult_sample\"), \"adult.csv\"))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TableDataset{DataFrames.DataFrame}(\u001b[1m32561×15 DataFrame\u001b[0m\n",
       "\u001b[1m   Row \u001b[0m│\u001b[1m age   \u001b[0m\u001b[1m workclass         \u001b[0m\u001b[1m fnlwgt \u001b[0m\u001b[1m education     \u001b[0m\u001b[1m education-num \u001b[0m\u001b[1m marit\u001b[0m ⋯\n",
       "\u001b[1m       \u001b[0m│\u001b[90m Int64 \u001b[0m\u001b[90m String            \u001b[0m\u001b[90m Int64  \u001b[0m\u001b[90m String        \u001b[0m\u001b[90m Float64?      \u001b[0m\u001b[90m Strin\u001b[0m ⋯\n",
       "───────┼────────────────────────────────────────────────────────────────────────\n",
       "     1 │    49   Private           101320   Assoc-acdm             12.0   Marr ⋯\n",
       "     2 │    44   Private           236746   Masters                14.0   Divo\n",
       "     3 │    38   Private            96185   HS-grad      \u001b[90m     missing   \u001b[0m  Divo\n",
       "     4 │    38   Self-emp-inc      112847   Prof-school            15.0   Marr\n",
       "     5 │    42   Self-emp-not-inc   82297   7th-8th      \u001b[90m     missing   \u001b[0m  Marr ⋯\n",
       "     6 │    20   Private            63210   HS-grad                 9.0   Neve\n",
       "     7 │    49   Private            44434   Some-college           10.0   Divo\n",
       "     8 │    37   Private           138940   11th                    7.0   Marr\n",
       "     9 │    46   Private           328216   HS-grad                 9.0   Marr ⋯\n",
       "    10 │    36   Self-emp-inc      216711   HS-grad      \u001b[90m     missing   \u001b[0m  Marr\n",
       "    11 │    23   Private           529223   Bachelors              13.0   Neve\n",
       "   ⋮   │   ⋮            ⋮            ⋮           ⋮              ⋮              ⋱\n",
       " 32552 │    60   Private           230545   7th-8th                 4.0   Divo\n",
       " 32553 │    39   Private           139743   HS-grad                 9.0   Sepa ⋯\n",
       " 32554 │    35   Self-emp-inc      135436   Prof-school            15.0   Marr\n",
       " 32555 │    53   Private            35102   Some-college           10.0   Divo\n",
       " 32556 │    48   Private           355320   Bachelors              13.0   Marr\n",
       " 32557 │    36   Private           297449   Bachelors              13.0   Divo ⋯\n",
       " 32558 │    23   ?                 123983   Bachelors              13.0   Neve\n",
       " 32559 │    53   Private           157069   Assoc-acdm             12.0   Marr\n",
       " 32560 │    32   Local-gov         217296   HS-grad                 9.0   Marr\n",
       " 32561 │    26   Private           182308   Some-college           10.0   Marr ⋯\n",
       "\u001b[36m                                               10 columns and 32540 rows omitted\u001b[0m)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In case our data was present in a different format for eg. parquet, it could be loaded in a TableContainer as shown below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "using Parquet\n",
    "TableDataset(read_parquet(parquet_path));"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`mapobs` is used here to split our target column from the rest of the row in a lazy manner."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "splitdata = mapobs(row -> (row, row[:salary]), data);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To create a learning method for tabular classification task, we need an input block, an output block, and the encodings to be performed on the data.\n",
    "\n",
    "The input block here is a `TableRow` which contains information about the nature of the columns (ie. categorical or continuous) along with an indexable collection mapping categorical column names to a collection with distinct classes in that column. We can get this mapping by using the `gettransformationdict` method with `DataAugmentation.Categorify`.\n",
    "\n",
    "The outblock block used is `Label` for single column classification and the unique classes have to passed to it.\n",
    "\n",
    "This is followed by the encodings which needs to be applied on our input and output blocks. For the input block, we have used the `gettransforms` function here to get a standard bunch of transformations to apply, but this can be easily customized by passing in any tabular transformation from DataAugmentation.jl or a composition of those, to `TabularTransforms`. In addition to this, we have just one-hot encoded the outblock."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "cat, cont = FastAI.getcoltypes(data)\n",
    "target = :salary\n",
    "cat = filter(!isequal(target), cat)\n",
    "catdict = FastAI.gettransformationdict(data, DataAugmentation.Categorify, cat);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "method = BlockMethod(\n",
    "    (\n",
    "        TableRow(cat, cont, catdict), \n",
    "        Label(unique(data.table[:, target]))\n",
    "    ),\n",
    "    ((FastAI.TabularTransform(FastAI.gettransforms(data))), FastAI.OneHot())\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "┌ Warning: There is a missing value present for category 'occupation' which will be removed from Categorify dict\n",
      "└ @ DataAugmentation /Users/manikyabardhan/.julia/dev/DataAugmentation/src/rowtransforms.jl:108\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BlockMethod(TableRow{8, 6} -> Label{String})"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In case our initial problem wasn't a classification task, and we had a continuous target column, we would need to perform tabular regression. To create a learning method suitable for regression, we use a `Continuous` block for representing our target column. This can be done even with multiple continuous target columns by just passing the number of columns in `Continuous`. For example, the method here could be used for 3 targets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "method2 = BlockMethod(\n",
    "    (\n",
    "        TableRow(cat, cont, catdict), \n",
    "        Continuous(3)\n",
    "    ),\n",
    "    ((FastAI.TabularTransform(FastAI.gettransforms(data)),)),\n",
    "    outputblock = Continuous(3)\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "┌ Warning: There is a missing value present for category 'occupation' which will be removed from Categorify dict\n",
      "└ @ DataAugmentation /Users/manikyabardhan/.julia/dev/DataAugmentation/src/rowtransforms.jl:108\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BlockMethod(TableRow{8, 6} -> Continuous)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get an overview of the learning method created, and as a sanity test, we can use the `describemethod` function. This shows us what encodings will be applied to which blocks, and how the predicted ŷ values are decoded."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "describemethod(method)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\u001b[1m  \u001b[36mLearningMethod\u001b[39m summary\u001b[22m\n",
       "\u001b[1m  ------------------------\u001b[22m\n",
       "\n",
       "    •  Task: \u001b[36mTableRow{8, 6} -> Label{String}\u001b[39m\n",
       "\n",
       "    •  Model blocks: \u001b[36mFastAI.EncodedTableRow{8, 6} ->\n",
       "       FastAI.OneHotTensor{0, String}\u001b[39m\n",
       "\n",
       "  Encoding a sample (\u001b[36mencode(method, context, sample)\u001b[39m)\n",
       "\n",
       "          Encoding            Name             \u001b[36mmethod.blocks[1]\u001b[39m               \u001b[36mmethod.blocks[2]\u001b[39m\n",
       "  –––––––––––––––– ––––––––––––––– –––––––––––––––––––––––––––– ––––––––––––––––––––––––––––––\n",
       "                   \u001b[36m(input, target)\u001b[39m               \u001b[36mTableRow{8, 6}\u001b[39m                  \u001b[36mLabel{String}\u001b[39m\n",
       "  \u001b[36mTabularTransform\u001b[39m                 \u001b[1m\u001b[36mFastAI.EncodedTableRow{8, 6}\u001b[39m\u001b[22m                  \u001b[36mLabel{String}\u001b[39m\n",
       "            \u001b[36mOneHot\u001b[39m          \u001b[36m(x, y)\u001b[39m \u001b[36mFastAI.EncodedTableRow{8, 6}\u001b[39m \u001b[1m\u001b[36mFastAI.OneHotTensor{0, String}\u001b[39m\u001b[22m\n",
       "\n",
       "  Decoding a model output (\u001b[36mdecode(method, context, ŷ)\u001b[39m)\n",
       "\n",
       "          Decoding        Name             \u001b[36mmethod.outputblock\u001b[39m\n",
       "  –––––––––––––––– ––––––––––– ––––––––––––––––––––––––––––––\n",
       "                            \u001b[36mŷ\u001b[39m \u001b[36mFastAI.OneHotTensor{0, String}\u001b[39m\n",
       "            \u001b[36mOneHot\u001b[39m                              \u001b[1m\u001b[36mLabel{String}\u001b[39m\u001b[22m\n",
       "  \u001b[36mTabularTransform\u001b[39m \u001b[36mtarget_pred\u001b[39m                  \u001b[36mLabel{String}\u001b[39m"
      ],
      "text/markdown": [
       "#### `LearningMethod` summary\n",
       "\n",
       "  * Task: `TableRow{8, 6} -> Label{String}`\n",
       "  * Model blocks: `FastAI.EncodedTableRow{8, 6} -> FastAI.OneHotTensor{0, String}`\n",
       "\n",
       "Encoding a sample (`encode(method, context, sample)`)\n",
       "\n",
       "|           Encoding |              Name |                 `method.blocks[1]` |                   `method.blocks[2]` |\n",
       "| ------------------:| -----------------:| ----------------------------------:| ------------------------------------:|\n",
       "|                    | `(input, target)` |                   `TableRow{8, 6}` |                      `Label{String}` |\n",
       "| `TabularTransform` |                   | **`FastAI.EncodedTableRow{8, 6}`** |                      `Label{String}` |\n",
       "|           `OneHot` |          `(x, y)` |     `FastAI.EncodedTableRow{8, 6}` | **`FastAI.OneHotTensor{0, String}`** |\n",
       "\n",
       "Decoding a model output (`decode(method, context, ŷ)`)\n",
       "\n",
       "|           Decoding |          Name |             `method.outputblock` |\n",
       "| ------------------:| -------------:| --------------------------------:|\n",
       "|                    |          `ŷ` | `FastAI.OneHotTensor{0, String}` |\n",
       "|           `OneHot` |               |              **`Label{String}`** |\n",
       "| `TabularTransform` | `target_pred` |                  `Label{String}` |\n"
      ],
      "text/latex": [
       "\\paragraph{\\texttt{LearningMethod} summary}\n",
       "\\begin{itemize}\n",
       "\\item Task: \\texttt{TableRow\\{8, 6\\} -> Label\\{String\\}}\n",
       "\n",
       "\n",
       "\\item Model blocks: \\texttt{FastAI.EncodedTableRow\\{8, 6\\} -> FastAI.OneHotTensor\\{0, String\\}}\n",
       "\n",
       "\\end{itemize}\n",
       "Encoding a sample (\\texttt{encode(method, context, sample)})\n",
       "\n",
       "\\begin{tabular}\n",
       "{r | r | r | r}\n",
       "Encoding & Name & \\texttt{method.blocks[1]} & \\texttt{method.blocks[2]} \\\\\n",
       "\\hline\n",
       " & \\texttt{(input, target)} & \\texttt{TableRow\\{8, 6\\}} & \\texttt{Label\\{String\\}} \\\\\n",
       "\\texttt{TabularTransform} &  & \\textbf{\\texttt{FastAI.EncodedTableRow\\{8, 6\\}}} & \\texttt{Label\\{String\\}} \\\\\n",
       "\\texttt{OneHot} & \\texttt{(x, y)} & \\texttt{FastAI.EncodedTableRow\\{8, 6\\}} & \\textbf{\\texttt{FastAI.OneHotTensor\\{0, String\\}}} \\\\\n",
       "\\end{tabular}\n",
       "Decoding a model output (\\texttt{decode(method, context, ŷ)})\n",
       "\n",
       "\\begin{tabular}\n",
       "{r | r | r}\n",
       "Decoding & Name & \\texttt{method.outputblock} \\\\\n",
       "\\hline\n",
       " & \\texttt{ŷ} & \\texttt{FastAI.OneHotTensor\\{0, String\\}} \\\\\n",
       "\\texttt{OneHot} &  & \\textbf{\\texttt{Label\\{String\\}}} \\\\\n",
       "\\texttt{TabularTransform} & \\texttt{target\\_pred} & \\texttt{Label\\{String\\}} \\\\\n",
       "\\end{tabular}\n"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`getobs` gets us a row of data from the `TableContainer`, which we encode here. This gives us a tuple with the input and target. The input here is again a tuple, containing the categorical values (which have been label encoded or \"categorified\") and the continuous values (which have been normalized and any missing values have been filled). "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "x=encode(method, Training(), getobs(splitdata, 1000))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(([5, 16, 2, 10, 5, 2, 3, 2], [1.6435221651965317, -0.2567538819371021, -2.751580937680526, -0.14591824281680102, -0.21665620002803673, -0.035428902921319616]), Float32[0.0, 1.0])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To quickly get a model suitable for our learning method, we can use the `methodmodel` function. The second argument here is a Dict which can be used to pass in any custom backbones if needed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "model = methodmodel(method, Dict())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Parallel(\n",
       "    vcat,\n",
       "    Chain(\n",
       "      FastAI.Models.var\"#42#44\"(),\n",
       "      Parallel(\n",
       "        vcat,\n",
       "        Embedding(10, 5),               \u001b[90m# 50 parameters\u001b[39m\n",
       "        Embedding(17, 8),               \u001b[90m# 136 parameters\u001b[39m\n",
       "        Embedding(8, 5),                \u001b[90m# 40 parameters\u001b[39m\n",
       "        Embedding(17, 8),               \u001b[90m# 136 parameters\u001b[39m\n",
       "        Embedding(7, 4),                \u001b[90m# 28 parameters\u001b[39m\n",
       "        Embedding(6, 4),                \u001b[90m# 24 parameters\u001b[39m\n",
       "        Embedding(3, 2),                \u001b[90m# 6 parameters\u001b[39m\n",
       "        Embedding(43, 13),              \u001b[90m# 559 parameters\u001b[39m\n",
       "      ),\n",
       "      identity,\n",
       "    ),\n",
       "    BatchNorm(6),                       \u001b[90m# 12 parameters\u001b[39m\u001b[90m, plus 12\u001b[39m\n",
       "  ),\n",
       "  Chain(\n",
       "    Dense(55, 200, relu; bias=false),   \u001b[90m# 11_000 parameters\u001b[39m\n",
       "    BatchNorm(200),                     \u001b[90m# 400 parameters\u001b[39m\u001b[90m, plus 400\u001b[39m\n",
       "    identity,\n",
       "  ),\n",
       "  Chain(\n",
       "    Dense(200, 100, relu; bias=false),  \u001b[90m# 20_000 parameters\u001b[39m\n",
       "    BatchNorm(100),                     \u001b[90m# 200 parameters\u001b[39m\u001b[90m, plus 200\u001b[39m\n",
       "    identity,\n",
       "  ),\n",
       "  Dense(100, 2),                        \u001b[90m# 202 parameters\u001b[39m\n",
       ")\u001b[90m                   # Total: 18 arrays, \u001b[39m32_793 parameters, 127.828 KiB."
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is really simple to create a custom backbone using the functions present in `FastAI.Models`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "cardict = Dict(col => length(classes) for (col, classes) in collect(catdict))\n",
    "embedszs = FastAI.Models.get_emb_sz(cardict, cat)\n",
    "catback = FastAI.Models.tabular_embedding_backbone(embedszs, 0.2);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The backbone Dict can take three kinds of backbones-\n",
    "- :categoricalbackbone\n",
    "- :continuousbackbone\n",
    "- :finalclassifier\n",
    "\n",
    "We can choose to pass in any combination of these in the `methodmodel` function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "backbone = Dict(:categoricalbackbone => catback)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dict{Symbol, Chain{Tuple{FastAI.Models.var\"#42#44\", Parallel{typeof(vcat), Vector{Flux.Embedding{Matrix{Float32}}}}, Dropout{Float64, Colon}}}} with 1 entry:\n",
       "  :categoricalbackbone => Chain(#42, Parallel(vcat, Embedding(10, 5), Embedding…"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "model = methodmodel(method, backbone)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Parallel(\n",
       "    vcat,\n",
       "    Chain(\n",
       "      FastAI.Models.var\"#42#44\"(),\n",
       "      Parallel(\n",
       "        vcat,\n",
       "        Embedding(10, 5),               \u001b[90m# 50 parameters\u001b[39m\n",
       "        Embedding(17, 8),               \u001b[90m# 136 parameters\u001b[39m\n",
       "        Embedding(8, 5),                \u001b[90m# 40 parameters\u001b[39m\n",
       "        Embedding(17, 8),               \u001b[90m# 136 parameters\u001b[39m\n",
       "        Embedding(7, 4),                \u001b[90m# 28 parameters\u001b[39m\n",
       "        Embedding(6, 4),                \u001b[90m# 24 parameters\u001b[39m\n",
       "        Embedding(3, 2),                \u001b[90m# 6 parameters\u001b[39m\n",
       "        Embedding(43, 13),              \u001b[90m# 559 parameters\u001b[39m\n",
       "      ),\n",
       "      Dropout(0.2),\n",
       "    ),\n",
       "    BatchNorm(6),                       \u001b[90m# 12 parameters\u001b[39m\u001b[90m, plus 12\u001b[39m\n",
       "  ),\n",
       "  Chain(\n",
       "    Dense(55, 200, relu; bias=false),   \u001b[90m# 11_000 parameters\u001b[39m\n",
       "    BatchNorm(200),                     \u001b[90m# 400 parameters\u001b[39m\u001b[90m, plus 400\u001b[39m\n",
       "    identity,\n",
       "  ),\n",
       "  Chain(\n",
       "    Dense(200, 100, relu; bias=false),  \u001b[90m# 20_000 parameters\u001b[39m\n",
       "    BatchNorm(100),                     \u001b[90m# 200 parameters\u001b[39m\u001b[90m, plus 200\u001b[39m\n",
       "    identity,\n",
       "  ),\n",
       "  Dense(100, 2),                        \u001b[90m# 202 parameters\u001b[39m\n",
       ")\u001b[90m                   # Total: 18 arrays, \u001b[39m32_793 parameters, 127.891 KiB."
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To directly get a `Learner` suitable for our method and data, we can use the `methodlearner` function. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "learner = methodlearner(method, splitdata, backbone, Metrics(accuracy), batchsize=128, dlkwargs=NamedTuple(zip([:buffered], [false])))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Learner()"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once we have our learner, we can just call `FluxTraining.fit!` on it to train it for the desired number of epochs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "FluxTraining.fit!(learner, 1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32mEpoch 1 TrainingPhase(): 100%|██████████████████████████| Time: 0:00:03\u001b[39m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "┌───────────────┬───────┬─────────┬──────────┐\n",
      "│\u001b[1m         Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n",
      "├───────────────┼───────┼─────────┼──────────┤\n",
      "│ TrainingPhase │   1.0 │ 0.42667 │  0.80796 │\n",
      "└───────────────┴───────┴─────────┴──────────┘\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32mEpoch 1 ValidationPhase(): 100%|████████████████████████| Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "┌─────────────────┬───────┬─────────┬──────────┐\n",
      "│\u001b[1m           Phase \u001b[0m│\u001b[1m Epoch \u001b[0m│\u001b[1m    Loss \u001b[0m│\u001b[1m Accuracy \u001b[0m│\n",
      "├─────────────────┼───────┼─────────┼──────────┤\n",
      "│ ValidationPhase │   1.0 │ 0.34653 │  0.84171 │\n",
      "└─────────────────┴───────┴─────────┴──────────┘\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}