{"attributes":{"kind":"function","backlinks":[{"tag":"sourcefile","title":"MLUtils/parallel.jl","docid":"MLUtils@0.3.1/src/parallel.jl"},{"tag":"document","title":"Performant data pipelines","docid":"FastAI@pr-273/doc/docs/background/datapipelines.md"},{"tag":"sourcefile","title":"MLUtils/eachobs.jl","docid":"MLUtils@0.3.1/src/eachobs.jl"}],"methods":[{"symbol_id":"MLUtils.eachobsparallel","module_id":"MLUtils","file":"parallel.jl","line":29,"signature":"(::Signature)"}],"package_id":"MLUtils@0.3.1","title":"eachobsparallel","symbol_id":"MLUtils.eachobsparallel","exported":false,"module_id":"MLUtils"},"tag":"documentation","children":[{"attributes":{"symbol":"MLUtils.eachobsparallel","line":1,"module":"MLUtils","file":"parallel.jl"},"tag":"docstring","children":[{"attributes":{},"tag":"md","children":[{"attributes":{"lang":""},"tag":"codeblock","children":["eachobsparallel(data; buffer, executor, channelsize)\n"],"type":"node"},{"attributes":{},"tag":"p","children":["Construct a data iterator over observations in container ",{"attributes":{},"tag":"code","children":["data"],"type":"node"},". It uses available threads as workers to load observations in parallel, leading to large speedups when threads are available."],"type":"node"},{"attributes":{},"tag":"p","children":["To ensure that the active Julia session has multiple threads available, check that ",{"attributes":{},"tag":"code","children":["Threads.nthreads() > 1"],"type":"node"},". You can start Julia with multiple threads with the ",{"attributes":{},"tag":"code","children":["-t n"],"type":"node"}," option. If your data loading is bottlenecked by the CPU, it is recommended to set ",{"attributes":{},"tag":"code","children":["n"],"type":"node"}," to the number of physical CPU cores."],"type":"node"},{"attributes":{},"tag":"h2","children":["Arguments"],"type":"node"},{"attributes":{},"tag":"ul","children":[{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"code","children":["data"],"type":"node"},": a data container that implements ",{"attributes":{},"tag":"code","children":["getindex/getobs"],"type":"node"}," and ",{"attributes":{},"tag":"code","children":["length/numobs"],"type":"node"}],"type":"node"}],"type":"node"},{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"code","children":["buffer = false"],"type":"node"},": whether to use inplace data loading with ",{"attributes":{},"tag":"code","children":["getobs!"],"type":"node"},". Only use this if you need the additional performance and ",{"attributes":{},"tag":"code","children":["getobs!"],"type":"node"}," is implemented for ",{"attributes":{},"tag":"code","children":["data"],"type":"node"},". Setting ",{"attributes":{},"tag":"code","children":["buffer = true"],"type":"node"}," means that when using the iterator, an observation is only valid for the current loop iteration. You can also pass in a preallocated ",{"attributes":{},"tag":"code","children":["buffer = getobs(data, 1)"],"type":"node"},"."],"type":"node"}],"type":"node"},{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"code","children":["executor = Folds.ThreadedEx()"],"type":"node"},": task scheduler You may specify a different task scheduler which can be any ",{"attributes":{},"tag":"code","children":["Folds.Executor"],"type":"node"},"."],"type":"node"}],"type":"node"},{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"code","children":["channelsize = Threads.nthreads()"],"type":"node"},": the number of observations that are prefetched. Increasing ",{"attributes":{},"tag":"code","children":["channelsize"],"type":"node"}," can lead to speedups when per-observation processing time is irregular but will cause higher memory usage."],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}