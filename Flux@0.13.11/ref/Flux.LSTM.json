{"attributes":{"kind":"function","backlinks":[{"tag":"sourcefile","title":"Flux/layers/recurrent.jl","docid":"Flux@0.13.11/src/layers/recurrent.jl"},{"tag":"sourcefile","title":"Flux/Flux.jl","docid":"Flux@0.13.11/src/Flux.jl"}],"methods":[{"symbol_id":"Flux.LSTM","module_id":"Flux","file":"layers/recurrent.jl","line":358,"signature":"(::Signature)"}],"package_id":"Flux@0.13.11","title":"LSTM","symbol_id":"Flux.LSTM","exported":true,"module_id":"Flux"},"tag":"documentation","children":[{"attributes":{"symbol":"Flux.LSTM","line":322,"module":"Flux","file":"layers/recurrent.jl"},"tag":"docstring","children":[{"attributes":{},"tag":"md","children":[{"attributes":{"lang":""},"tag":"codeblock","children":["LSTM(in => out)\n"],"type":"node"},{"attributes":{},"tag":"p","children":[{"attributes":{"href":"https://www.researchgate.net/publication/13853244_Long_Short-term_Memory","title":""},"tag":"a","children":["Long Short Term Memory"],"type":"node"}," recurrent layer. Behaves like an RNN but generally exhibits a longer memory span over sequences."],"type":"node"},{"attributes":{},"tag":"p","children":["The arguments ",{"attributes":{},"tag":"code","children":["in"],"type":"node"}," and ",{"attributes":{},"tag":"code","children":["out"],"type":"node"}," describe the size of the feature vectors passed as input and as output. That is, it accepts a vector of length ",{"attributes":{},"tag":"code","children":["in"],"type":"node"}," or a batch of vectors represented as a ",{"attributes":{},"tag":"code","children":["in x B"],"type":"node"}," matrix and outputs a vector of length ",{"attributes":{},"tag":"code","children":["out"],"type":"node"}," or a batch of vectors of size ",{"attributes":{},"tag":"code","children":["out x B"],"type":"node"},"."],"type":"node"},{"attributes":{},"tag":"p","children":["This constructor is syntactic sugar for ",{"attributes":{},"tag":"code","children":["Recur(LSTMCell(a...))"],"type":"node"},", and so LSTMs are stateful. Note that the state shape can change depending on the inputs, and so it is good to ",{"attributes":{},"tag":"code","children":["reset!"],"type":"node"}," the model between inference calls if the batch size changes. See the examples below."],"type":"node"},{"attributes":{},"tag":"p","children":["See ",{"attributes":{"href":"https://colah.github.io/posts/2015-08-Understanding-LSTMs/","title":""},"tag":"a","children":["this article"],"type":"node"}," for a good overview of the internals."],"type":"node"},{"attributes":{},"tag":"h1","children":["Examples"],"type":"node"},{"attributes":{"lang":"jldoctest"},"tag":"codeblock","children":["julia> l = LSTM(3 => 5)\nRecur(\n  LSTMCell(3 => 5),                     # 190 parameters\n)         # Total: 5 trainable arrays, 190 parameters,\n          # plus 2 non-trainable, 10 parameters, summarysize 1.062 KiB.\n\njulia> l(rand(Float32, 3)) |> size\n(5,)\n\njulia> Flux.reset!(l);\n\njulia> l(rand(Float32, 3, 10)) |> size # batch size of 10\n(5, 10)\n"],"type":"node"},{"attributes":{"class":"warning"},"tag":"admonition","children":[{"attributes":{},"tag":"admonitiontitle","children":["Batch size changes"],"type":"node"},{"attributes":{},"tag":"admonitionbody","children":[{"attributes":{},"tag":"p","children":["Failing to call ",{"attributes":{},"tag":"code","children":["reset!"],"type":"node"}," when the input batch size changes can lead to unexpected behavior. See the example in ",{"attributes":{"reftype":"symbol","href":"@ref","title":"","document_id":"Flux@0.13.11/ref/Flux.RNN"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["RNN"],"type":"node"}],"type":"node"},"."],"type":"node"}],"type":"node"}],"type":"node"},{"attributes":{},"tag":"h1","children":["Note:"],"type":"node"},{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"code","children":["LSTMCell"],"type":"node"},"s can be constructed directly by specifying the non-linear function, the ",{"attributes":{},"tag":"code","children":["Wi"],"type":"node"}," and ",{"attributes":{},"tag":"code","children":["Wh"],"type":"node"}," internal matrices, a bias vector ",{"attributes":{},"tag":"code","children":["b"],"type":"node"},", and a learnable initial state ",{"attributes":{},"tag":"code","children":["state0"],"type":"node"},". The  ",{"attributes":{},"tag":"code","children":["Wi"],"type":"node"}," and ",{"attributes":{},"tag":"code","children":["Wh"],"type":"node"}," matrices do not need to be the same type. See the example in ",{"attributes":{"reftype":"symbol","href":"@ref","title":"","document_id":"Flux@0.13.11/ref/Flux.RNN"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["RNN"],"type":"node"}],"type":"node"},"."],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}