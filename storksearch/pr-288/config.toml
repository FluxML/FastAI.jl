[input]
base_directory = "."
url_prefix = ""

    [[input.files]]
    title = "FastAI/interpretation/learner.jl"
    contents = "task learner n context backend cb getcallback learner devicefn isnothing cb identity cb movedatafn backfn isnothing cb identity xs ys learner n n context ŷs learner model devicefn xs backfn backend task xs ys ŷs \n"
    url = "FastAI@pr-288/src/interpretation/learner.jl"
    [[input.files]]
    title = "FastAI/datablock/loss.jl"
    contents = "predblock predblock predblock \n"
    url = "FastAI@pr-288/src/datablock/loss.jl"
    [[input.files]]
    title = "showprediction"
    contents = "showprediction([backend], task, pred)\nshowprediction([backend], task, sample, pred)\nShow a prediction pred. If a sample is also given, show it next to the prediction. ŷ\n\n"
    url = "FastAI@pr-288/ref/FastAI.showprediction"
    [[input.files]]
    title = "blockbackbone"
    contents = "blockbackbone(inblock)\nCreate a default backbone that takes in block inblock.\n\n"
    url = "FastAI@pr-288/ref/FastAI.blockbackbone"
    [[input.files]]
    title = "FastAIDataset"
    contents = "FastAIDataset"
    url = "FastAI@pr-288/ref/FastAI.Datasets.FastAIDataset"
    [[input.files]]
    title = "Setup"
    contents = "Setup\nFastAI.jl is a Julia package. You can download Julia from the official website. You can install FastAI.jl like any other Julia package using the REPL as follows.\nPkg Pkg add \nVisualization Aside from text-based visualizations, FastAI.jl also defines Makie.jl plotting recipes to visualize data. If you want to use them, you'll have to install and one of the Makie.jl backends CairoMakie.jl, GLMakie.jl or WGLMakie.jl and load the package.\nPkg Pkg add CairoMakie \nColab If you don't have access to a GPU or want to try out FastAI.jl without installing Julia, try out this FastAI.jl Colab notebook. We're working on adding a \"Launch Colab\" button to every documentation page based off a notebook file, but for now you can copy the code over manually.\nThreaded data loading To make use of multi-threaded data loading, you need to start Julia with multiple threads, either with the -t auto commandline flag or by setting the environment variable JULIA_NUM_THREADS. See the IJulia.jl documentation for instructions on setting these for Jupyter notebook kernels.\n\n"
    url = "FastAI@pr-288/doc/docs/setup.md"
    [[input.files]]
    title = "decodestate"
    contents = "decodestate"
    url = "FastAI@pr-288/ref/FastAI.decodestate"
    [[input.files]]
    title = "Steepest"
    contents = "Steepest <: LREstimator\nEstimate the optimal learning rate to be where the gradient of the loss is the steepest, i.e. the decrease is largest.\n\n"
    url = "FastAI@pr-288/ref/FastAI.Steepest"
    [[input.files]]
    title = "shouldbatch"
    contents = "shouldbatch(::LearningTask)\nDefine whether encoded samples for a learning task should be batched. The default is true.\n\n"
    url = "FastAI@pr-288/ref/FastAI.shouldbatch"
    [[input.files]]
    title = "FastAI/Registries/recipes.jl"
    contents = "_datareciperegistry datasetregistry name Registry id Field String name formatfn x sprint show x blocks Field Any name description filterfn formatfn b FeatureRegistries code_format _formatblock b description Field String name optional description formatfn FeatureRegistries md_format downloaded Field Bool name description computefn row key datasetregistry row datasetid loader datasetid Field String name description computefn row key val row key haskey datasetregistry val throw ArgumentError val val package Field Module name recipe Field name formatfn FeatureRegistries type_format name loadfn loadrecipeentry row dataset load datasetregistry row datasetid row recipe dataset description _datareciperegistry kwargs isempty kwargs filter kwargs m Module recipes m recipes reg Registry m Module recipes Dict datasetid rs recipes recipe rs recipeid recipe recipe Pair joinpath datasetid recipe recipe datasetid recipe haskey reg recipeid push! reg id recipeid datasetid datasetid blocks recipe package m recipe recipe \n"
    url = "FastAI@pr-288/src/Registries/recipes.jl"
    [[input.files]]
    title = "AbstractBlock"
    contents = "abstract type AbstractBlock\nAbstract supertype of all blocks. You should not subtype form this, but instead from Block or WrapperBlock.\n\n"
    url = "FastAI@pr-288/ref/FastAI.AbstractBlock"
    [[input.files]]
    title = "DATASETS"
    contents = "DATASETS"
    url = "FastAI@pr-288/ref/FastAI.Datasets.DATASETS"
    [[input.files]]
    title = "tsregression"
    contents = "Time Series Regression\n\nImporting things we would need.\n\nFastTimeSeries \nLoading The Data\nWe are going to select a dataset from the recently released Monash, UEA & UCR\nTime Series Extrinsic Regression Repository (2020) (web, paper).\n\ndata blocks load \ngetobs gets us a sample from the TimeSeriesDataset. It returns a tuple with the input time series and the correspodning label.\n\ninput label sample data \nNow we create a learning task for time-series regression. This means using the time-series to predict labels. We will use the TimeSeriesRow block as input and Continuous block as the target.\n\ntask TSRegression blocks data \nThe encodings passed in transform samples into formats suitable as inputs and outputs for a model\n\nLet's check that samples from the created data container conform to the blocks of the learning task:\n\ntask blocks sample sample \nTo get an overview of the learning task created, and as a sanity test, we can use describetask. This shows us what encodings will be applied to which blocks, and how the predicted ŷ values are decoded.\n\ntask \nencoded_sample task sample \nTraining\n\nWe will use an InceptionTime model for this task.\n\nmodel FastTimeSeries Models InceptionTime \nWe can tasklossfn to get a loss function suitable for our task.\n\nlossfn task \nNext we create a pair of training and validation data loaders. They take care of batching and loading the data in parallel in the background.\n\ntraindl validdl data task \nWe will use an ADAM optimzer for this task.\n\noptimizer ADAM \nWith the addition of an optimizer and a loss function, we can now create a Learner and start training.\n\nlearner model lossfn data traindl validdl optimizer optimizer callbacks \nlearner \nWe can save the model for later inference using savetaskmodel\n\ntask learner model force \n\n"
    url = "FastAI@pr-288/doc/docs/notebooks/tsregression.ipynb"
    [[input.files]]
    title = "FastAI/datasets/batching.jl"
    contents = "batch Union Tuple NamedTuple batch batch Dict batch first keys batch batch AbstractArray T N T N size batch N batch collect batch batch batch i i batch batch AbstractArray T N i T N view batch _ N i batch AbstractVector i batch i batch Tuple i Tuple batch j i j length batch batch NamedTuple i zip keys batch values batch i batch Dict i Dict k v i k v batch \n"
    url = "FastAI@pr-288/src/datasets/batching.jl"
    [[input.files]]
    title = "frozen_optimizer"
    contents = "frozen_optimizer(optim, grouper, model)\nCreate an optimizer that only updates parameters which ParamGrouper puts into group 2.\n\n"
    url = "FastAI@pr-288/ref/FastAI.frozen_optimizer"
    [[input.files]]
    title = "showpredictions"
    contents = "showpredictions([backend], task, preds)\nshowpredictions([backend], task, samples, preds)\nShow predictions pred. If samples are also given, show them next to the prediction.\n\n"
    url = "FastAI@pr-288/ref/FastAI.showpredictions"
    [[input.files]]
    title = "FastAI/training/finetune.jl"
    contents = "learner nepochs base_lr freezeepochs grouper learner model backbone_factor div kwargs foptim learner optimizer grouper learner model learner optimizer foptim learner freezeepochs base_lr pct_start kwargs doptim learner optimizer grouper learner model backbone_factor learner optimizer doptim learner nepochs base_lr div div kwargs learner optim grouper model optim grouper model optim grouper model factor paramgroups grouper model paramgroups Dict factor optim model model length model error length model length model \n"
    url = "FastAI@pr-288/src/training/finetune.jl"
    [[input.files]]
    title = "FastAI/tasks/task.jl"
    contents = "args kwargs args kwargs buf task ctx sample task ctx sample buf task ctx sample task ctx sample buf task ctx ypred task ctx ypred args kwargs args kwargs task task task \n"
    url = "FastAI@pr-288/src/tasks/task.jl"
    [[input.files]]
    title = "mocksample"
    contents = "mocksample(task)\nGenerate a random sample compatible with task.\n\n"
    url = "FastAI@pr-288/ref/FastAI.mocksample"
    [[input.files]]
    title = "encodestate"
    contents = "encodestate"
    url = "FastAI@pr-288/ref/FastAI.encodestate"
    [[input.files]]
    title = "OneHot"
    contents = "OneHot()\nOneHot(T, threshold)\nEncoding that turns categorical labels into one-hot encoded arrays of type T.\nEncodes\n      `Mask{N, U}` -> `OneHotTensor{N, T}`\n`LabelMulti{N, U}` -> `OneHotTensorMulti{N, T}`\n        `Label{U}` -> `OneHotTensor{N, T}`\n\n"
    url = "FastAI@pr-288/ref/FastAI.OneHot"
    [[input.files]]
    title = "accuracy_thresh"
    contents = "accuracy_thresh"
    url = "FastAI@pr-288/ref/FastAI.accuracy_thresh"
    [[input.files]]
    title = "encodingscolumn"
    contents = "encodingscolumn"
    url = "FastAI@pr-288/ref/FastAI.encodingscolumn"
    [[input.files]]
    title = "Context"
    contents = "abstract type Context\nRepresents a context in which a data transformation is made. This allows using dispatching for varying behavior, for example, to apply augmentations only during training or use non-destructive cropping during inference.\nSee Training, Validation and Inference.\n\n"
    url = "FastAI@pr-288/ref/FastAI.Context"
    [[input.files]]
    title = "TestWrapper"
    contents = "TestWrapper"
    url = "FastAI@pr-288/ref/FastAI.TestWrapper"
    [[input.files]]
    title = "tuplemap"
    contents = "tuplemap"
    url = "FastAI@pr-288/ref/FastAI.tuplemap"
    [[input.files]]
    title = "showoutput"
    contents = "showoutput([backend], task, output)\nshowoutput([backend], task, encsample, output)\nShow a model output to backend. If an encoded sample encsample is also given, show it next to the output.\n\n"
    url = "FastAI@pr-288/ref/FastAI.showoutput"
    [[input.files]]
    title = "FastAI/training/paramgroups.jl"
    contents = "map IdDict IdDict Base show io IO print io pg x AbstractArray get pg map x nothing pg grouper m m_ grouper m p params m_ pg map p idxs Any grouper m Dict i m is i is enumerate grouper idxs grouper m pg pg grouper m pg model paramgroups model paramgroups model weight paramgroups model weight paramgroups rand nothing \n"
    url = "FastAI@pr-288/src/training/paramgroups.jl"
    [[input.files]]
    title = "FastAI"
    contents = "FastAI"
    url = "FastAI@pr-288/ref/FastAI"
    [[input.files]]
    title = "FastAI/tasks/taskdata.jl"
    contents = "TData TTask TContext data TData task TTask context TContext Base length ds ds data Base getindex ds idx ds task ds context ds data idx getobs! buf ds idx buf ds task ds context ds data idx traindata validdata task shuffle validbsfactor parallel collate kwargs traindata task shuffle collate parallel kwargs validdata task validbsfactor collate parallel kwargs data task pctgval kwargs traindata validdata data at pctgval traindata validdata task kwargs \n"
    url = "FastAI@pr-288/src/tasks/taskdata.jl"
    [[input.files]]
    title = "datarecipes"
    contents = "datarecipes(; filters...)\nShow a registry of available dataset recipes. A dataset recipe defines how to load a dataset into a suitable format for use with a learning task.\nPass in filters as keyword arguments to look at a subset.\nSee also finding functionality, datasets, and learningtasks. For more information on registries, see FeatureRegistries.jl.\nExamples\nShow all available dataset recipes:\n\nShow all recipes for datasets that have \"image\" in their name:\ndatasetid \nShow all data recipes usable for classification tasks, that is where the target block is a Label:\nblocks Any \nGet an explanation of fields in the dataset recipe registry:\ninfo \n\n"
    url = "FastAI@pr-288/ref/FastAI.Registries.datarecipes"
    [[input.files]]
    title = "mockinput"
    contents = "mockinput(task)\nGenerate a random input compatible with task.\n\n"
    url = "FastAI@pr-288/ref/FastAI.mockinput"
    [[input.files]]
    title = "listencodeblocks"
    contents = "listencodeblocks"
    url = "FastAI@pr-288/ref/FastAI.listencodeblocks"
    [[input.files]]
    title = "wrapped"
    contents = "wrapped"
    url = "FastAI@pr-288/ref/FastAI.wrapped"
    [[input.files]]
    title = "runtests"
    contents = "FastAI.Registries.runtests(pattern...; kwargs...)\nEquivalent to ReTest.retest(FastAI.Registries, pattern...; kwargs...). This function is defined automatically in any module containing a @testset, possibly nested within submodules.\n\n"
    url = "FastAI@pr-288/ref/FastAI.Registries.runtests"
    [[input.files]]
    title = "TaskDataset"
    contents = "taskdataset(data, task, context)\nTransform data container data of samples into a data container of encoded samples. Maps encodesample(task, context, sample) over the observations in data. Also handles in-place MLUtils.getobs! through encodesample!.\n\n"
    url = "FastAI@pr-288/ref/FastAI.TaskDataset"
    [[input.files]]
    title = "checkblock"
    contents = "checkblock(block, obs)\ncheckblock(blocks, obss)\nCheck whether obs is compatible with block, returning a Bool.\nExamples\nrand RGB \nrand RGB \nExtending\nAn implementation of checkblock should be as specific as possible. The default method returns false, so you only need to implement methods for valid types and return true.\n\n"
    url = "FastAI@pr-288/ref/FastAI.checkblock"
    [[input.files]]
    title = "FastAI/blocks/label.jl"
    contents = "T classes AbstractVector T label T obs T T obs label classes label T T rand label classes T Type data unique data T classes AbstractVector T label T v AbstractVector T T all map x x label classes v label unique rand label classes _ rand length label classes Type data unique data InlineTest block InlineTest FastAI block InlineTest FastAI block targets block targets InlineTest block classes InlineTest block InlineTest FastAI block InlineTest FastAI block targets block targets InlineTest block classes \n"
    url = "FastAI@pr-288/src/blocks/label.jl"
    [[input.files]]
    title = "BlockTask"
    contents = "BlockTask(blocks, encodings)\nCreate an AbstractBlockTask directly, passing in a named tuple blocks and encodings. See SupervisedTask for supervised training tasks.\n\n"
    url = "FastAI@pr-288/ref/FastAI.BlockTask"
    [[input.files]]
    title = "obsslices"
    contents = "obsslices"
    url = "FastAI@pr-288/ref/FastAI.Datasets.obsslices"
    [[input.files]]
    title = "Validation"
    contents = "Validation <: Context\nA context for applying data transformations during validation/testing. Encodings and LearningTasks can dispatch on this when certain transformations, like random augmentations, should not be applied during validation, only in training.\n\n"
    url = "FastAI@pr-288/ref/FastAI.Validation"
    [[input.files]]
    title = "taskdataloaders"
    contents = "taskdataloaders(data, task[, batchsize])\ntaskdataloaders(traindata, validdata, task[, batchsize; shuffle = true, dlkwargs...])\nCreate training and validation DataLoaders from two data containers (traindata, valdata). If only one container data is passed, splits it into two, with pctgvalid% of the data going into the validation split.\nArguments\nPositional:\nbatchsize = 16\n\n\nKeyword:\nshuffle = true: Whether to shuffle the training data container\n\nvalidbsfactor = 2: Factor to multiply batchsize for validation data loader with (validation batches can be larger since no GPU memory is needed for the backward pass)\n\n\nAll remaining keyword arguments are passed to DataLoader.\nExamples\nBasic usage:\ntraindl validdl data task \nExplicit validation data container and no shuffling of training container:\ntraindl validdl traindata validdata task shuffle \nCustomizing the DataLoader\ntraindl validdl data task parallel buffered \n\n"
    url = "FastAI@pr-288/ref/FastAI.taskdataloaders"
    [[input.files]]
    title = "FastAI/Registries/Registries.jl"
    contents = "FastAI FastAI FastAI Markdown DataDeps FeatureRegistries FeatureRegistries Registry Field InlineTest _formatblock t Type Tuple _formatblock Tuple t types _formatblock t Tuple map _formatblock t _formatblock T Type T _formatblock T T T BSupported Type BWanted Type BSupported BWanted BWanted BSupported B1 Type Tuple B2 Type Tuple all b1 b2 b1 b2 zip B1 types B2 types BSupported Type Type Any BSupported Type Any Type BSupported Type Any Type Any bsupported bwanted bsupported bwanted FastAI FastAI String FastAI FastAI FastAI String FastAI Tuple FastAI Tuple FastAI Tuple FastAI String Tuple FastAI Tuple FastAI String Any FastAI String FastAI String FastAI FastAI String FastAI String FastAI FastAI FastAI FastAI String FastAI FastAI FastAI FastAI String include include include find info load __init__ _registerdatasets \n"
    url = "FastAI@pr-288/src/Registries/Registries.jl"
    [[input.files]]
    title = "DATARECIPES"
    contents = "DATARECIPES"
    url = "FastAI@pr-288/ref/FastAI.Registries.DATARECIPES"
    [[input.files]]
    title = "test_task_show"
    contents = "test_task_show(task, backend::ShowBackend)\nTest suite that tests that all learning task-related show* functions work for backend\nKeyword arguments\nsample = mockblock(getblocks(task)): Sample data to use for tests.\n\noutput = mockblock(getblocks(task).ŷ): Model output data to use for tests.\n\n\n\n"
    url = "FastAI@pr-288/ref/FastAI.test_task_show"
    [[input.files]]
    title = "datasets"
    contents = "datasets(; filters...)\nShow a registry of available datasets. Pass in filters as keyword arguments to look at a subset.\nSee also finding functionality, learningtasks, and datarecipes. For more information on registries, see FeatureRegistries.jl.\nExamples\nShow all available learning tasks:\n\nDownload a dataset:\npath load \nGet an explanation of fields in the dataset registry:\ninfo \nShow all datasets with \"image\" in their name:\nid \n\n"
    url = "FastAI@pr-288/ref/FastAI.Registries.datasets"
    [[input.files]]
    title = "Block"
    contents = "abstract type Block\nA block describes the meaning of a piece of data in the context of a learning task. For example, for supervised learning tasks, there is an input and a target and we want to learn to predict targets from inputs. Learning to predict a cat/dog label from 2D images is a supervised image classification task that can be represented with the Blocks Image{2}() and Label([\"cat\", \"dog\"]).\nBlocks are used in virtually every part of the high-level interfaces, from data processing over model creation to visualization.\nExtending\nConsider the following when subtyping Block. A block\nDoes not hold observation data itself. Instead they are used in conjunction with data to annotate it with some meaning.\n\nIf it has any fields, they should be metadata that cannot be derived from the data itself and is constant for every sample in the dataset. For example Label holds all possible classes which are constant for the learning problem.\n\n\nInterfaces\nThere are many interfaces that can be implemented for a Block. See the docstrings of each function for more info about how to implement it.\ncheckblock(block, obs): check whether an observation is a valid block\n\nmockblock(block): randomly generate an observation\n\nblocklossfn(predblock, yblock): loss function for comparing two blocks\n\nblockmodel(inblock, outblock[, backbone]): construct a task-specific model\n\nblockbackbone(inblock): construct a backbone model that takes in specific data\n\nshowblock!(block, obs): visualize an observation\n\n\n\n"
    url = "FastAI@pr-288/ref/FastAI.Block"
    [[input.files]]
    title = "showoutputbatch"
    contents = "showoutputbatch([backend], task, outputbatch)\nshowoutputbatch([backend], task, batch, outputbatch)\nShow collated batch of outputs to backend. If a collated batch of encoded samples batch is also given, show them next to the outputs. See showoutputs if you have vectors of outputs and not collated batches.\n\n"
    url = "FastAI@pr-288/ref/FastAI.showoutputbatch"
    [[input.files]]
    title = "Keypoint regression"
    contents = "Keypoint regression\n\nSingle keypoint regression consists of localizing a keypoint in an image. Here we'll be training on a head pose dataset, where every image has a person in it and the head of the person is annotated. Since keypoint datasets all have different formats, we have to do a bit more manual work to get the task dataset loaded. First we import everything we'll need:\n\nCairoMakie CairoMakie activate! type DelimitedFiles readdlm FastVision Metalhead FilePathsBase FastVision \nCreating a task data container\n\nload(datasets()[id]) downloads the files, but it's up to us to load them into a usable format. In the end, the task data container should contain tuples of an image and a keypoint each.\n\npath load \nfiles path files \nFirst we create a loadfolderdata from the directory where the dataset has been downloaded to:\n\nLoading a loadfolderdata simply treats every file as a single observation. However, that is not what we want here: for every observation we have one image and one annotation file that make up one observation and we want to ignore all other files, like the README. To achieve this, we'll create two data containers containing all the image paths and annotation paths respectively by filtering the container with all paths.\n\nimagefiles path filterfn FastVision annotfiles path filterfn p occursin p \nimagefiles annotfiles \nNext we need to map functions over each observation that load the data from the files. An image file can be loaded using the loadfile utility. The keypoints have a custom format, so we write a helper function to parse them from a text file. The details of how the format is loaded aren't important.\n\nreadcalibrationfile p readdlm string p CAL readcalibrationfile joinpath path loadannotfile annotpath cal CAL ctr readdlm string annotpath cx ctr cal ctr cal cy ctr cal ctr cal FastVision SVector cy cx \nNow we can use mapobs to lazily map the loading function over the container. Note that beside loading the image and keypoint, we also extract the subject ID from the path. We'll use this in a bit for splitting the dataset appropriately and we don't have access to the path information anymore once we have a container of loaded data.\n\ndata imagefiles loadannotfile annotfiles ids map p parse Int p imagefiles obs image ks data \nWe can visualize an observation using DataAugmentation.showitems if we wrap the data in item types:\n\nimage ks size image \nBefore we can start using this data container for training, we need to split it into a training and validation dataset. Since there are 13 different persons with many images each, randomly splitting the container does not make sense. The validation dataset would then contain many images that are very similar to those seen in training, and would hence say little about the generalization ability of a model. We instead use the first 12 subjects as a training dataset and validate on the last.\n\ntraindata data data ids validdata data data ids traindata validdata \nThe learning task\n\nNext we need to define a learning task that encodes and augments each image and keypoint in a form that we can train a model on. Here we make use of ProjectiveTransforms for resizing, cropping and augmenting the image and keypoint and ImagePreprocessing to reshape and normalize the image. Finally, KeypointPreprocessing makes sure keypoints fall between -1 and 1.\n\nsz task sz buffered augmentations max_warp sz \nWe can check that each image is resized to (224, 224) and the keypoints are normalized:\n\nim k traindata x y task im k summary x y \nDecoding the encoded targets should give back a point within the original image bounds:\n\ntask y \nxs ys task traindata task xs ys \nThat is looking good! We can see that the keypoint is aligned with center of the head even after heavy augmentation. Now it is finally time to train a model.\n\nTraining\n\nWe'll use a modified ResNet as a model backbone. and add a couple layers that regress the keypoint. taskmodel knows how to do this by looking at the data blocks used and calling blockmodel(KeypointTensor{2, Float32}((1,)), KeypointTensor{2, Float32}((1,)), backbone).\n\nThe implementation, for reference, looks like this:\ninblock N outblock N backbone N outsz backbone ntuple _ N inblock nchannels outch outsz end head Models visionhead outch prod outblock sz N p backbone head \n\nbackbone Metalhead ResNet layers end model task backbone \nNext we create a pair of training and validation data loaders. They take care of batching and loading the data in parallel in the background.\n\ntraindl validdl traindata validdata task \nWith the addition of an optimizer and a loss function, we can now create a Learner and start training. Just like taskmodel, tasklossfn selects the appropriate loss function for a BlockTasks blocks. Here both the encoded target block and model output block are block = KeypointTensor{2, Float32}((1,)), so blocklossfn(block, block) is called which returns Mean Squared Error as a suitable loss function.\n\nlearner model task data traindl validdl optimizer callbacks \nlearner \nWe can save the model for later inference using savetaskmodel:\n\ntask learner model \nThe loss is going down during training which is a good sign, but visualizing the predictions against the ground truth will give us a better idea of how well the model performs. We'll use showoutputs to compare batches of encoded targets and model outputs. For this we can run the model on a batch from the validation dataset and see how it performs.\n\ntask learner n context \nWe can also see that the trained model generalizes well to the heavy augmentation employed during training. The augmentation also explains why the training loss is so much higher than the validation loss.\n\ntask learner n context \n"
    url = "FastAI@pr-288/doc/docs/notebooks/keypointregression.ipynb"
    [[input.files]]
    title = "FastAI/interpretation/backend.jl"
    contents = "handle backend title block Pair obs handle backend block obs backend block obs handle backend handle backend block obs Base Base backend block obss backend backend block obss handle backend block obs handle backend parent block obs backend wrapper backend parent wrapper \n"
    url = "FastAI@pr-288/src/interpretation/backend.jl"
    [[input.files]]
    title = "Building a sysimage"
    contents = "Building a sysimage\nBuilding a sysimage for FastAI.jl and its dependecies can greatly reduce the amount of time waiting for code to compile when using it to train models.\nThis folder contains a script to build such a sysimage.\n\n"
    url = "FastAI@pr-288/doc/sysimage/README.md"
    [[input.files]]
    title = "DATASETCONFIGS"
    contents = "DATASETCONFIGS"
    url = "FastAI@pr-288/ref/FastAI.Datasets.DATASETCONFIGS"
    [[input.files]]
    title = "FastAI/datablock/encoding.jl"
    contents = "inblocks Tuple outblocks Tuple map inblocks outblocks inblock Nothing inblock inblocks Tuple Nothing inblocks outblock outblock enc block block enc block enc block block enc block encoding ctx block obs kwargs isempty kwargs obs encoding ctx block obs encodings NTuple N context blocks data N encoding encodings data encoding context blocks data blocks encoding blocks data encoding context blocks Tuple obss Tuple length blocks length obss map block obs encoding context block obs blocks obss encoding context blocks NamedTuple obss NamedTuple length blocks length obss NamedTuple zip keys obss encoding context values blocks values obss encoding ctx block obs kwargs isempty kwargs obs encoding ctx block obs encodings NTuple N context blocks obs N encoding Iterators reverse encodings obs encoding context blocks obs blocks encoding blocks obs encoding context blocks Tuple obss Tuple length blocks length obss map block obs encoding context block obs blocks obss encoding context blocks NamedTuple obss NamedTuple length blocks length obss NamedTuple zip keys obss encoding context values blocks values obss nothing encoding blocks Tuple map block encoding block blocks encodings Tuple blocks encoded encoding encodings encoded encoded isnothing encoding blocks blocks encoding blocks encoded blocks nothing nothing encoding blocks Tuple map block encoding block blocks encodings blocks decoded encoding Iterators reverse encodings decoded decoded isnothing encoding blocks blocks encoding blocks decoded blocks nothing encoding context blocks obs nothing encoding context blocks obs nothing encoding context blocks Tuple obss Tuple state encoding context blocks obss map block obs encoding context block obs state state blocks obss encoding context blocks Tuple obss Tuple state encoding context blocks obss length blocks length obss Tuple encoding context block obs state state block obs zip blocks obss encoding block obs block Test typeof encoding block Test block obs Test isnothing encoding block outblock encoding block outobs encoding block obs Test outblock outobs outblock Tuple idx length outblock inblock encoding outblock idx isnothing inblock Test block idx inblock inobs encoding outblock idx outobs idx Test inblock inobs inblock encoding outblock isnothing inblock Test block inblock inobs encoding outblock outobs Test inblock inobs \n"
    url = "FastAI@pr-288/src/datablock/encoding.jl"
    [[input.files]]
    title = "Label"
    contents = "Label(classes) <: Block\nsetup(LabelMulti, data)\nBlock for a categorical label in a single-class context. data is valid for Label(classes) if data ∈ classes.\nSee LabelMulti for the multi-class setting where an observation can belong to multiple classes.\nExamples\nblock block block \nYou can use setup to create a Label instance from a data container containing possible classes:\ntargets block targets block \n\n"
    url = "FastAI@pr-288/ref/FastAI.Label"
    [[input.files]]
    title = "group"
    contents = "group"
    url = "FastAI@pr-288/ref/FastAI.group"
    [[input.files]]
    title = "DATASETS_IMAGECLASSIFICATION"
    contents = "DATASETS_IMAGECLASSIFICATION"
    url = "FastAI@pr-288/ref/FastAI.Datasets.DATASETS_IMAGECLASSIFICATION"
    [[input.files]]
    title = "FastAI/learner.jl"
    contents = "task traindata validdata backbone nothing model nothing callbacks pctgval optimizer Adam lossfn task kwargs isnothing model model isnothing backbone task task backbone dls traindata validdata task kwargs model dls optimizer lossfn callbacks task data pctgval kwargs traindata validdata data at pctgval task traindata validdata kwargs learner context n nothing dl context learner data validation learner data training batch first dl b min isnothing n Inf n batch batch batch s s _ zip batch b batch batch rand rand learner identity mse data batch batch size learner size learner n task data rand rand learner task data model identity learner task data model identity length learner data training length learner data validation learner task data model identity pctgval length learner data training length learner data validation learner task data model identity validbsfactor length learner data training length learner data validation learner task data model identity callbacks mktempdir isnothing getcallback learner \n"
    url = "FastAI@pr-288/src/learner.jl"
    [[input.files]]
    title = "MonashRegressionDataset"
    contents = "MonashRegressionDataset"
    url = "FastAI@pr-288/ref/FastAI.Datasets.MonashRegressionDataset"
    [[input.files]]
    title = "withcallbacks"
    contents = "withcallbacks(f, learner, callbacks...)\nRun f with callbacks on learner. Existing callbacks on learner of the same type as in callbacks are swapped during the execution of f.\n\n"
    url = "FastAI@pr-288/ref/FastAI.withcallbacks"
    [[input.files]]
    title = "Glossary"
    contents = "Glossary\nTerms commonly used in FastAI.jl.\nType abbreviations\nIn many docstrings, generic types are abbreviated with the following symbols. Many of these refer to a learning task; the context should make clear which task is meant.\nDC{T}: A data container of type T, meaning a type that implements the data container interface getindex/getobs and length/numobs where getobs : (DC{T}, Int) -> Int, that is, each observation is of type T.\n\nI: Type of the unprocessed input in the context of a task.\n\nT: Type of the target variable.\n\nX: Type of the processed input. This is fed into a model, though it may be batched beforehand. Xs represents a batch of processed inputs.\n\nY: Type of the model output. Ys represents a batch of model outputs.\n\nmodel/M: A learnable mapping M : (X,) -> Y or M : (Xs,) -> Ys. It predicts an encoded target from an encoded input. The learnable part of a learning task.\n\n\nSome examples of these in use:\nLearningTask is a concrete approach to learning to predict T from I by using the encoded representations X and Y.\n\nencodeinput : (task, context, I) -> X encodes an input so that a prediction can be made by a model.\n\nA task dataset is a DC{(I, T)}, i.e. a data container where each observation is a 2-tuple of an input and a target.\n\n\nDefinitions\nData container\nA data structure that is used to load a number of data observations separately and lazily. It defines how many observations it holds with numobs and how to load a single observation with getobs.\nLearning task\nAn instance of DLPipelines.LearningTask. A concrete approach to solving a learning task. Encapsulates the logic and configuration for processing data to train a model and make predictions.\nSee the DLPipelines.jl documentation for more information.\nTask data container / dataset\nDC{(I, T)}. A data container containing pairs of inputs and targets. Used in taskdataset, taskdataloaders and evaluate.\n\n"
    url = "FastAI@pr-288/doc/docs/glossary.md"
    [[input.files]]
    title = "FastAI/tasks/check.jl"
    contents = "task model task sample task devicefn identity Test Test context Test task context sample Test context x _ task context sample Test ŷ _predictx task model x devicefn Test context x _ task context sample ŷ _predictx task model x devicefn Test task context ŷ _predictx method model x device identity method x batch x ŷs device model device x method ŷ ŷs _ ndims ŷs ŷ \n"
    url = "FastAI@pr-288/src/tasks/check.jl"
    [[input.files]]
    title = "createhandle"
    contents = "createhandle(backend::ShowBackend)\nCreates a context to which blocks of data can be shown using the mutating functions showblock! and showblocks!. It is called internally when using showblock or showblocks.\nbackend backend block obs backend block obs \n\n"
    url = "FastAI@pr-288/ref/FastAI.createhandle"
    [[input.files]]
    title = "batchsize"
    contents = "batchsize"
    url = "FastAI@pr-288/ref/FastAI.Datasets.batchsize"
    [[input.files]]
    title = "How to augment vision data"
    contents = "How to augment vision data\nData augmentation is important to train models with good generalization ability, especially when the size of your dataset is limited. FastAI.jl gives you high-level helpers to use data augmentation in vision learning tasks, but also allows directly using DataAugmentation.jl, the underlying data augmentation library.\nBy default, the only augmentation that will be used in computer vision tasks is a random crop, meaning that after images, keypoints and masks are resized to a similar size a random portion will be cropped during training. We can demonstrate this on the image classification task.\nFastVision FastMakie CairoMakie CairoMakie activate! type data blocks load task blocks xs ys task data fill task xs ys \nMost learning tasks let you pass additional augmentations as keyword arguments. For example, ImageClassification takes the aug_projection and aug_image arguments. FastAI.jl provides the augs_projection helper to quickly construct a set of projective data augmentations.\ntask2 blocks augmentations xs2 ys2 task2 data fill task2 xs2 ys2 \nLikewise, there is an augs_lighting helper that adds contrast and brightness augmentation:\ntask3 blocks augmentations augmentations xs3 ys3 task3 data fill task3 xs3 ys3 \n\n"
    url = "FastAI@pr-288/doc/docs/howto/augmentvision.md"
    [[input.files]]
    title = "lrfindtextplot"
    contents = "lrfindtextplot"
    url = "FastAI@pr-288/ref/FastAI.lrfindtextplot"
    [[input.files]]
    title = "decodewhile"
    contents = "decodewhile(f, encodings, ctx, block, obs) -> (block', obs')\nDecode block by successively applying encodings to decode in reverse order until f(block') == false.\n\n"
    url = "FastAI@pr-288/ref/FastAI.decodewhile"
    [[input.files]]
    title = "fitonecycle!"
    contents = "fitonecycle!(learner, nepochs[, lrmax])\nFit learner for nepochs using a one-cycle learning rate schedule. The learning rate starts at lrmax/div for pct_start*nepochs epochs, rising to lrmax and then goes down to lrmax/div_final over the remaining duration.\nKeyword arguments\nwd = 0: weight decay\n\npct_start = 0.25: Percentage of time spent raising the learning rate\n\ndiv = 25: Starting learning rate is lr_max/div\n\ndiv_final = 1e5: Ending learning rate is lr_max/div_final\n\n\n\n"
    url = "FastAI@pr-288/ref/FastAI.fitonecycle!"
    [[input.files]]
    title = "Tabular Classification"
    contents = "Tabular Classification\n\nTabular Classification involves having a categorical column as the target. Here, we'll use the adult sample dataset from fastai and try to predict whether the salary is above 50K or not, making this a binary classification task.\n\nFastTabular FastTabular Tables FastTabular \nWe can quickly download and get the path of any dataset from fastai by using datasets. Once we have the path, we'll load the data in a TableDataset. By default, if we pass in just the path to TableDataset, the data is loaded in a DataFrame, but we can use any package for accessing our data, and pass an object satisfying the Tables.jl interface to it.\n\npath joinpath load data path \nIn case our data was present in a different format for eg. parquet, it could be loaded into a data container as follows:\n\nParquet read_parquet parquet_path \n\nmapobs is used here to split our target column from the rest of the row in a lazy manner, so that each observation consists of a row of inputs and a target variable.\n\nsplitdata row row row salary data \nTo create a learning task for tabular classification task, we need an input block, an output block, and the encodings to be performed on the data.\nThe input block here is a TableRow which contains information about the nature of the columns (ie. categorical or continuous) along with an indexable collection mapping categorical column names to a collection with distinct classes in that column. We can get this mapping by using the gettransformationdict task with DataAugmentation.Categorify.\nThe outblock block used is Label for single column classification and the unique classes have to passed to it.\nThis is followed by the encodings which needs to be applied on our input and output blocks. For the input block, we have used the gettransforms function here to get a standard bunch of transformations to apply, but this can be easily customized by passing in any tabular transformation from DataAugmentation.jl or a composition of those, to TabularPreprocessing. In addition to this, we have just one-hot encoded the outblock.\n\ncat cont FastTabular data target salary cat filter isequal target cat catdict FastTabular data cat \ninputblock cat cont catdict targetblock unique data table target task inputblock targetblock inputblock data \nIn case our initial problem wasn't a classification task, and we had a continuous target column, we would need to perform tabular regression. To create a learning task suitable for regression, we use a Continuous block for representing our target column. This can be done even with multiple continuous target columns by just passing the number of columns in Continuous. For example, the task here could be used for 3 targets.\n\ntask2 cat cont catdict data outputblock \n\nTo get an overview of the learning task created, and as a sanity test, we can use describetask. This shows us what encodings will be applied to which blocks, and how the predicted ŷ values are decoded.\n\ntask \ngetobs gets us a row of data from the TableDataset, which we encode here. This gives us a tuple with the input and target. The input here is again a tuple, containing the categorical values (which have been label encoded or \"categorified\") and the continuous values (which have been normalized and any missing values have been filled).\n\nsplitdata \nx task splitdata \nTo get a model suitable for our learning task, we can use taskmodel which constructs a suitable model based on the target block.\n\nmodel task \nOf course you can also create a custom backbone using the functions present in FastAI.Models.\n\ncardinalities collect map col length catdict col cat ovdict Dict workclass education Symbol overrides collect map col col keys ovdict ovdict col nothing cat embedszs FastTabular _get_emb_sz cardinalities overrides catback FastTabular embedszs \nWe can then pass a named tuple (categorical = ..., continuous = ...) to taskmodel to replace the default backbone.\n\nbackbone categorical catback continuous length cont model task backbone \nTo directly get a Learner suitable for our task and data, we can use the tasklearner function. This creates both batched data loaders and a model for us.\n\nlearner task splitdata backbone backbone callbacks \nOnce we have a Learner, we can call fitonecycle! on it to train it for the desired number of epochs:\n\nlearner \n"
    url = "FastAI@pr-288/doc/docs/notebooks/tabularclassification.ipynb"
    [[input.files]]
    title = "FastAI/interpretation/task.jl"
    contents = "backend task sample blocks task task backend blocks sample task sample task sample backend task samples backend task sample samples task samples task samples backend task encsample backend task task encodedsample encsample task encsample task encsample backend task encsamples AbstractVector backend task task encodedsample encsamples backend task batch backend task batch task batch task batch backend task pred backend task pred pred backend task sample pred blocks task backend blocks sample blocks pred sample pred task args task args backend task preds predblock task task ŷ backend predblock preds backend task samples preds predblock task task ŷ backend task predblock collect zip samples preds task args task args backend task output backend task task ŷ output backend task encsample output blocks task backend task blocks encodedsample blocks ŷ encsample output task args task args backend task outputs backend task task ŷ outputs backend task encsamples outputs blocks task backend task blocks encodedsample blocks ŷ collect zip encsamples outputs task args task args backend task outputbatch backend task outputbatch backend task batch outputbatch backend task batch outputbatch task args task args task backend sample task sample output task ŷ context encsample task context sample pred task context output Test task typeof backend backend task sample backend task encsample backend task output backend task encsample output backend task pred backend task sample pred \n"
    url = "FastAI@pr-288/src/interpretation/task.jl"
    [[input.files]]
    title = "LearningTask"
    contents = "abstract type LearningTask\nRepresents a concrete approach for solving a learning task.\nA LearningTask defines how data is processed encoded and decoded before and after going through a model.\nExtending\nIt is recommended to use AbstractBlockTasks like BlockTask and SupervisedTask to construct tasks, but you may subtype LearningTask for lower-level control.\nThere is a core interface that will allow you to train models and perform inference (for supervised tasks). It consists of\nencodesample\n\nencodeinput\n\ndecodeypred\n\n\nYou can optionally implement additional interfaces to get support for higher-level features of the library.\nTraining interface: tasklossfn, taskmodel\n\nTesting interface: mocksample, mockinput, mocktarget, mockmodel\n\nBatching: shouldbatch\n\n\n\n"
    url = "FastAI@pr-288/ref/FastAI.LearningTask"
    [[input.files]]
    title = "getgroup"
    contents = "getgroup"
    url = "FastAI@pr-288/ref/FastAI.getgroup"
    [[input.files]]
    title = "decode"
    contents = "decode"
    url = "FastAI@pr-288/ref/FastAI.decode"
    [[input.files]]
    title = "loadfolderdata"
    contents = "loadfolderdata"
    url = "FastAI@pr-288/ref/FastAI.Datasets.loadfolderdata"
    [[input.files]]
    title = "Inference"
    contents = "Inference"
    url = "FastAI@pr-288/ref/FastAI.Inference"
    [[input.files]]
    title = "FastAI/encodings/onehot.jl"
    contents = "N T classes AbstractVector T T T FastAI Base summary io IO T T print io T block N a AbstractArray T M M N T N M last size a length block classes block block classes rand block classes N T classes AbstractVector T block N a AbstractArray T M M N T N M last size a length block classes block labelblock block classes labelblock labelblock TT Type T TT threshold Float32 Float32 block T T T block classes block block classes _ block obs onehot obs block classes context block obs block classes argmax obs block T T T block classes block block classes enc _ block obs collect enc T c obs c block classes enc _ block obs block classes softmax obs enc threshold outblock yblock outblock classes yblock classes error outblock yblock Losses logitcrossentropy outblock yblock outblock classes yblock classes error outblock yblock Losses logitbinarycrossentropy enc enc enc \n"
    url = "FastAI@pr-288/src/encodings/onehot.jl"
    [[input.files]]
    title = "unbatch"
    contents = "unbatch"
    url = "FastAI@pr-288/ref/FastAI.Datasets.unbatch"
    [[input.files]]
    title = "showsample"
    contents = "showsample([backend], task, sample)\nShow an unprocessed sample for LearningTask task to backend::ShowBackend.\nExamples\ndata blocks loaddataset task data sample data task sample task sample \n\n"
    url = "FastAI@pr-288/ref/FastAI.showsample"
    [[input.files]]
    title = "LabelMulti"
    contents = "LabelMulti(classes)\nsetup(LabelMulti, data)\nBlock for a categorical label in a multi-class context. data is valid for Label(classes) if data ∈ classes.\nExamples\nblock block block \nYou can use setup to create a Label instance from a data container containing possible classes:\ntargets block targets block \n\n"
    url = "FastAI@pr-288/ref/FastAI.LabelMulti"
    [[input.files]]
    title = "assigngroups!"
    contents = "assigngroups!"
    url = "FastAI@pr-288/ref/FastAI.assigngroups!"
    [[input.files]]
    title = "Custom learning tasks"
    contents = "Custom learning tasks\nThis tutorial explains the low-level interface behind BlockTasks and how to use it to create your custom learning tasks without the data block interface.\nIn the quickstart section, you've already seen a learning task in action: BlockTask. The learning task abstraction powers FastAI.jl's high-level interface allowing you to make training models for a task simple. BlockTask is a particularly convenient and composable interface for creating learning tasks and should be preferred for most use cases.\nHowever, to get a look behind the scenes, in this tutorial we'll use the lower-level learning task interface to implement our own version of an image classification learning task. You're encouraged to follow along in a REPL or notebook. This tutorial can also serve as a template for implementing a custom learning task for your own project.\nA learning task describes how we need to process data so we can train a model for some task. In our case, the task we want to solve is to classify an image. The task defines what kind of data we need, here pairs of images and class labels. That alone, however, isn't enough to train a model since we can't just throw an image in any format into a model and get a class out. Almost always the input data needs to be processed in some way before it is input to a model (we call this encoding) and the same goes for the model outputs (we call this decoding).\nSo let's say we have an image and a trained model. How do we make a prediction? First we encode the image, run it through the model, and then decode the output. Similarly, how we can use a pair of image and class to train a model? We encode both, run the encoded input through the model and then compare the output with the encoded class using a loss function. The result tells us how we'll need to update the weights of the model to improve its performance.\nIn essence, the learning task interface allows us to implement these steps and derive useful functionality from it, like training and evaluating models. Later we'll also cover some optional interfaces that allow us to define other parts of a deep learning project.\nDatasets\nBefore we get started, let's load up a data container that we can test our code on as we go. It's always a good idea to interactively test your code! Since we'll be implementing a task for image classification, the observations in our data container will of course have to be pairs of images and classes. We'll use one of the many image classification datasets available from the fastai dataset repository. I'll use ImageNette, but you can use any of the datasets listed in FastAI.Datasets.DATASETS_IMAGECLASSIFICATION. The way the interface is built allows you to easily swap out the dataset you're using.\nFastVision Colors FastVision data load filterfn FastVision loadfn \nWe'll also collect the unique class names:\nimages targets data classes unique targets \nImplementation\nLearning task struct\nNow let's get to it! The first thing we need to do is to create a LearningTask struct. The LearningTask struct should contain all the configuration needed for encoding and decoding the data. We'll keep it simple here and include a list of the classes and the image dimensions input to the model.\nImageClassification classes size \nNow we can create an instance of it, though of course it can't do anything (yet!).\ntask ImageClassification classes \nEncoding and decoding\nThere are 3 tasks we need to define before we can use our learning task to train models and make predictions:\nencodesample which encodes an image and a class\n\nencodeinput will encode an image so it can be input to a model\n\ndecodeypred decodes a model output into a class label\n\n\nNote: These functions always operate on single images and classes, even if we want to pass batches to the model later on.\nWhile it's not the focus of this tutorial, let's give a quick recap of how the data is encoded and decoded for image classification.\nImages are cropped to a common size so they can be batched, converted to a 3D array with dimensions (height, width, color channels) and normalized\n\nClasses are encoded as one-hot vectors, teaching the model to predict a confidence distribution over all classes. To decode a predicted one-hot vector, we can simply find the index with the highest value and look up the class label.\n\n\nEach of the tasks also takes a context::FastAI.Context argument which allows it to behave differently during training, validation and inference. We'll make use of that to choose a different image crop for each situation. During training we'll use a random crop for augmentation, while during validation a center crop will ensure that any metrics we track are the same every epoch. During inference, we won't crop the image so we don't lose any information.\nInputs\nWe implement encodeinput using DataAugmentation.jl. Feel free to look at its documentation, we won't focus on it here.\nFastVision getresizecrop context sz sz getresizecrop context sz sz getresizecrop context sz sz task ImageClassification context image tfm getresizecrop context task size RGB Float32 tfm image \nIf we test this out on an image, it should give us a 3D array of size (128, 128, 3), and indeed it does:\nsample image class data x task image summary x \nOutputs\nencodetarget is much simpler:\ntask ImageClassification class idx findfirst isequal class task classes v Float32 length task classes v idx v task ImageClassification ctx input target task ctx input task ctx target \ny task class \nThe same goes for the decoding step:\ntask ImageClassification ypred task classes argmax ypred \ntask y class \nTraining\nAnd that's all we need to start training models! There are some optional interfaces that make that even easier, but let's use what we have for now.\nWith our LearningTask defined, we can use taskdataloaders to turn a dataset into a set of training and validation data loaders that can be thrown into a training loop.\ntraindl valdl data task \nNow, with a makeshift model, an optimizer and a loss function we can create a Learner.\nmodel FastVision Models xresnet18 length task classes optimizer lossfn logitcrossentropy learner model lossfn data traindl valdl optimizer \nFrom here, you're free to start training using  fit! or fitonecycle!.\nThese tasks are also enough to use predict and predictbatch once you've trained a model.\nAdditional interfaces\nTraining interface\nWe can implement some additional tasks to make our life easier. Specifically, let's implement every task needed to use tasklearner:\ntasklossfn: return a loss function lossfn(ys, ys) comparing a batch of model outputs and encoded targets\n\ntaskmodel: from a backbone, construct a model suitable for the task\n\n\nLet's start with the loss function. We want to compare two one-hot encoded categorical variables, for which categorical cross entropy is the most commonly used loss function.\nFastAI.tasklossfn(task::ImageClassification) = Flux.Losses.logitcrossentropy\nFor the model, we'll assume we're getting a convolutional feature extractor passed in as a backbone so its output will be of size (height, width, channels, batch size). Flux.outputsize can be used to calculate the output size of arbitrary models without having to evaluate the model. We'll use it to check the number of output channels of the backbone. Then we add a global pooling layer and some dense layers on top to get a classification output.\ntask ImageClassification backbone h w outch b backbone inblock nchannels head outch length task classes backbone head \n\n"
    url = "FastAI@pr-288/doc/docs/learning_methods.md"
    [[input.files]]
    title = "PropagateSameWrapper"
    contents = "struct PropagateSameWrapper <: PropagateWrapper end\nPropagate a wrapper type only if the encoded block is the exact same, including any wrappers.\nSee propagate for more information.\n\n"
    url = "FastAI@pr-288/ref/FastAI.PropagateSameWrapper"
    [[input.files]]
    title = "Introduction"
    contents = "Introduction\nThis tutorial explains the quickstart examples and some core abstractions FastAI.jl is built on.\nOn the quickstart page, we showed how to train models on common tasks in a few lines of code like these:\ndata blocks load task blocks learner task data callbacks learner task learner \nEach of the five lines encapsulates one part of the deep learning pipeline to give a high-level API while still allowing customization. Let's have a closer look.\nDataset\ndata blocks load \nThis line downloads and loads the ImageNette image classification dataset, a small subset of ImageNet with 10 different classes. data is a data container that can be used to load individual observations, here of images and the corresponding labels. We can use getobs(data, i) to load the i-th observation and numobs to find out how many observations there are.\nimage class sample data class image \nblocks describe the format of the data that you want to use for learning. For supervised training tasks, they are a tuple of (inputblock, targetblock). Since we want to do image classification, the input block is Image{2}(), representing a 2-dimensional image and the target block is Label(classes), representing the class the image belongs to.\nblocks \nLearning task\ntask blocks \nThe next line defines a learning task which encapsulates the data preprocessing pipeline and other logic related to the task. ImageClassificationSingle is a simple wrapper around BlockTask which takes in blocks and data processing steps, so-called encodings. Using it, we can replace the above line with\ntask classes \nBased on the blocks and encodings, the learning task can derive lots of functionality:\ndata processing\n\nvisualization\n\nconstructing task-specific models from a backbone\n\ncreating a loss function\n\n\nLearner\nlearner task data callbacks \nNext we create a Learner that encapsulates everything needed for training, including:\nparallelized training and validation data loaders using taskdataloaders\n\na loss function using tasklossfn\n\na task-specific model using taskmodel\n\n\nThe customizable, expanded version of the code looks like this:\ndls data task model task Models xresnet18 lossfn task learner model dls lossfn \nAt this step, we can also pass in any number of callbacks to customize the training. Here ToGPU ensures an available GPU is used, and Metrics adds additional metrics to track during training.\nTraining\nlearner \nTraining now is quite simple. You have several options for high-level training schedules:\nlrfind to run a learning rate finder\n\nfinetune! for when you're using a pretrained backbone\n\nfitonecycle! for when you're training a model from scratch\n\n\nVisualization\ntask learner \nFinally, the last line visualizes the predictions of the trained model. It takes some samples from the training data loader, runs them through the model and decodes the outputs. How each piece of data is visualized is also inferred through the blocks in the learning task.\n\n"
    url = "FastAI@pr-288/doc/docs/introduction.md"
    [[input.files]]
    title = "How to train a model"
    contents = "How to train a model\n\nFastVision FastMakie Metalhead CairoMakie \nFinding a learning rate\n\nUsing a good learning rate is important for a balance between model convergence and training speed, but finding one isn't always easy. FastAI.jl includes a learning rate finder that runs a mock training run with increasing learning rates to find a good one. You can use it with lrfind.\n\ndata blocks load task blocks learner task data callbacks finderresult learner \nWe can also use the Makie backend to show the results of the learning rate finder:\n\nFastMakie Makie plot finderresult \nTraining a model from scratch\n\nWhen using randomly intialized models like you can use fitonecycle! to train:\n\nMetalhead data blocks load task blocks backbone Metalhead ResNet layers end learner task data callbacks learner \nFinetuning a pretrained model\n\nWhen finetuning a pretrained model, it is recommended to use finetune! which uses a warmup schedule to train the newly initiliazed head more quickly than the pretrained backbone.\n\ndata blocks loaddataset task blocks learner task data backbone Metalhead ResNet50 pretrain layers end callbacks learner \n"
    url = "FastAI@pr-288/doc/docs/notebooks/training.ipynb"
    [[input.files]]
    title = "learningtasks"
    contents = "learningtasks(; filters...)\nShow a registry of available learning tasks. Pass in filters as keyword arguments to look at a subset.\nSee also finding functionality, datasets, and datarecipes. For more information on registries, see FeatureRegistries.jl.\nExamples\nShow all available learning tasks:\nFastVision \nShow all computer vision tasks:\npackage FastVision \nShow all classification tasks, i.e. where the target block is a Label:\nblocks Any \nGet an explanation of fields in the learning task registry:\ninfo \n\n"
    url = "FastAI@pr-288/ref/FastAI.Registries.learningtasks"
    [[input.files]]
    title = "FastAI/datablock/describe.jl"
    contents = "encodings blocks filledblocks Any blocks changedblocks Any _ blocks encoding encodings push! changedblocks x isnothing x encoding blocks blocks encoding blocks push! filledblocks blocks filledblocks changedblocks encodings blocks filledblocks Any blocks changedblocks Any _ blocks encoding Iterators reverse encodings push! changedblocks x isnothing x encoding blocks blocks encoding blocks push! filledblocks blocks filledblocks changedblocks f args f args f args Vararg Tuple map as f as args encodings block blocks changed encodings block encodings block n length blocks blockscol b c _blockcell b c i bs ch i bs ch zip n blocks changed block Tuple blockscol join row row blockscol reshape blockscol _blockcell block haschanged i haschanged summary block i summary block encodings reshape typeof enc name name enc encodings encodings blocks Tuple inname outname blocknames repeat length blocks markdown tf tf_markdown namescol reshape inname _ length encodings outname data hcat reverse encodings encodings namescol encodings block block blocks s pretty_table String data header blocknames alignment r r l _ length blocknames tf tf markdown Markdown parse s s task blocks task input target x ŷ blocks input blocks target blocks x blocks ŷ encoding task task sample blocknames inname outname s summary input summary target summary x summary ŷ encoding Markdown parse s task blocks task encoding task blocks sample blocknames inname outname s join k summary v k v zip keys blocks values blocks encoding Markdown parse s \n"
    url = "FastAI@pr-288/src/datablock/describe.jl"
    [[input.files]]
    title = "showoutputs"
    contents = "showoutputs([backend], task, outputs)\nshowoutputs([backend], task, encsamples, outputs)\nShow model outputs to backend. If a vector of encoded samples encsamples is also given, show them next to the outputs. Use showoutputbatch to show collated batches of outputs.\n\nshowoutputs(task, learner[; n = 4, context = Validation()])\nRun a trained model in learner on n samples and visualize the outputs.\n\n"
    url = "FastAI@pr-288/ref/FastAI.showoutputs"
    [[input.files]]
    title = "DataDepLoader"
    contents = "struct DataDepLoader(datadep) <: DatasetLoader\nA dataset loader that uses DataDeps.jl to load datasets. The DataDep has to be registered before creating the loader, and will error otherwise.\n\n"
    url = "FastAI@pr-288/ref/FastAI.Datasets.DataDepLoader"
    [[input.files]]
    title = "blocklossfn"
    contents = "blocklossfn(predblock, yblock)\nConstruct a loss function that compares a batch of model outputs ŷs and encoded targets ys and returns a scalar loss.\nFor example for block = OneHotLabel(classes) (i.e. an encoded Label(classes)), we have blocklossfn(block, block) == Flux.Losses.logitcrossentropy.\n\n"
    url = "FastAI@pr-288/ref/FastAI.blocklossfn"
    [[input.files]]
    title = "savetaskmodel"
    contents = "savetaskmodel(path, task, model[; force = false])\nSave a trained model along with a task to path for later inference. Use loadtaskmodel for loading both back into a session. If path already exists, only write to it if force = true.\nIf model weights are on a GPU, they will be moved to the CPU before saving so they can be loaded in a non-GPU environment.\nJLD2.jl is used for serialization.\n\n"
    url = "FastAI@pr-288/ref/FastAI.savetaskmodel"
    [[input.files]]
    title = "DESCRIPTIONS"
    contents = "DESCRIPTIONS"
    url = "FastAI@pr-288/ref/FastAI.Datasets.DESCRIPTIONS"
    [[input.files]]
    title = "Many"
    contents = "Many(block) <: WrapperBlock\nMany indicates that you can variable number of instances for block. Consider a bounding box detection task where there may be any number of targets in an image and this number varies for different samples. The blocks (Image{2}(), BoundingBox{2}() imply that there is exactly one bounding box for every image, which is not the case. Instead you would want to use (Image{2}(), Many(BoundingBox{2}()).\n\n"
    url = "FastAI@pr-288/ref/FastAI.Many"
    [[input.files]]
    title = "decodedblock"
    contents = "decodedblock(encoding, block)\ndecodedblock(encoding, blocks)\ndecodedblock(encodings, blocks)\nReturn the block that is obtained by decoding block with encoding E. This needs to be constant for an instance of E, so it cannot depend on the sample or on randomness. The default is to return nothing, meaning the same block is returned and not changed. Encodings that return the same block but change the data when decoding should return block.\n\n"
    url = "FastAI@pr-288/ref/FastAI.decodedblock"
    [[input.files]]
    title = "withfields"
    contents = "withfields(f, x; kwargs...)\nReplace fields on x with given keyword arguments, run f and then restore the fields. x needs to be a mutable struct.\nEvery keyword argument is a mapping (field, value) or (field, (setfn!, value)). setfn!(x, val) will be used to set the field; if as in the first case none is given setfield! is used.\n\n"
    url = "FastAI@pr-288/ref/FastAI.withfields"
    [[input.files]]
    title = "ShowText"
    contents = "ShowText([io; kwargs...]) <: ShowBackend\nA backend for showing block data using text for REPL use. Text is displayed to io and kwargs are keyword arguments for PrettyTables.pretty_table, which is used to display collections of blocks.\n\n"
    url = "FastAI@pr-288/ref/FastAI.ShowText"
    [[input.files]]
    title = "DATASETS"
    contents = "DATASETS"
    url = "FastAI@pr-288/ref/FastAI.Registries.DATASETS"
    [[input.files]]
    title = "FastAI/datasets/recipe.jl"
    contents = "R R R recipe path AbstractPath data blocks recipe path recipe data blocks recipe data blocks blocks recipe blocks data \n"
    url = "FastAI@pr-288/src/datasets/recipe.jl"
    [[input.files]]
    title = "smoothvalues"
    contents = "smoothvalues(xs, β)\nApply exponential smoothing with parameter β to vector xs.\n\n"
    url = "FastAI@pr-288/ref/FastAI.smoothvalues"
    [[input.files]]
    title = "initdatadeps"
    contents = "initdatadeps"
    url = "FastAI@pr-288/ref/FastAI.Datasets.initdatadeps"
    [[input.files]]
    title = "encodeinput"
    contents = "encodeinput"
    url = "FastAI@pr-288/ref/FastAI.encodeinput"
    [[input.files]]
    title = "How to find functionality"
    contents = "How to find functionality\nFor some kinds of functionality, FastAI.jl provides feature registries that allow you to search for and use features. The following registries currently exist:\ndatasets to download and unpack datasets,\n\ndatarecipes to load datasets into data containers that are compatible with a learning task; and\n\nlearningtasks to find learning tasks that are compatible with a dataset\n\n\nDomain packages\nFunctionality is registered by domain packages such as FastVision and FastTabular. You need to import the respective packages to be able to find their functionality in their registry.\n\n\nTo load functionality:\nGet an entry using its ID\nFastVision entry \n\nAnd load it\nload entry \n\nDatasets\n\nData recipes\n\nLearning tasks\n\n\n"
    url = "FastAI@pr-288/doc/docs/howto/findfunctionality.md"
    [[input.files]]
    title = "blockcolumn"
    contents = "blockcolumn"
    url = "FastAI@pr-288/ref/FastAI.blockcolumn"
    [[input.files]]
    title = "FastAI/Registries/models.jl"
    contents = "\n"
    url = "FastAI@pr-288/src/Registries/models.jl"
    [[input.files]]
    title = "describetask"
    contents = "describetask"
    url = "FastAI@pr-288/ref/FastAI.describetask"
    [[input.files]]
    title = "checktask_core"
    contents = "checktask_core(task; sample, model, device = identity)\nchecktask_core(task; device = identity)\nCheck if task conforms to the LearningTask interface. sample and model are used for testing. If you have implemented the testing interface and don't supply these as arguments, mocksample(task) and mockmodel(task) will be used.\n\n"
    url = "FastAI@pr-288/ref/FastAI.checktask_core"
    [[input.files]]
    title = "FastAI/datasets/loaders.jl"
    contents = "datadep String datadep haskey DataDeps registry datadep throw ArgumentError datadep new datadep loader isnothing DataDeps try_determine_load_path loader datadep loader DataDeps resolve loader datadep loader loader loader DataDeps resolve loader datadep \n"
    url = "FastAI@pr-288/src/datasets/loaders.jl"
    [[input.files]]
    title = "Image segmentation"
    contents = "Image segmentation\n\nIn the quickstart section, you saw a short example of how to train an image segmentation model. In this tutorial, we'll recreate that using the mid-level APIs.\n\nFastVision Metalhead CairoMakie CairoMakie activate! type \nIn image segmentation, instead of assigning a class to a whole image as in image classification, we want to classify each pixel of an image. We'll use the CamVid dataset which contains street images with every pixel annotated as one of 32 classes.\n\nInside the dataset folder, we find images/, a subfolder with all the input images, labels/, a subfolder containing the segmentation masks (saved as images) and codes.txt which contains the class names:\n\ndir load readdir dir \nEvery line in codes.txt corresponds to one class, so let's load it:\n\nclasses readlines open joinpath dir \nTo create our data containers we'll use loadfolderdata, which creates data containers from folder contents. The function loadfn is applied to the file paths and we use loadfile to loadmask to get images and masks.\n\nimages joinpath dir filterfn FastVision loadfn masks joinpath dir filterfn FastVision loadfn f FastVision f classes data images masks \nNow we can get an observation:\n\nimage mask sample data \nimage \nEach mask is an array with elements the same type as our classes (here String), but efficiently stored using integers under the hood.\n\nview mask \nNext we need to create a learning task for image segmentation. This means using images to predict masks, so we'll use the Image and Mask blocks as input and target. Since the dataset is 2D, we'll use 2-dimensional blocks.\n\ntask classes \nThe encodings passed in transform samples into formats suitable as inputs and outputs for a model, and also allow decoding model outputs to get back to our target format: an array of class labels for every pixel.\n\nLet's check that samples from the created data container conform to the blocks of the learning task:\n\ntask blocks sample sample \nWe can ascertain the encodings work as expected by creating a batch of encoded data and visualizing it. Here the segmentation masks are color-coded and overlayed on top of the image.\n\nxs ys task data task xs ys \nWe can use describetask to get more information about learning tasks created through the data block API. We see which representations our data goes through and which encodings transform which parts.\n\ntask \nWith a task and a matching data container, the only thing we need before we can create a Learner is a backbone architecture to build the segmentation model from. We'll use a slightly modified ResNet, but you can use any convolutional architecture.\nWe'll use taskmodel to construct the model from the backbone. Since we want mask outputs, the intermediate feature representation needs to be scaled back up. Based on the Blocks we built our task from, taskmodel knows that it needs to build a mapping ImageTensor{2} -> OneHotTensor{2} and constructs a U-Net model.\n\nbackbone Metalhead ResNet layers end model task backbone \nIn a similar vein, tasklossfn creates a loss function suitable for comparing model outputs and encoded targets.\n\nlossfn task \nNext we turn our data container into training and validation data loaders that will iterate over batches of encoded data and construct a Learner.\n\ntraindl validdl data task optimizer learner model lossfn data traindl validdl optimizer callbacks \nNote that we could also have used tasklearner which is a shorthand that calls taskdataloaders and taskmodel for us.\n\nNow let's train the model:\n\nlearner \nAnd look at the results on a batch of validation data:\n\ntask learner n \n\nThe data block API can also be used for 3D segmentation, but the tutorial for that is still in the works.\n\n"
    url = "FastAI@pr-288/doc/docs/notebooks/imagesegmentation.ipynb"
    [[input.files]]
    title = "PropagateNever"
    contents = "struct PropagateNever <: PropagateWrapper end\nNever propagate a wrapper type.\nSee propagate for more information.\n\n"
    url = "FastAI@pr-288/ref/FastAI.PropagateNever"
    [[input.files]]
    title = "recipeblocks"
    contents = "recipeblocks(TRecipe) -> TBlocks\nrecipeblocks(recipe) -> TBlocks\nReturn the Block types for the data container that recipe type TRecipe creates. Does not return Block instances as the exact configuration may not be known until the dataset is being loaded.\nExamples\nrecipeblocks Tuple \n\n"
    url = "FastAI@pr-288/ref/FastAI.Datasets.recipeblocks"
    [[input.files]]
    title = "Discovery"
    contents = "Discovery\nAs you may have seen in the introduction, FastAI.jl makes it possible to train models in just 5 lines of code. However, if you have a task in mind, you need to know what datasets you can train on and if there are convenience learning task constructors. For example, the introduction loads the \"imagenette2-160\" dataset and uses ImageClassificationSingle to construct a learning task. Now what if, instead of classifying an image into one class, we want to classify every single pixel into a class (semantic segmentation)? Now we need a dataset with pixel-level annotations and a learning task that can process those segmentation masks.\nFor finding both, we can make use of Blocks. A Block represents a kind of data, for example images, labels or keypoints. For supervised learning tasks, we have an input block and a target block. If we wanted to classify whether 2D images contain a cat or a dog, we could use the blocks (Image{2}(), Label([\"cat\", \"dog\"])), while for semantic segmentation, we'll have an input Image block and a target Mask block.\nFinding a dataset\nTo find a dataset with compatible samples, we can pass the types of these blocks as a filter to datasets which will show us only dataset recipes for loading those blocks.\nFastVision blocks \nWe can see that the \"camvid_tiny\" dataset can be loaded so that each sample is a pair of an image and a segmentation mask. Let's use a data recipe to load a data container and concrete blocks.\nENV \ndata blocks load findfirst id blocks \nAs with every data container, we can load a sample using getobs which gives us a tuple of an image and a segmentation mask.\nimage mask sample data size sample eltype sample \nLoading the dataset recipe also returned blocks, which are the concrete [Block] instances for the dataset. We passed in types of blocks ((Image, Mask)) and get back instances since the specifics of some blocks depend on the dataset. For example, the returned target block carries the labels for every class that a pixel can belong to.\ninputblock targetblock blocks targetblock \nWith these blocks, we can also validate a sample of data using checkblock which is useful as a sanity check when using custom data containers.\ninputblock targetblock image mask \nSummary\nIn short, if you have a learning task in mind and want to load a dataset for that task, then\ndefine the types of input and target block, e.g. blocktypes = (Image, Label),\n\nuse filter(datarecipes(), blocks=blocktypes) to find compatbile dataset recipes; and\n\nrun load(datarecipes()[id]) to load a data container and the concrete blocks\n\nExercises\nFind and load a dataset for multi-label image classification. (Hint: the block for multi-category outputs is called LabelMulti).\n\nList all datasets with Image as input block and any target block. (Hint: the supertype of all types is Any)\n\nFinding a learning task\nArmed with a dataset, we can go to the next step: creating a learning task. Since we already have blocks defined, this amounts to defining the encodings that are applied to the data before it is used in training. Here, FastAI.jl already defines some convenient constructors for learning tasks and you can find them with learningtasks. Here we can pass in either block types as above or the block instances:\nblocks blocks \nLooks like we can use the ImageSegmentation function to create a learning task. Every function returned can be called with blocks and, optionally, some keyword arguments for customization.\ntask blocks size \nAnd that's the basic workflow for getting started with a supervised task.\nExercises\nFind all learning task functions with images as inputs.\n\n\n"
    url = "FastAI@pr-288/doc/docs/discovery.md"
    [[input.files]]
    title = "lrfindtextplot!"
    contents = "lrfindtextplot!"
    url = "FastAI@pr-288/ref/FastAI.lrfindtextplot!"
    [[input.files]]
    title = "FastAI/encodings/only.jl"
    contents = "Name B block B name Symbol block B B name B block named name block name name block E fn Any encoding E name Symbol encoding name encoding B Type encoding block block B encoding only block only fn block only encoding block nothing only block only fn block only encoding block nothing only block only fn block only encoding block nothing only block inblock only encoding block isnothing inblock nothing only fn inblock nothing inblock only block inblock only encoding block isnothing inblock nothing only fn inblock nothing inblock only block inblock only encoding block isnothing inblock nothing only fn inblock nothing inblock only args only encoding args only args only encoding args only ctx block obs kwargs _encode only ctx block obs kwargs only ctx block obs kwargs _encode only ctx block obs kwargs _encode only ctx block obs kwargs only fn block only encoding ctx block obs kwargs obs only ctx block obs kwargs _decode only ctx block obs kwargs only ctx block obs kwargs _decode only ctx block obs kwargs _decode only ctx block obs kwargs only fn only encoding block only encoding ctx block obs kwargs obs InlineTest encx x inblock inblocknamed x inblock obs inblock encobs inblock obs encx inblock nothing encx inblocknamed x inblock nothing inblocknamed x outblock inblock outblocknamed inblocknamed encx outblock nothing encx outblocknamed x outblock nothing outblocknamed x encx inblock obs obs encx inblocknamed obs obs encx outblock encobs encobs encx outblocknamed encobs encobs tfm only name tfm block name obs block only block obs only block block obs obs only block Nothing \n"
    url = "FastAI@pr-288/src/encodings/only.jl"
    [[input.files]]
    title = "FastAI/datablock/block.jl"
    contents = "obs blocks Tuple obss Tuple length blocks length obss all block obs block obs zip blocks obss blocks Tuple map blocks T Type T t Tuple Tuple map t block FastAI typeof block block string nameof typeof block blocks Tuple join map blocks \n"
    url = "FastAI@pr-288/src/datablock/block.jl"
    [[input.files]]
    title = "FastAI/datablock/task.jl"
    contents = "task task blocks task task encodings task context sample task context task sample sample task context input task context task input input task context target task context task target target task context encodedsample task context task encodedsample encodedsample task context ŷ task context task ŷ ŷ task context y task context task y y task backbone task x task ŷ backbone task backbone task x task x task ŷ backbone task task ŷ task y task task sample task name Symbol task name task task x task ŷ _ ŷblock mockmodel_block xs batch ŷblock _ xs B NamedTuple E blocks B encodings E Base show io IO task print io keys task B NamedTuple E blocks B encodings E blocks Tuple Any Any encodings ŷblock nothing sample input target blocks x y encodedsample encodings sample ŷ isnothing ŷblock y ŷblock pred encodings ŷ blocks input target sample encodedsample x y ŷ pred blocks encodings Base show io IO task print io summary task input summary task target Base blocks Tuple Any Any encodings kwargs blocks encodings kwargs \n"
    url = "FastAI@pr-288/src/datablock/task.jl"
    [[input.files]]
    title = "decodedblockfilled"
    contents = "decodedblockfilled"
    url = "FastAI@pr-288/ref/FastAI.decodedblockfilled"
    [[input.files]]
    title = "decodey"
    contents = "decodey"
    url = "FastAI@pr-288/ref/FastAI.decodey"
    [[input.files]]
    title = "Training"
    contents = "Training <: Context\nA context for applying data transformations during training. Encodings and LearningTasks can dispatch on this when certain transformations, like random augmentations, should only be applied during training.\n\n"
    url = "FastAI@pr-288/ref/FastAI.Training"
    [[input.files]]
    title = "FastAI/serialization.jl"
    contents = "path task model force force isfile path error path jldsave string path model model task task path isfile path error path task model jldopen string path f f f task model \n"
    url = "FastAI@pr-288/src/serialization.jl"
    [[input.files]]
    title = "Blocks and encodings"
    contents = "Blocks and encodings\nUnstructured notes on blocks and encodings\nBlocks\nA block describes the meaning of a piece of data in the context of a learning task.\n\nFor example, for supervised learning tasks, there is an input block and a target block and we want to learn to predict targets from inputs. Learning to predict a cat/dog label (Label([\"cat\", \"dog\"])) from 2D images (Image{2}()) is a supervised image classification task.\n\nA block is not a piece of data itself. Instead it describes the meaning of a piece of data in a context. That a piece of data is a block can be checked using [checkblock](block, data). A piece of data for the Label block above needs to be one of the labels, so checkblock(Label([\"cat\", \"dog\"]), \"cat\") == true, but checkblock(Label([\"cat\", \"dog\"]), \"cat\") == false.\n\nWe can say that a data container is compatible with a learning task if every observation in it is a valid sample of the sample block of the learning task. The sample block for supervised tasks is sampleblock = (inputblock, targetblock) so sample = getobs(data, i) from a compatible data container implies that checkblock(sampleblock, sample). This also means that any data stored in blocks must not depend on individual samples; we can store the names of possible classes inside the Label block because they are the same across the whole dataset.\n\n\nData pipelines\nWe can use blocks to formalize the data processing pipeline.\nDuring training we want to create pairs of data (x, y) s.t. output = model(x) and loss = lossfn(output, y). In terms of blocks that means model is a function (x,) -> output and the loss function maps (outputblock, yblock) -> loss. Usually, (input, target) != (x, y) and instead we have an encoding step that transforms a sample into representations suitable to train a model on, i.e. encode :: sample -> (x, y).\nFor the above image classification example we have sampleblock = (Image{2}(), Label([\"cat\", \"dog\"])) but we cannot put raw images into a model and get out a class. Instead, the image is converted to an array that includes the color dimension and its values are normalized; and the class label is one-hot encoded. So xblock = ImageTensor{2}() and yblock = OneHotTensor{0}. Hence to do training, we need a sample encoding function (Image{2}, Label) -> (ImageTensor{2}, OneHotTensor{0})\n\n\nDuring inference, we have an input and want to use a trained model to predict a target, i.e. input -> target. The model is again a mapping xblock -> outputblock, so we can build the transformation with an encoding step that encodes the input and a decoding step that takes the model output back into a target.\nThis gives us\n(predict :: input -> target) = decodeoutput ∘ model ∘ encodeinput where\n(encodeinput :: input -> x)\n\n(model :: x -> y)\n\n(decodeoutput :: y -> target)\n\n\n\nIn the classification example we have, written in blocks, predict :: Image{2} -> Label and hence encodeinput :: Image{2} -> ImageTensor{2} and decodeoutput :: OneHotTensor{0} -> Label\n\n\nWhere do we draw the line between model and data processing? In general, the encoding and decoding steps are non-learnable transformations, while the model is a learnable transformation.\nEncodings\nEncodings are reversible transformations that model the non-learnable parts (encoding and decoding) of the data pipeline.\n\nWhat an encoding does depends on what block is passed in. Most encodings only transform specific blocks. For example, the ImagePreprocessing encoding maps blocks Image{N} -> ImageTensor{N}, but leaves other blocks unchanged. Encodings are called with encode and decode which take in the block and the data. The actual encoding and decoding takes in an additional context argument which can be specialized on to implement different behavior for e.g. training and validation.\nFastVision FastVision RGB enc data rand RGB summary data encdata enc data summary encdata data_ enc encdata \n\nUsing an encoding to encode and then decode must be block-preserving, i.e. if, for an encoding, encode :: Block1 -> Block2 then decode :: Block2 -> Block1. To see the resulting block of applying an encoding to a block, we can use encodedblock and decodedblock.\nenc enc enc enc enc \nYou can use testencoding to test these invariants to make sure an encoding is implemented properly for a specific block.\nenc \n\nThe default implementations of encodedblock and decodedblock is to return nothing indicating that it doesn't transform the data. This is overwritten for blocks for which encode and decode are implemented to indicate that the data is transformed. Using encodedblockfilled(block, data) will replace returned nothings with the unchanged block.\nenc nothing \nenc \n\nEncodings can be applied to tuples of blocks. The default behavior is to apply the encoding to each block separately.\nenc \n\nApplying a tuple of encodings will encode the data by applying one encoding after the other. When decoding, the order is reversed.\n\n\nBlock learning tasks\nBlockTask creates a learning task from blocks and encodings. You define the sample block (recall for supervised tasks this is a tuple of input and target) and a sequence of encodings that are applied to all blocks.\nThe below example defines the same learning task as ImageClassificationSingle does. The first two encodings only change Image, and the last changes only Label, so it's simple to understand.\ntask \nNow encode expects a sample and just runs the encodings over that, giving us an encoded input x and an encoded target y.\ndata joinpath load filterfn loadfn sample data x y task sample summary x summary y \nThis is equivalent to:\nx y task encodings task sample sample summary x summary y \nImage segmentation looks almost the same except we use a Mask block as target. We're also using OneHot here, because it also has an encode task for Masks. For this task, ProjectiveTransforms will be applied to both the Image and the Mask, using the same random state for cropping and augmentation.\ntask \nThe easiest way to understand how encodings are applied to each block is to use describetask and describeencodings which print a table of how each encoding is applied successively to each block. Rows where a block is bolded indicate that the data was transformed by that encoding.\ntask \nThe above tables make it clear what happens during training (\"encoding a sample\") and inference (encoding an input and \"decoding an output\"). The more general form describeencodings takes in encodings and blocks directly and can be useful for building an understanding of how encodings apply to some blocks.\ntask encodings \n\nNotes\nSince most encodings just operate on a small number of blocks and keep the rest unchanged, applying them to all blocks is usually not a problem. When it is because you want some encoding to apply to a specific block only, you can use Named and Only to get around it.\n\n\n\n"
    url = "FastAI@pr-288/doc/docs/background/blocksencodings.md"
    [[input.files]]
    title = "decodeypred"
    contents = "decodeypred"
    url = "FastAI@pr-288/ref/FastAI.decodeypred"
    [[input.files]]
    title = "Presizing vision datasets for performance"
    contents = "Presizing vision datasets for performance\n\nIn this tutorial, we'll look at how we can improve the performance of computer vision data pipelines by presizing. Presizing is a dataset preprocessing step executed before any training in which all images are loaded and saved at a smaller size. This can improve performance if image loading is a bottleneck.\nPresizing is useful when the original image sizes in a dataset are much larger than the size we use during training; if the images are downsized every time they are loaded anyway, a lot of work can be avoided by doing this once before training. For example, if we train an image classification model on resized image crops of size (160, 160) then we can presize every image so that the shorter length is at least 160.\n\nLet's first look at the performance difference at this makes. Conveniently, the ImageNette dataset, comes in 3 sizes: original, 320px and 160px. Let's download them and create data containers that load the images and do nothing else:\n\np_160 load p_320 load p_orig load \nimagedatacontainer dir dir data_160 imagedatacontainer p_160 data_320 imagedatacontainer p_320 data_orig imagedatacontainer p_orig \nNow every observation is simply a single image, but the sizes vary:\n\nMosaicViews mosaicview data_160 data_320 data_orig nrow \nLet's see how long it takes to load a random subset of 100 images:\n\nidxs rand data_160 name data zip data_160 data_320 data_orig println name i idxs data i \nQuite a difference! The 320px version loads about 4 times slower than the 160px version; after all there are 4 times as many pixels.\nNote that image loading is only a part of the data pipeline. Optimizing it with presizing only makes sense if it becomes a bottleneck. See Performant data pipelines for a more general discussion.\n\nImplementing presizing\n\nNext we'll look at how to do presizing ourselves. For an image classification dataset, this entails copying the folder structure but replacing every image with a downscaled version. Let's say, as above, we want to train a model on images of size (160, 160). Since we still want to use random crops during training, we don't want to do the cropping yet. Instead we downscale the image while preserving the aspect ratio so that the smallest side is still at least 160 pixels long.\n\nThe following function does just that for a single image:\n\nImages presizeimage image sz ratio maximum sz size image newsz round Int size image ratio σ ratio ratio k KernelFactors gaussian σ imresize imfilter image k NA newsz \nSZ image data_orig presizeimage image SZ \nNow we need to run this over every image in a folder. To speed things up, we run this in parallel using Threads.@threads.\n\nFilePathsBase DSTDIR Path mktempdir presizeimagedir srcdir dstdir sz pathdata srcdir i pathdata mkpath parent pathdata i Threads i pathdata srcp pathdata i p relpath srcp srcdir dstp joinpath dstdir p img srcp img_presized presizeimage img sz save string dstp img_presized \npresizeimagedir p_orig DSTDIR SZ \nWe can now load the created dataset as a regular image classification dataset:\n\ndata loadtaskdata DSTDIR ImageClassification \nRemarks\n\nKeypoint and segmentation data: Presizing can of course be useful with other image datasets like those for segmantic segmentation and with keypoint data. You have to be more careful when presizing those, though, since the target variable is affected by the resizing: if an image is downsized, then any segmentation masks and keypoints on it also need to be downsized.\n\nProgressive resizing: Presizing can also be used in conjunction with progressive resizing, a technique pioneered by Jeremy Howard, where the training starts with small image sizes for speed and uses larger image sizes later for better performance. This can improve convergence speed quite a bit.\n\n\n\n"
    url = "FastAI@pr-288/doc/docs/notebooks/presizing.ipynb"
    [[input.files]]
    title = "pathparent"
    contents = "pathparent"
    url = "FastAI@pr-288/ref/FastAI.Datasets.pathparent"
    [[input.files]]
    title = "fillblock"
    contents = "fillblock(inblocks, outblocks)\nReplaces all nothings in outblocks with the corresponding block in inblocks. outblocks may be obtained by\n\n"
    url = "FastAI@pr-288/ref/FastAI.fillblock"
    [[input.files]]
    title = "showblocks"
    contents = "showblocks([backend], block, obss)\nshowblocks!(handle, backend, block, obss)\nShow a vector of observations obss of the same block type.\nExamples\ndata blocks loaddataset samples data i i range blocks samples \nExtending\nThis is used for showing batches of observations, unlike the Tuple variant of showblock! which assumes an observation consists of multiple blocks.\nUsually, a ShowBackend will show an observation in one row with showblock! and showblocks! will show multiple rows.\n\n"
    url = "FastAI@pr-288/ref/FastAI.showblocks"
    [[input.files]]
    title = "FastAI/interpretation/makie.jl"
    contents = "size Tuple Int Int kwargs Any sz kwargs sz kwargs \n"
    url = "FastAI@pr-288/src/interpretation/makie.jl"
    [[input.files]]
    title = "matches"
    contents = "matches"
    url = "FastAI@pr-288/ref/FastAI.Datasets.matches"
    [[input.files]]
    title = "Only"
    contents = "Only(fn, encoding)\nOnly(BlockType, encoding)\nOnly(name, encoding)\nWrapper that applies the wrapped encoding to a block if fn(block) === true. Instead of a function you can also pass in a type of block BlockType or the name of a Named block.\n\n"
    url = "FastAI@pr-288/ref/FastAI.Only"
    [[input.files]]
    title = "getblocks"
    contents = "getblocks"
    url = "FastAI@pr-288/ref/FastAI.getblocks"
    [[input.files]]
    title = "mockblock"
    contents = "mockblock(block)\nmockblock(blocks)\nRandomly generate an instance of block. It always holds that checkblock(block, mockblock(block)) === true.\n\n"
    url = "FastAI@pr-288/ref/FastAI.mockblock"
    [[input.files]]
    title = "blocktypesmatch"
    contents = "blocktypesmatch"
    url = "FastAI@pr-288/ref/FastAI.Registries.blocktypesmatch"
    [[input.files]]
    title = "showblock"
    contents = "showblock([backend], block, obs)\nshowblock([backend], blocks, obss)\nshowblock([backend], title => block, obs)\nShow a block or blocks of obs to backend <: ShowBackend.\nblock can be a Block, a tuple of blocks, or a Pair of title => block.\n\n"
    url = "FastAI@pr-288/ref/FastAI.showblock"
    [[input.files]]
    title = "decodeŷ!"
    contents = "decodeŷ!"
    url = "FastAI@pr-288/ref/FastAI.decodeŷ!"
    [[input.files]]
    title = "How to visualize data"
    contents = "How to visualize data\n\nVisualizing the data we're working with is indispensible both to check that data pipelines are set up correctly and to check the predictions of a trained model. For visualization, the Makie.jl plotting package is used which requires you to install a plotting backend. Learning tasks define how the data is visualized, allowing you to use the following functions for visualization:\n\nshowsample (showsamples): Visualize an unprocessed sample (usually a pair of inputs and targets) or a vector of samples.\n\nshowencodedsample (showbatch): Visualize processed model input and output x, y or a batch of xs and ys.\n\nshowprediction  (showpredictions): Compare a model output with the ground truth.\n\n\n\nTo add support for these to a learning task, you have to implement the plotting interface for a block: showblock!.\n\n\nLet's look at an example using the Cat/Dog classifier from the saving and loading tutorial.\n\nCairoMakie CairoMakie activate! type task model dir joinpath load data dir filterfn loadfn \nFirst we load a vector of unprocessed samples, a batch of training data and the corresponding model outputs:\n\nidxs rand data samples data i i idxs xs ys task data idxs ŷs model xs \nThen we can visualize the data with the functions listed above:\n\ntask samples \ntask xs ys \ntask xs ŷs ys \n"
    url = "FastAI@pr-288/doc/docs/notebooks/how_to_visualize.ipynb"
    [[input.files]]
    title = "describeencodings"
    contents = "describeencodings"
    url = "FastAI@pr-288/ref/FastAI.describeencodings"
    [[input.files]]
    title = "CONTEXTS"
    contents = "CONTEXTS"
    url = "FastAI@pr-288/ref/FastAI.CONTEXTS"
    [[input.files]]
    title = "encodesample!"
    contents = "encodesample!"
    url = "FastAI@pr-288/ref/FastAI.encodesample!"
    [[input.files]]
    title = "taskmodel"
    contents = "taskmodel(task, backbone)\nConstruct a model for task from a backbone architecture, for example by attaching a task-specific head model.\n\n"
    url = "FastAI@pr-288/ref/FastAI.taskmodel"
    [[input.files]]
    title = "Registries"
    contents = "Registries"
    url = "FastAI@pr-288/ref/FastAI.Registries"
    [[input.files]]
    title = "blockmodel"
    contents = "blockmodel(inblock, outblock[, backbone = blockbackbone(inblock)])\nFrom a backbone model, construct a model suitable for learning a mapping from inblock to outblock.\n\n"
    url = "FastAI@pr-288/ref/FastAI.blockmodel"
    [[input.files]]
    title = "showblock!"
    contents = "showblock!(handle, backend, block, obs)\nshowblock!(handle, backend, blocks, obss)\nshowblock!(handle, backend, title => block, obs)\nShow block of data to an existing context handle using backend.\nSee showblock for examples.\nExtending\nEvery ShowBackend should implement the following versions of this method:\nshowblock!(handle, backend, block::Block, obs) to show a single block of obs; should be implemented for every block type you want to show\n\nshowblock!(handle, backend, blocks::Tuple, obss::Tuple) to show several blocks that belong to the same observation.\n\n\nOptionally, you can also implement\nshowblock!(handle, backend, pair::Pair, obs) where (title, block) = pair gives the name for a block. If this is not implemented for a backend, then calling it will default to the untitled method.\n\n\n\n"
    url = "FastAI@pr-288/ref/FastAI.showblock!"
    [[input.files]]
    title = "loadrecipe"
    contents = "loadrecipe(recipe, path)\nLoad a recipe from a path. Return a data container data and concrete blocks.\n\n"
    url = "FastAI@pr-288/ref/FastAI.Datasets.loadrecipe"
    [[input.files]]
    title = "decodeŷ"
    contents = "decodeŷ"
    url = "FastAI@pr-288/ref/FastAI.decodeŷ"
    [[input.files]]
    title = "parentname"
    contents = "parentname"
    url = "FastAI@pr-288/ref/FastAI.Datasets.parentname"
    [[input.files]]
    title = "Contributor guide for FastAI.jl"
    contents = "Contributor guide for FastAI.jl\nFirst off, thank you for considering contributing to FastAI.jl. We welcome contributions and are happy to work with you!\nFastAI.jl is part of the FluxML GitHub organization and follows the same guidelines laid out in Flux.jl's CONTRIBUTING.md. That guide also includes a lot of tips for first-time contributors to open source.\nExample contributions\nThe list below just gives a few examples of welcome contributions, but of course, you can always open an issue to discuss other kinds of contributions.\nBug reports: If you encounter an error and think it is due to a bug in FastAI.jl, open an issue with a bug report as explained here: How to file a bug report\n\nFeatures: There are many kinds of features that you can contribute to FastAI.jl, like datasets, models, recipes and tasks. In most cases, it makes sense to open an issue first to discuss the scope and implementation of the feature.\n\nExamples: We're always happy about more usage examples for the documentation. A good starting point for this can be an existing tutorial in another language like Python that you're recreating using FastAI.jl and the Julia ecosystem.\n\n\nTo get started with code or documentation contributions, please see DEVELOPING.md for instructions on setting up a local dev environment and runnning the tests as well as the documentation.\n\n"
    url = "FastAI@pr-288/doc/CONTRIBUTING.md"
    [[input.files]]
    title = "defaultgrouper"
    contents = "defaultgrouper"
    url = "FastAI@pr-288/ref/FastAI.defaultgrouper"
    [[input.files]]
    title = "OneHotLabel"
    contents = "OneHotLabel"
    url = "FastAI@pr-288/ref/FastAI.OneHotLabel"
    [[input.files]]
    title = "listdecodeblocks"
    contents = "listdecodeblocks"
    url = "FastAI@pr-288/ref/FastAI.listdecodeblocks"
    [[input.files]]
    title = "TASKS"
    contents = "TASKS"
    url = "FastAI@pr-288/ref/FastAI.Registries.TASKS"
    [[input.files]]
    title = "decodeypred!"
    contents = "decodeypred!"
    url = "FastAI@pr-288/ref/FastAI.decodeypred!"
    [[input.files]]
    title = "FastAI/training/utils.jl"
    contents = "learner phase schedules Vararg Pair haskey learner cbstate history offset length learner data training learner cbstate history phase epochs offset schedules Tuple HP schedule offset HP schedule schedules scheduler schedules oldscheduler replacecallback! learner scheduler oldscheduler f x kwargs values Dict Symbol Any field value kwargs value Tuple setfn! val value setfn! obj val setfield! obj field val val value values field getfield x field setfn! x val f e rethrow e field value kwargs value Tuple setfn! val value setfn! obj val setfield! obj field val val value setfn! x values field f learner callbacks origcallbacks replacecallback! learner cb cb callbacks f e rethrow e i cb enumerate origcallbacks isnothing cb removecallback! learner typeof callbacks i replacecallback! learner cb task data idxs context xys deepcopy task context data i i idxs batch xys \n"
    url = "FastAI@pr-288/src/training/utils.jl"
    [[input.files]]
    title = "MinDivByTen"
    contents = "MinDivByTen <: LREstimator\nEstimate the optimal learning rate to be value at the minimum loss divided by 10.\n\n"
    url = "FastAI@pr-288/ref/FastAI.MinDivByTen"
    [[input.files]]
    title = "predict"
    contents = "predict(task, model, input[; device, context])\nPredict a target from input using model. Optionally apply function device to x before passing to model and use context instead of the default context Inference.\n\n"
    url = "FastAI@pr-288/ref/FastAI.predict"
    [[input.files]]
    title = "FastAI/training/metrics.jl"
    contents = "ŷs ys thresh mean sigmoid ŷs thresh ys \n"
    url = "FastAI@pr-288/src/training/metrics.jl"
    [[input.files]]
    title = "showencodedsample"
    contents = "showencodedsample([backend], task, encsample)\nShow an encoded sample encsample to backend.\n\n"
    url = "FastAI@pr-288/ref/FastAI.showencodedsample"
    [[input.files]]
    title = "IndexGrouper"
    contents = "IndexGrouper"
    url = "FastAI@pr-288/ref/FastAI.IndexGrouper"
    [[input.files]]
    title = "ParamGroups"
    contents = "ParamGroups(grouper, m)\nA logical grouping of parameters in m created by ParamGrouper grouper. Parameters in m are assigned a group that can be queried using getgroup(paramgroups, param). If a parameter is not assigned a group, getgroup returns nothing.\nExamples\nmodel paramgroups model paramgroups model weight paramgroups model weight paramgroups rand nothing \n\n"
    url = "FastAI@pr-288/ref/FastAI.ParamGroups"
    [[input.files]]
    title = "ParamGrouper"
    contents = "ParamGrouper"
    url = "FastAI@pr-288/ref/FastAI.ParamGrouper"
    [[input.files]]
    title = "tasklearner"
    contents = "tasklearner(task, traindata, validdata[; callbacks=[], kwargs...]) -> Learner\ntasklearner(task, data; pctgval = 0.2, kwargs...)\nCreate a Learner to train a model for learning task task using data.\nKeyword arguments\ncallbacks = []: Callbacks to use during training.\n\nbatchsize = 16: Batch size used for the training data loader.\n\nbackbone = nothing: Backbone model to construct task-specific model from using taskmodel(task, backbone).\n\nmodel = nothing: Complete model to use. If given, the backbone argument is ignored.\n\noptimizer = Adam(): Optimizer passed to Learner.\n\nlossfn = tasklossfn(task): Loss function passed to Learner.\n\n\nAny other keyword arguments will be passed to taskdataloaders.\nExamples\nFull example:\ndata blocks loaddataset task blocks learner task data learner \nCustom training and validation split:\nlearner task traindata validdata \nUsing callbacks:\nlearner task data callbacks TensorboardBackend \n\n"
    url = "FastAI@pr-288/ref/FastAI.tasklearner"
    [[input.files]]
    title = "mocktarget"
    contents = "mocktarget(task)\nGenerate a random target compatible with task.\n\n"
    url = "FastAI@pr-288/ref/FastAI.mocktarget"
    [[input.files]]
    title = "Encoding"
    contents = "abstract type Encoding\nTransformation of Blocks. Can encode some Blocks ([encode]), and optionally decode them [decode]\nInterface\nencode(::E, ::Context, block::Block, data) encodes block of data. The default is to do nothing. This should be overloaded for an encoding E, concrete Block types and possibly a context.\n\ndecode(::E, ::Context, block::Block, data) decodes block of data. This should correspond as closely as possible to the inverse of encode(::E, ...). The default is to do nothing, as not all encodings can be reversed. This should be overloaded for an encoding E, concrete Block types and possibly a context.\n\nencodedblock(::E, block::Block) -> block' returns the block that is obtained by encoding block with encoding E. This needs to be constant for an instance of E, so it cannot depend on the sample or on randomness. The default is to return nothing, meaning the same block is returned and not changed. Encodings that return the same block but change the data (e.g. ProjectiveTransforms) should return block.\n\ndecodedblock(::E, block::Block) -> block' returns the block that is obtained by decoding block with encoding E. This needs to be constant for an instance of E, so it cannot depend on the sample or on randomness. The default is to return nothing, meaning the same block is returned and not changed.\n\nencode!(buf, ::E, ::Context, block::Block, data) encodes data inplace.\n\ndecode!(buf, ::E, ::Context, block::Block, data) decodes data inplace.\n\n\n\n"
    url = "FastAI@pr-288/ref/FastAI.Encoding"
    [[input.files]]
    title = "pathname"
    contents = "pathname"
    url = "FastAI@pr-288/ref/FastAI.Datasets.pathname"
    [[input.files]]
    title = "ROOT_URL_FastAI"
    contents = "ROOT_URL_FastAI"
    url = "FastAI@pr-288/ref/FastAI.Datasets.ROOT_URL_FastAI"
    [[input.files]]
    title = "encodedblockfilled"
    contents = "encodedblockfilled"
    url = "FastAI@pr-288/ref/FastAI.encodedblockfilled"
    [[input.files]]
    title = "OneHotTensorMulti"
    contents = "OneHotTensorMulti"
    url = "FastAI@pr-288/ref/FastAI.OneHotTensorMulti"
    [[input.files]]
    title = "decay_optim"
    contents = "decay_optim(optim, wd)\nAdd WeightDecay with value wd to optimizer optim.\n\n"
    url = "FastAI@pr-288/ref/FastAI.decay_optim"
    [[input.files]]
    title = "DatasetRecipe"
    contents = "abstract type DatasetRecipe\nA recipe that contains configuration for loading a data container. Calling it with a path returns a data container and the blocks that each sample is made of.\nExamples\nFor example implementations, see FastVision.ImageFolders.\nExtending\nInterface\nloadrecipe(::DatasetRecipe, args...; kwargs...) -> (data, blocks) This loads a data container the [Block]s that each observation corresponds to. For most recipes the only argument beside the recipe is a path to a folder on disk.\n\nrecipeblocks(::Type{DatasetRecipe}) -> TBlocks The type of blocks returned by loadrecipe. Should be as specific as possible. Used for discovery.\n\n\nInvariants\nGiven\ndata blocks loadrecipe recipe args kwargs \nthe following must hold:\n∀i ∈ 1:numobs(data): checkblock(blocks, data[i]), i.e. data must be a data container of observations that are valid blocks.\n\nnumobs(data) ≥ 1, i.e. there is at least one observation if the data was loaded without error.\n\n\n\n"
    url = "FastAI@pr-288/ref/FastAI.Datasets.DatasetRecipe"
    [[input.files]]
    title = "encodesample"
    contents = "encodesample"
    url = "FastAI@pr-288/ref/FastAI.encodesample"
    [[input.files]]
    title = "learningmethod"
    contents = "\n"
    url = "FastAI@pr-288/doc/docs/tutorials/learningmethod.md"
    [[input.files]]
    title = "propagatedecode"
    contents = "propagatedecode(wrapper::WrapperBlock, encoding::Encoding) -> true|false\nWhether the wrapper type should be kept after decoding the wrapped block with encoding.\n\n"
    url = "FastAI@pr-288/ref/FastAI.propagatedecode"
    [[input.files]]
    title = "SupervisedTask"
    contents = "SupervisedTask((inputblock, targetblock), encodings)\nA AbstractBlockTask learning task for the supervised task of learning to predict a target given an input. encodings are applied to samples before being input to the model. Model outputs are decoded using those same encodings to get a target prediction.\nIn addition, to the blocks defined by AbstractBlockTask, getblocks(::SupervisedTask) defines the following blocks:\nBy default the model output is assumed to be an encoded target, but the ŷblock keyword argument to overwrite this.\nblocks.input: An unencoded input and the first element in the tuple sample = (input, target)\n\nblocks.target: An unencoded target and the second element in the tuple sample = (input, target)\n\nblocks.pred: A prediction. Usually the same as blocks.target but may differ if a custom ŷblock is specified.\n\n\nA SupervisedTask also enables some additional functionality:\nencodeinput\n\nencodetarget\n\nshowprediction, showpredictions\n\n\n\n"
    url = "FastAI@pr-288/ref/FastAI.SupervisedTask"
    [[input.files]]
    title = "DiscriminativeLRs"
    contents = "DiscriminativeLRs(paramgroups, factors)\nUse different learning rates based on paramgroups. factors maps each group to a factor that the learning rate is multiplied by, so for a parameter x the factor is get(factors, getgroup(paramgroups, x), 1).\nSee ParamGroups.\nExamples\nCombining with regular gradient descent, but only training a part of the model.\nmodel paramgroups model dlro paramgroups Dict o dlro \n\n"
    url = "FastAI@pr-288/ref/FastAI.DiscriminativeLRs"
    [[input.files]]
    title = "Text Classification"
    contents = "Text Classification\n\nWe'll use the IMDB dataset for this task. This is a dataset for binary sentiment classification containing 25,000 highly polarized movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n\n\ndata blocks load \nEach sample is a review, this'll be our input data. The output is the sentiment of the input, either positive or negative.\n\nprintln data println blocks \ntask TextClassificationSingle blocks data \nThe task consists of encodings that needs to be applied to the input data and output data.\nThe encodings for the input data are as follows:\nSanitize: Involves text cleaning steps like case trimming, remove punctuations, removing stop words, and some fastai specific preprocessing steps (xxbos, xxup, etc).\n\nTokenize: Tokenizing the text into words.\n\nEmbedVocabulary: Embedding the words into a vector space. This step constructs the vocabulary for the training data and returns the vector embedding for the input data.\n\n\n\ntask encodings \ninput target data \nencoded_input encoded_output task input target println encoded_input println encoded_output \nLet us now look at each step of the above encoding process.\n\nSanitizeThe sanitized input data will have no stop words, no punctuations, and no case. Along with those, it'll also contain some fastai specific tokens like xxbos (beginning of the sentence), xxup (the next word if uppercase in the original text), xxmaj (the first letter is uppercase in the original text), etc.\n\nencoding_1 Textual Sanitize sanitized_data encoding_1 Paragraph input \nTokenizeTokenize the sanitized input data.\n\nencoding_2 Textual Tokenize tokenized_data encoding_2 Paragraph sanitized_data \nEmbedVocabularyThis step is the most important step in the encoding process. It constructs the vocabulary for the training data and returns the vector embedding for the input data.\n\nvocab Textual EmbedVocabulary data encoding_3 Textual EmbedVocabulary vocab vocab vocab \nvector_data encoding_3 Textual Tokens tokenized_data \n"
    url = "FastAI@pr-288/doc/docs/notebooks/textclassification.ipynb"
    [[input.files]]
    title = "Performant data pipelines"
    contents = "Performant data pipelines\nBottlenecks in data pipelines and how to measure and fix them\nWhen training large deep learning models on a GPU we clearly want wait as short as possible for the training to complete. The hardware bottleneck is usually the GPU power you have available to you. This means that data pipelines need to be fast enough to keep the GPU at 100% utilization, that is, keep it from \"starving\". Reducing the time the GPU has to wait for the next batch of data directly lowers the training time until the GPU is fully utilized. There are other ways to reduce training time like using hyperparameter schedules and different optimizers for faster convergence, but we'll only talk about improving GPU utilization here.\nReasons for low GPU utilization\nThe main cause of low GPU utilization is that the next batch of data is not available after a training step and the GPU has to wait. This means that in order to get full GPU utilization,\nloading a batch must not take longer than a training step; and\n\nthe data must be loaded in the background, so that it is ready the moment the GPU needs it.\n\nThese issues can be addressed by\nusing worker threads to load multiple batches in parallel\n\nkeeping the primary thread free; and\n\nreducing the time it takes to load a single batch\n\nFastAI.jl by default uses DataLoader from the DataLoaders.jl package which addresses points 1. and 2. For those familiar with PyTorch, it closely resembles torch.utils.data.DataLoader. It also efficiently collates the data by reusing a buffer where supported.\nWe can measure the large performance difference by comparing a naive sequential data iterator with eachobsparallel, the data iterator that DataLoader uses.\nDataLoaders batchviewcollated data blocks load task blocks size taskdata data task batchdata batchviewcollated taskdata NBATCHES i enumerate batchdata i i batchdata i NBATCHES i enumerate batchdata i NBATCHES \nRunning each timer twice to forego compilation time, the sequential iterator takes 20 seconds while the parallel iterator using 11 background threads only takes 2.5 seconds. This certainly isn't a proper benchmark, but it shows the performance can be improved by an order of magnitude with no effort.\nBeside increasing the amount of compute available with worker threads as above, the data loading performance can also be improved by reducing the time it takes to load a single batch. Since a batch is made up of some number of observations, this usually boils down to reducing the loading time of a single observation. If you're using the LearningTask API, this can be further broken down into the loading and encoding part.\nMeasuring performance\nSo how do you know if your GPU is underutilized? If it isn't, then improving data pipeline performance won't help you at all! One way to check this is to start training and run > watch -n 0.1 nvidia-smi in a terminal which displays and refreshs GPU stats every 1/10th of a second. If GPU-Util stays between 90% and 99%, you're good!\nIf that's not the case, you might see it frantically jumping up and down. We can get a better estimate of how much training time can be sped up by running the following experiment:\nLoad one batch and run n optimization steps on this batch. The time this takes corresponds to the training time when the GPU does not have to wait for data to be available.\n\nNext take your data iterator and time iterating over the first n batches without an optimization step.\n\nThe speed of the complete training loop (data loading and optimization) will be around the maximum of either measurement. Roughly speaking, if 1. takes 100 seconds and 2. takes 200 seconds, you know that you can speed up training by about a factor of 2 if you reduce data loading time by half, after which the GPU will become the bottleneck.\ndata blocks load task blocks learner task data NBATCHES first learner data training learner model model i NBATCHES learner i zip learner data training NBATCHES \nAgain, make sure to run each measurement twice so you don't include the compilation time.\n\nTo find performance bottlenecks in the loading of each observation, you'll want to compare the time it takes to load an observation of the task data container and the time it takes to encode that observation.\nBenchmarkTools N data data N i N data i obss data i i N i N task obss i \nThis will give you a pretty good idea of where the performance bottleneck is. Note that the encoding performance is often dependent of the task configuration. If we used ImageClassification with input size (64, 64) it would be much faster.\nImproving performance\nSo, you've identified the data pipeline as a performance bottleneck. What now? Before anything else, make sure you're doing the following:\nUse DataLoaders.DataLoader as a data iterator. If you're using taskdataloaders or tasklearner, this is already the case.\n\nStart Julia with multiple threads by specifying the -t n/-t auto flag when starting Julia. If it is successful, Threads.nthreads() should be larger than 1.\n\n\nIf the data loading is still slowing down training, you'll probably have to speed up the loading of each observation. As mentioned above, this can be broken down into observation loading and encoding. The exact strategy will depend on your use case, but here are some examples.\nReduce loading time of image datasets by presizing\nFor many computer vision tasks, you will resize and crop images to a specific size during training for GPU performance reasons. If the images themselves are large, loading them from disk itself can take some time. If your dataset consists of 1920x1080 resolution images but you're resizing them to 256x256 during training, you're wasting a lot of time loading the large images. Presizing means saving resized versions of each image to disk once, and then loading these smaller versions during training. We can see the performance difference using ImageNette since it comes in 3 sizes: original, 360px and 180px.\ndata_orig load data_orig buffered data_320px load data_320px buffered data_160px load data_160px buffered \nReducing allocations with inplace operations\nWhen implementing the LearningTask interface, you have the option to implement encode!(buf, task, context, sample), an inplace version of encode that reuses a buffer to avoid allocations. Reducing allocations often speeds up the encoding step and can also reduce the frequency of garbage collector pauses during training which can reduce GPU utilization.\nUsing efficient data augmentation\nMany kinds of augmentation can be composed efficiently. A prime example of this are image transformations like resizing, scaling and cropping which are powered by DataAugmentation.jl. See its documentation to find out how to implement efficient, composable data transformations.\n\n"
    url = "FastAI@pr-288/doc/docs/background/datapipelines.md"
    [[input.files]]
    title = "FastAI/datablock/wrappers.jl"
    contents = "Base parent w w block Base parent b b w parent w b b w b Setfield w block b w w w obs w obs wrapper w string nameof typeof wrapper b parent wrapper w b _ _ _ _ _ _ _ _ encoding block encoding block block encoding block encoding block block encoding block encoding block block encoding block encoding block block wrapper encoding wrapper encoding parent wrapper wrapper encoding wrapper encoding parent wrapper encoding wrapper encblock encoding parent wrapper isnothing encblock nothing wrapper encoding wrapper encblock encblock encoding wrapper decblock encoding parent wrapper isnothing decblock nothing wrapper encoding wrapper decblock decblock enc ctx wrapper obs kwargs enc ctx parent wrapper obs kwargs enc ctx wrapper obs kwargs enc ctx parent wrapper obs kwargs enc ctx w obs enc ctx parent w obs enc ctx w obs enc ctx parent w obs enc ctx w obs state enc ctx w obs enc ctx parent w obs state enc ctx w obs state enc ctx w obs enc ctx parent w obs state wrapper wrapper wrapper out args wrapper out args in out args in out args wrapper out wrapper out in out in out B P block B propagation P w w propagation block enc encblock enc block w block w enc w block w enc w block w enc w block w enc enc block enc encblock enc block enc encblock \n"
    url = "FastAI@pr-288/src/datablock/wrappers.jl"
    [[input.files]]
    title = "setwrapped"
    contents = "setwrapped"
    url = "FastAI@pr-288/ref/FastAI.setwrapped"
    [[input.files]]
    title = "FastAI/tasks/predict.jl"
    contents = "task model input device undevice context task task model input device device undevice undevice context context only task context undevice model device task context input task model inputs device undevice context xs device batch copy task context input input inputs ŷs undevice model xs targets task context ŷ ŷ ŷs targets \n"
    url = "FastAI@pr-288/src/tasks/predict.jl"
    [[input.files]]
    title = "isavailable"
    contents = "isavailable"
    url = "FastAI@pr-288/ref/FastAI.Datasets.isavailable"
    [[input.files]]
    title = "FastAI.jl"
    contents = "FastAI.jl\n\nFastAI.jl is a Julia library for training state-of-the art deep learning models.\nFrom loading datasets and creating data preprocessing pipelines to training, FastAI.jl takes the boilerplate out of deep learning projects. It equips you with reusable components for every part of your project while remaining customizable at every layer. FastAI.jl comes with support for common computer vision and tabular data learning tasks, with more to come.\nFastAI.jl's high-level workflows combine functionality from many packages in the ecosystem, most notably Flux.jl, FluxTraining.jl, DataAugmentation.jl and MLUtils.jl.\nSee our documentation to find out more.\nExample\nAs an example, here is how to train an image classification model:\nFastVision data blocks load task blocks learner task data callbacks learner task learner \nSetup\nTo get started, install FastAI.jl and domain packages using the Julia package manager:\nPkg \nor try it out with this Google Colab template.\nGetting started\nTo dive in, you may be interested in\nan overview of the high-level API,\n\nseeing some example learning tasks,\n\nfinding out how you can search for and find datasets and other functionality; or\n\nour contributor guide\n\n\nGet in touch\nYou can get in touch here on GitHub or on the JuliaLang Zulip in the #ml-contributors channel.\n\nAcknowledgements\nFastAI.jl takes inspiration from the fantastic fastai library for Python. Jeremy Howard and the fastai team kindly approved this project and its use of the fastai name.\nThis project also builds on many packages in the Julia ecosystem.\n\n"
    url = "FastAI@pr-288/doc/README.md"
    [[input.files]]
    title = "FastAI/datasets/fastaidatasets.jl"
    contents = "name Any subfolder Any extension Any description Any checksum Any datadepname Any subpath Any size Any name extension description checksum datadepname size name dset_id extension description checksum datadepname splits size name subfolder checksum extension description datadepname name subpath name size name subfolder extension description checksum datadepname subpath size name checksum extension description datadepname size name extension description checksum datadepname size name dset_id checksum extension description splits datadepname size name dset_id extension description checksum datadepname splits size Dict size size size subpath size description size description description description description description description size size description description description description description size description size size size size size size size size size size size size size size size size size size size size size size size size datadepname extension datadepname extension datadepname extension datadepname extension datadepname extension datadepname extension datadepname extension datadepname extension datadepname extension description datadepname size description datadepname size description datadepname size description datadepname size d datadepname d vcat d datadepname d typeof d d subfolder DataDeps DataDep d DataDep d datadepname d name d description d size d subfolder d name d extension d checksum post_fetch_method f DataDeps unpack f extracted readdir pwd temp mktempdir mv extracted temp force mv temp pwd force DataDeps DataDep d DataDep d datadepname d name d description d size d name d extension d checksum post_fetch_method f DataDeps unpack f DataDeps DataDep d remote_paths d dset_id d name split split d splits DataDep d datadepname d name d description d size remote_paths d checksum d DataDeps register DataDep d \n"
    url = "FastAI@pr-288/src/datasets/fastaidatasets.jl"
    [[input.files]]
    title = "TSClassificationDataset"
    contents = "TSClassificationDataset"
    url = "FastAI@pr-288/ref/FastAI.Datasets.TSClassificationDataset"
    [[input.files]]
    title = "FastAI/training/discriminativelrs.jl"
    contents = "Optimise AbstractOptimiser pg factorfn Any pg factors Dict pg get factors o x Δ AbstractArray T T factor convert T o factorfn o pg x factor one T Δ Δ factor setlearningrate! optimizer value setlearningrate! optimizer os end value model pg FastAI FastAI model o FastAI pg Dict Descent x1 model weight x2 model weight o x1 ones size x1 zeros size x1 o x2 ones size x2 fill size x1 \n"
    url = "FastAI@pr-288/src/training/discriminativelrs.jl"
    [[input.files]]
    title = "PropagateSameBlock"
    contents = "struct PropagateSameBlock <: PropagateWrapper end\nPropagate a wrapper type only if the encoded block is same, ignoring any wrappers.\nSee propagate for more information.\n\n"
    url = "FastAI@pr-288/ref/FastAI.PropagateSameBlock"
    [[input.files]]
    title = "AbstractBlockTask"
    contents = "abstract type AbstractBlockTask <: LearningTask\nAbstract supertype for learning tasks that derive their functionality from Blocks and Encodings.\nThese learning tasks require you only to specify blocks and encodings by defining which blocks of data show up at which stage of the pipeline. Generally, a subtype will have a field blocks of type NamedTuple that contains this information and a field encodings of encodings that are applied to samples. They can be accessed with getblocks and getencodings respectively. For example, SupervisedTask represents a learning task where each sample consists of an input and a target.\nFastVision task task \nTo implement a new AbstractBlockTask either\nuse the helper BlockTask (simpler)\n\nor subtype AbstractBlockTask (allows customization through dispatch)\n\n\nBlocks and interfaces\nTo support different learning task interfaces, a AbstractBlockTask's blocks need to contain different blocks. Below we list first block names with descriptions, and afterwards relevant interface functions and which blocks are required to use them.\nBlocks\nEach name corresponds to a key of the named tuple blocks = getblocks(task)). A block is referred to with blocks.$name and an instance of data from a block is referred to as $name.\nblocks.sample: The most important block, representing one full observation of unprocessed data. Data containers used with a learning task should have compatible observations, i.e. checkblock(blocks.sample, data[i]).\n\nblocks.x: Data that will be fed into the model, i.e. (neglecting batching) model(x) should work\n\nblocks.ŷ: Data that is output by the model, i.e. (neglecting batching) checkblock(blocks.ŷ, model(x))\n\nblocks.y: Data that is compared to the model output using a loss function, i.e. lossfn(ŷ, y)\n\nblocks.encodedsample: An encoded version of blocks.sample. Will usually correspond to encodedblockfilled(getencodings(task), blocks.sample).\n\n\nInterfaces/functionality and required blocks:\nCore:\nencode(task, ctx, sample) requires sample. Also enables use of taskdataset, taskdataloaders\n\ndecode(task, ctx, encodedsample) requires encodedsample\n\ndecodeypred(task, ctx, ŷ) requires ŷ\n\ndecodey(task, ctx, y) requires y\n\n\nTraining:\ntaskmodel(task) requires x, ŷ\n\ntasklossfn(task) requires y, ŷ\n\n\nVisualization:\nshowsample, showsamples require sample\n\nshowencodedsample, showencodedsamples, showbatch require encodedsample\n\nshowsample, showsamples require sample\n\nshowoutput, showoutputs, showoutputbatch require ŷ, encodedsample\n\n\nTesting:\nmockmodel(task) requires x, ŷ\n\nmocksample(task) requires sample\n\n\n\n"
    url = "FastAI@pr-288/ref/FastAI.AbstractBlockTask"
    [[input.files]]
    title = "Saving and loading models for inference"
    contents = "Saving and loading models for inference\n\nIn the end, we train models because we want to use them for inference, that is, using them to generate predictions on new targets. The general formula for doing this in FastAI.jl is to first train a model for a task, for example using fitonecycle! or finetune! and then save the model and the learning task configuration to a file using savetaskmodel. In another session you can then use loadtaskmodel to load both. Since the learning task contains all preprocessing logic we can then use predict and predictbatch to generate predictions for new inputs.\nLet's fine-tune an image classification model (see here for more info) and go through that process.\n\nFastVision Metalhead \ndir joinpath load data dir filterfn FastVision loadfn classes unique data task classes backbone Metalhead ResNet50 layers end learner task data backbone backbone callbacks \nlearner \nNow we can save the model using savetaskmodel.\n\ntask learner model force \nIn another session we can now use loadtaskmodel to load both model and learning task from the file. Since the model weights are transferred to the CPU before being saved, we need to move them to the GPU manually if we want to use that for inference.\n\ntask model model model \nFinally, let's select 9 random images from the dataset and see if the model classifies them correctly:\n\nx y samples data i i rand data images sample sample samples labels sample sample samples preds task model images device context \nacc sum labels preds length preds \nFastMakie CairoMakie task collect zip images preds \n"
    url = "FastAI@pr-288/doc/docs/notebooks/serialization.ipynb"
    [[input.files]]
    title = "FastAI/interpretation/detect.jl"
    contents = "Ref block obs block obs block obss block obss encodings block obs encodings block obs encodings block obs encodings block obs \n"
    url = "FastAI@pr-288/src/interpretation/detect.jl"
    [[input.files]]
    title = "getencodings"
    contents = "getencodings"
    url = "FastAI@pr-288/ref/FastAI.getencodings"
    [[input.files]]
    title = "estimatelr"
    contents = "estimatelr(::LREstimator, losses, lrs)\nEstimate the optimal learning rate using losses and lrs.\n\n"
    url = "FastAI@pr-288/ref/FastAI.estimatelr"
    [[input.files]]
    title = "propagate"
    contents = "propagate(wrapper::WrapperBlock, encoding::Encoding) -> true|false\nWhether the wrapper type should be kept after encoding the wrapped block with encoding.\n\n"
    url = "FastAI@pr-288/ref/FastAI.propagate"
    [[input.files]]
    title = "PropagateAlways"
    contents = "PropagateAlways"
    url = "FastAI@pr-288/ref/FastAI.PropagateAlways"
    [[input.files]]
    title = "loadfile"
    contents = "loadfile(file)\nLoad a file from disk into the appropriate format.\n\n"
    url = "FastAI@pr-288/ref/FastAI.Datasets.loadfile"
    [[input.files]]
    title = "Data containers"
    contents = "Data containers\nThis tutorial explains what data containers are, how they are used in FastAI.jl and how to create your own. You are encouraged to follow along in a REPL or a Jupyter notebook and explore the code. You will find small exercises at the end of some sections to deepen your understanding.\nIntroduction\nIn the quickstart section, you have already come in contact with data containers. The following code was used to load a data container for image classification:\nFastVision data _ load findfirst datasetid \nA data container is any type that holds observations of data and allows us to load them with getobs and query the number of observations with numobs. In this case, each observation is a tuple of an image and the corresponding class; after all, we want to use it for image classification.\nimage class obs data class image \ndata \nload(datasets[id]) makes it easy to a load a data container that is compatible with some block types, but to get a better feel for what it does, let's look under the hood by creating the same data container using some mid-level APIs.\nCreating data containers from files\nBefore we recreate the data container, we'll download the dataset and get the path where the files are saved to:\ndir load \nNow we'll start with loadfolderdata which creates a data container (here a Vector) of files given a path. We'll use the path of the downloaded dataset:\nfiles dir \nfiles is a data container where each observation is a path to a file. We'll confirm that using getobs:\np files \nNext we need to load an image and the corresponding class from the path. If you have a look at the folder structure of dir you can see that the parent folder of each file gives the name of class. So we can use the following function to load the (image, class) pair from a path:\nloadimageclass p p p image class loadimageclass p class image \nFinally, we use mapobs to lazily transform each observation and have a data container ready to be used for training an image classifier.\ndata loadimageclass files \nExercises\nUsing mapobs and loadfile, create a data container where every observation is only an image.\n\nChange the above code to run on a different dataset from the list in Datasets.DATASETS_IMAGECLASSIFICATION.\n\nSplitting a data container into subsets\nUntil now, we've only created a single data container containing all observations in a dataset. In practice, though, you'll want to have at least a training and validation split. The easiest way to get these is to randomly split your data container into two parts. Here we split data into 80% training and 20% validation data. Note the use of shuffleobs to make sure each split has approximately the same class distribution.\ntraindata valdata data at \nThis is great for experimenting, but where possible you will want to use the official training/validation split for a dataset. Consider the image classification dataset folder structure:\n- $dir\n    - train\n        - class1\n            - image1.jpg\n            - image2.jpg\n            - ...\n        - class2\n        - ...\n    - valid\n        - class1\n        - class2\n        - ...\nAs you can see, the grandparent folder of each image indicates which split it is a part of. groupobs allows us to partition a data container using a function. Let's use it to split filedata based on the name of the grandparent directory. (We can't reuse data for this since it no longer carries the file information.)\ndatagroups files p p trainfiles validfiles datagroups datagroups \nUsing this official split, it will be easier to compare the performance of your results with those of others'.\nDataset recipes\nWe saw above how different image classification datasets can be loaded with the same logic as long as they are in a common format. To encapsulate the logic for loading common dataset formats, FastAI.jl has DatasetRecipes. When we used datarecipes in the discovery tutorial, it showed us such recipes that allow loading a dataset for a specific task. For example, \"imagenette2-160\" has an associated ImageFolders recipe which we can load by getting the entry and calling load on it:\nentry \ndata blocks load entry \nThese recipes also take care of loading the data block information for the dataset. Read the discovery tutorial to find out more about that.\n\n"
    url = "FastAI@pr-288/doc/docs/data_containers.md"
    [[input.files]]
    title = "fastai API comparison"
    contents = "fastai API comparison\nFastAI.jl is in many ways similar to the original Python fastai, but also has its differences. This reference goes through all the sections in the fastai: A Layered API for Deep Learning paper and comments what the interfaces for the same functionality in FastAI.jl are, and where they differ or functionality is still missing.\nApplications\nFastAI.jl's own data block API makes it possible to derive every part of a high-level interface with a unified API across tasks. Instead it suffices to create a learning task and based on the blocks and encodings specified the proper model builder, loss function, and visualizations are implemented (see below). For a high-level API, a complete Learner can be constructed using tasklearner without much boilerplate. There are some helper functions for  creating these learning tasks, for example ImageClassificationSingle and ImageSegmentation.\nFastAI.jl additionally has a unified API for registering and discovering functionality across applications also based on the data block abstraction.  datasets and datarecipes let you quickly load common datasets matching some data modality and [learningtasks] lets you find learning task helpers for common tasks. See the discovery tutorial for more info.\nVision\nComputer vision is well-supported in FastAI.jl with different tasks and optimized data pipelines for N-dimensional images, masks and keypoints. See the tutorial section for many examples.\nTabular\nFastAI.jl also has support for tabular data.\nDeployment\nThrough FastAI.jl's LearningTask interface, the data processing logic is decoupled from the dataset creation and training and can be easily serialized and loaded to make predictions. See the tutorial on saving and loading models.\n\nThere is no integration (yet!) for text and collaborative filtering applications.\nHigh-level API\nHigh-level API foundations\nFastAI.jl also has a data block API but it differs from fastai's in a number of ways. In the Julia package it only handles the data encoding and decoding part, and doesn't concern itself with creating datasets. For dataset loading, see the data container API. As mentioned above, the high-level application-specific logic is also derived from the data block API. To use it you need to specify a tuple of input and target blocks as well as a tuple of encodings that are applied to the data. The encodings  are invertible data-specific data processing steps which correspond to fastai.Transforms. As in fastai, dispatch is used to transform applicable data and pass other data through unchanged. Unlike in fastai, there are no default steps associated with a block, allowing greater flexibility.\nWe can create a BlockTask (similar to fastai.DataBlock) and get information about the representations the data goes through.\nFastVision task task \nFrom this short definition, many things can be derived:\ndata encoding\n\nmodel output decoding\n\nhow to create a model from a backbone\n\nthe loss function to use\n\nhow to visualize samples and predictions\n\n\nTogether with a data container data, we can quickly create a Learner using tasklearner which, like in fastai, handles the training for us. There are no application-specific Learner constructors like cnn_learner or unet_learner in FastAI.jl.\nlearner task data \nHigh-level training protocols like the one-cycle learning rate schedule, fine-tuning and the learning rate finder are then available to us:\nlearner learner learner res learner plot res \nIncrementally adapting PyTorch code\nSince it is a Julia package, FastAI.jl is not written on top of PyTorch, but a Julia library for deep learning: Flux.jl. In any case, the point of this section is to note that the abstractions in fastai are decoupled and existing projects can easily be reused. This is also the case for FastAI.jl as it is built on top of several decoupled libraries. Many of these were built specifically for FastAI.jl, but they are unaware of each other and useful in their own right:\nFlux.jl provides models, optimizers, and loss functions, fulfilling a similar role to PyTorch\n\nMLUtils.jl gives you tools for building and transforming data containers. Also, it takes care of efficient, parallelized iteration of data containers.\n\nDataAugmentation.jl takes care of the lower levels of high-performance, composable data augmentations.\n\nFluxTraining.jl contributes a highly extensible training loop with 2-way callbacks\n\n\nIf that seems like a lot: don't worry! If you've installed FastAI.jl, the functionality of most of these packages is reexported and you don't have to install any of them explicitly.\nConsistency across domains\nWhile computer vision is the only domain with mature support for now, the abstractions underlying FastAI.jl are carefully crafted to ensure that learning tasks for different domains can be created using the same set of interfaces. This shows in that there's no need for application-specific functionality above the data block API.\nMid-level APIs\nLearner\nThe Learner is very similar to fastai's. It takes\na model: any parameterized, differentiable function like a neural network or even a trebuchet simulator\n\ntraining and validation data iterators: these can be DataLoaders which paralellize data loading but any iterator over batches can be used\n\noptimizer\n\nloss function\n\n\nTwo-way callbacks\nThe training loop also supports two-way callbacks. See the FluxTraining.jl docs for a list of all available callbacks. While supporting all the functionality of fastai's callbacks and training loop, it also provides an extensible training loop API that makes it straightforward to integrate custom training steps with the available callbacks. As a result, different training steps for problems other than standard supervised training can make use of existing callbacks  without the need to handle control flow through callbacks. Additionally, callbacks have an additional level of safety by being required to declare what state they access and modify. With a little more effort up-front, this guarantees correct ordering of callback execution through a dependency graph. In the future, this will also make it possible to automatically run callbacks in parallel and asynchronously to reduce overhead by long-running callbacks like costly metric calculations and logging over the network.\nEncodings and blocks\nIn the paper, this subsection is in the low-level section (named Transforms and Pipelines), but I'm putting it here since it is the core of FastAI.jl's data block API. FastAI.jl provides Encodings and Blocks which correspond to fastai's Transforms and Blocks. Encodings implement an encode (and optionally decode) function that describes how data corresponding to some blocks is transformed and how that transformation can be inverted. There is also support for stateful encodings like ProjectiveTransforms which need to use the same random state to augment every data point. Additionally, encodings describe what kind of block data is returned from encoding, allowing inspection of the whole data pipeline. The Blocks are used to dispatch in the encode function to implement block-specific transformations. If no encode task is implemented for a pair of encoding and block, the default is to pass the data through unchanged like in fastai.\nThe Blocks also allow implementing task-specific functionality:\nblocklossfn takes a prediction and encoded target block to determine a good loss function to use. For example, for image classification we want to compare two one-hot encoded labels and hence define blocklossfn(::OneHotTensor{0}, ::OneHotTensor{0}) = logitcrossentropy.\n\nblockmodel constructs a model from a backbone that maps an input block to an output block. For example, for image segmentation we have ImageTensor{N}() as the input block and OneHotTensor{N} (one-hot encoded N-dimensional masks) as output, so blockmodel turns the backbone into a U-Net.\n\nshowblock! defines how to visualize a block of data.\n\n\nGeneric optimizer\nFastAI.jl uses the optimizers from Flux.jl, which provides a similarly composable API for optimzers.\nGeneralized metric API\nMetrics are handled by the Metrics callback which takes in reducing metric functions or FluxTraining.AbstractMetrics which have a similar API to fastai's.\nfastai.data.external\nFastAI.jl makes all the same datasets available in fastai.data.external available. See datasets for a list of all datasets that can be downloaded.\nfuncs_kwargs and DataLoader, fastai.data.core\nIn FastAI.jl, you are not restricted to a specific type of data iterator and can pass any iterator over batches to Learner. In cases where performance is important DataLoader can speed up data iteration by loading and batching samples in parallel on background threads. All transformations of data happen through the data container interface which requires a type to implement Base.getindex/MLUtils.getobs and Base.length/MLUtils.numobs, similar to PyTorch's torch.utils.data.Dataset. Data containers are then transformed into other data containers. Some examples:\nmapobs(f, data) lazily maps a function f of over data such that getobs(mapobs(f, data), idx) == f(getobs(data, idx)). For example mapobs(loadfile, files) turns a vector of image files into a data container of images.\n\nDataLoader(data; batchsize) is a wrapper around BatchView which turns a data container of samples into one of collated batches and eachobsparallel which creates a parallel, buffered iterator over the observations (here batches) in the resulting container.\n\ngroupobs(f, data) splits a container into groups using a grouping function f. For example, groupobs(grandparentname, files) creates training splits for files where the grandparent folder indicates the split.\n\nMLUtils.ObsView(data, idxs) lazily takes a subset of the observations in data.\n\n\nFor more information, see the data container tutorial and the MLUtils.jl docs. At a higher level, there are also convenience functions like loadfolderdata to create data containers.\nLayers and architectures\nFlux.jl already does a better job at functionally creating model architectures than PyTorch, so FastAI.jl makes use of its layers. For example Flux.SkipConnection  corresponds to fastai's MergeLayer. The FastAI.Models submodule currently provides some high-level architectures like xresnet18 and a U-Net builder UNetDynamic that can create U-Nets from any convolutional feature extractor. The optional dependency Metalhead.jl also provides common pretrained vision models.\nLow-level APIs\nDue to the nature of the Julia language and its design around multiple dispatch, packages tend to compose really well, so it was not necessary to reimplement or provide a unified API for low-level operations. We'll comment on the libraries that we were able to use.\nPyTorch foundations\nUnlike Python, Julia has native support for N-dimensional regular arrays. As such, there is a standard interface for arrays and libraries don't need to implement their own. Consider that every deep learning framework in Python implements their own CPU and GPU arrays, which is part of the reason they are frameworks, not libraries (with the latter being vastly preferable). Julia's standard libraries implements the standard CPU Array type. GPU arrays are implemented through CUDA.jl CuArray type (with unified support for GPU vendors other than nvidia in the works). As a result, Flux.jl, the deep learning library of choice for FastAI.jl, does not need to reimplement their own CPU and GPU array versions. This kind of composability in general largely benefits what can be accomplished in Julia.\nSome other libraries which are used under the hood: for image processing, the Images.jl ecosystem of packages is used; for reading and processing tabular data DataFrames.jl and Tables.jl; for plotting Makie.jl.\nType dispatch\nMultiple dispatch already is a core feature of the Julia language, hence the extensible interfaces in FastAI.jl are built around it and are natural fit for the language.\nObject-oriented semantic tensors\nAs mentioned above, Julia has great support for arrays with extra functionality available to packages that provide wrapper arrays like NamedDims.jl which should generally just work with every part of the library. Hence there is no need for an addtional API that unifies separate packages, which in turn makes FastAI.jl more composable with other packages.\nIn encodings, the array types are used for dispatch only where an especially performant implementation is possible, and the block information is used for dispatching the semantics of the encoding.\nGPU-accelerated augmentation\nFastAI.jl does not support GPU-accelerated augmentation (yet). Please open an issue if you run into a situation where data processing becomes the bottleneck and we'll prioritize this. The affine transformations implemented in DataAugmentation.jl and used in FastAI.jl are properly composed to ensure high quality results. They are also optimized for speed and memory usage (with complete support for inplace transformations).\nConvenience functionality\nMuch of the convenience provided by fastai is not required in Julia:\n@delegates: Due to the absence of deep class hierarchies, keyword arguments are seldom passed around (the only instance where this happens in FastAI.jl is tasklearner).\n\n@patch: since Julia is built around multiple dispatch, not classes, you just implement the task for a type, no patching needed\n\nL: due to first-class array support such a wrapper list container isn't needed\n\n\nnbdev\nThere is no nbdev-equivalent in Julia at the moment. That said, this documentation is generated by a document creation package Pollen.jl that could be extended to support such a workflow. It already has support for different source and output formats like Jupyter notebooks, code execution and is built for interactive work with incremental rebuilds.\n\nHopefully this page has given you some context for how FastAI.jl relates to fastai and how to map concepts between the two. You are encouraged to go through the tutorials to see the design decisions made in practice.\n\n"
    url = "FastAI@pr-288/doc/docs/fastai_api_comparison.md"
    [[input.files]]
    title = "showblocksinterpretable"
    contents = "showblocksinterpretable(backend, encodings, block, obss)\nMulti-sample version showblockinterpretable.\n\n"
    url = "FastAI@pr-288/ref/FastAI.showblocksinterpretable"
    [[input.files]]
    title = "OneHotTensor"
    contents = "OneHotTensor{N, T}(classes) <: Block\nA block representing a one-hot encoded, N-dimensional array categorical variable. For example, a single categorical label is a OneHotTensor{0, T} (aliased to OneHotTensor{T}).\nUse the OneHot encoding to one-hot encode Labels or LabelMultis.\n\n"
    url = "FastAI@pr-288/ref/FastAI.OneHotTensor"
    [[input.files]]
    title = "tasklossfn"
    contents = "tasklossfn(task)\nDefault loss function to use when training models for task.\n\n"
    url = "FastAI@pr-288/ref/FastAI.tasklossfn"
    [[input.files]]
    title = "FastAI/Registries/datasets.jl"
    contents = "_datasetregistry name registry Registry id Field String name formatfn FeatureRegistries string_format description Field String name optional description formatfn FeatureRegistries md_format size Field String name description optional tags Field Vector String name defaultfn row key String package Field Module name formatfn FeatureRegistries code_format downloaded Field Bool name description computefn row key row loader loader Field name formatfn x FeatureRegistries type_format name loadfn row row loader startswith row loader datadep dir row loader isdir joinpath dir row id cd dir temp mktempdir mv joinpath dir row id temp force mv temp pwd force row loader description registry _datasetregistry kwargs isempty kwargs filter kwargs _registerdatasets registry Registry config FastAI id config datadepname haskey registry id push! registry id description config description missing config description loader config datadepname size config size missing config size package FastAI \n"
    url = "FastAI@pr-288/src/Registries/datasets.jl"
    [[input.files]]
    title = "axiskwargs"
    contents = "axiskwargs"
    url = "FastAI@pr-288/ref/FastAI.axiskwargs"
    [[input.files]]
    title = "lrfind"
    contents = "lrfind(learner[, dataiter; kwargs...]) -> LRFinderResult\nRun the learning rate finder. Exponentially increases the learning rate from a very low value to a very high value and uses the losses to estimate an optimal learning rate. Return a LRFinderResult.\nKeyword arguments\nnsteps = 100: maximum number of steps to run the learning rate finder for\n\nstartlr = 1e-7: minimum learning rate\n\nendlr = 10: maximum learning rate\n\ndivergefactor: stop finder early if loss goes higher than lowest loss times this factor\n\nestimators = [Steepest(), MinDivByTen()]: list of LREstimators\n\n\n\n"
    url = "FastAI@pr-288/ref/FastAI.lrfind"
    [[input.files]]
    title = "Variational autoencoders"
    contents = "Variational autoencoders\n\nSo far, we've covered many examples of how to train models in a supervised fashion. However, there are many applications of neural networks outside the supervised regime. In this tutorial, we'll implement and train a variational autoencoder (VAE) to embed and generate images from the MNIST dataset.\nYou'll learn how to\nimplement custom Flux.jl models\n\nwrite a custom training loop\n\ngenerate new images and visualize them\n\n\n\nFastVision FastMakie FastVision SVector Gray CairoMakie \nSetting up the data\n\nFirst we load the MNIST dataset. Since we're not using it for supervised learning, we only need the input images, and don't load the labels. We get a data container where every observation is an image:\n\npath load data path filterfn FastVision loadfn data \nNext we need to define a learning task that will handle data encoding and decoding as well as visualization for us. So far, we've used SupervisedTask a lot which assumes there is an input that is fed to the model and a corresponding target output. Since we want to do unsupervised learning, we'll instead create a custom learning task using BlockTask. It defines what kind of data we'll have at each step in the data pipeline for example x is a model input and ŷ a model output. See AbstractBlockTask for more info.\n\nEmbeddingTask block encodings sample block encodedsample x y ŷ encodings sample blocks sample x y ŷ encodedsample blocks encodings \nWith this helper defined, we can create a learning task for our task: Image{2}() is the kind of data we want to learn with and ImagePreprocessing makes sure to encode and decode these images so they can be used to train a model.\n\ntask EmbeddingTask means SVector stds SVector C Gray Float32 buffered \nWith the learning task set up, we can use encode to get samples ready to be input to a model, and all show* functions to visualize data at various points of the pipeline:\n\nx task data task x \nFor later training, the last thing we need to do with the data is to create a data iterator over batches of encoded samples. Since the dataset comfortably fits into memory, we preload it all at once by using collect on the DataLoader. This saves us having to reload each image again every epoch.\n\ntd data task td \nBATCHSIZE dl _ data task BATCHSIZE pctgval dataiter collect dl \nWith that, we have a data iterator of batches that we can use in a training loop just by iterating over it:\n\nxs dataiter print size xs \nNext, we need to construct a model and define a loss function so it can be optimized.\n\nModelling\n\nThe variational autoencoder consists of two parts: an encoder, and a decoder. The encoder takes in data and outputs parameters of a probability distribution. These are then used to sample latent vectors which are fed into the decoder to produce new samples. A loss function (ELBO) rewards the model if the outputs are similar to the inputs. The challenge is that the latent space is of much lower dimensionality than the data space, so the model needs to learn to compress the information contained in the data. If you're interested in more mathematical background on VAEs and the loss function, Lilian Weng has written a great write-up on autoencoders. The architecture looks like this:\n\nDiagram of VAE architecture\n\nWe define the Variational Autoencoder model as a new type that wraps an encoder and decoder model and define the forward pass and loss function as regular Julia functions.\n\nVAE E D encoder E decoder D VAE vae VAE xs μ logσ² vae encoder xs zs sample_latent μ logσ² x̄s vae decoder zs x̄s μ logσ² Random randn! Statistics mean sample_latent μ AbstractArray T logσ² AbstractArray T T μ exp logσ² randn! similar logσ² βELBO x x̄ μ logσ² β reconstruction_error mean sum x̄ x dims kl_divergence mean sum μ exp logσ² logσ² dims reconstruction_error β kl_divergence \nNext we define the encoder and decoder models by composing basic Flux.jl layers. Dlatent is the size of the latent space and controls how much the model has to compress the information. Feel free to try out smaller or larger numbers and see how the quality of the generated images changes.\n\nSIZE Din prod SIZE Dhidden Dlatent encoder Din Dhidden relu tuple Dhidden Dlatent Dhidden Dlatent decoder Dlatent Dhidden relu Dhidden Din sigmoid xs reshape xs SIZE model VAE encoder decoder \nCustom training loop\n\nWhen dealing with a unconvential learning scheme, we usually need to write a custom training loop. FastAI.jl is build on top of FluxTraining.jl which allows you to write custom training loops with very little boilerplate while retaining compatibility with its extensive callback system. In fact, the built-in training loops for supervised learning are defined in just the same way as we will.\n\nVAETrainingPhase AbstractTrainingPhase \nWe just defined our own training phase. All that is required to take advantage of the FastAI.jl framework is to define the FluxTraining.step! function. We'll use a utility, FluxTraining.runstep, to reduce the boilerplate involved in handling callback events and state. runstep's first argument is a function with inputs (handle, state). handle can be used to dispatch events which callbacks can react to. state holds data generated on each call to step! like the batch, gradients, and loss. These are also accessible to callbacks, for example to calculate metrics. We also give runstep the initial step state which just contains our batch.\n\nVAETrainingPhase AbstractTrainingPhase learner phase VAETrainingPhase learner phase xs gs gradient learner μ logσ² learner model encoder xs zs sample_latent μ logσ² x̄s learner model decoder zs LossBegin loss learner lossfn xs x̄s μ logσ² BackwardBegin loss BackwardEnd update! learner optimizer learner gs \nSince the step state is a little different from the supervised case (there are no targets ys), we also overwrite the default task for the ToDevice callback for our training phase:\n\nStepBegin VAETrainingPhase cb learner learner step xs cb movedatafn learner step xs \nTraining\n\nNow we can put the pieces together, creating a Learner.\n\nlearner model βELBO callbacks learner \nTo override the default supervised training loops, we pass our custom training phase and the data iterator we want to run it on to fitonecycle!:\n\nlearner phases VAETrainingPhase dataiter \nFinally, we can visualize how well inputs are reconstructed:\n\nxs task data rand data ypreds _ model xs task xs ypreds \n\n"
    url = "FastAI@pr-288/doc/docs/notebooks/vae.ipynb"
    [[input.files]]
    title = "ShowMakie"
    contents = "ShowMakie([; kwargs...]) <: ShowBackend\nA backend for showing block data that uses Makie.jl figures for visualization.\nKeyword arguments kwargs are passed to the constructed Figures.\nImplementing a Block visualization\nAs with other ShowBackend, implementing a visualization for a block type B <: AbstractBlock requires you to implement showblock!.\nFor ShowMakie, the first argument is a Makie.Axis, i.e. you have to implement\nax Makie Axis block B obs \nThe axis is created by FastMakie.makeaxis. The default options will result in an axis cleaned of all decorations. To customize it, implement FastMakie.axiskwargs(block::B). See the docstring of makeaxis for available options.\n\n"
    url = "FastAI@pr-288/ref/FastAI.ShowMakie"
    [[input.files]]
    title = "predictbatch"
    contents = "predictbatch(task, model, inputs[; device, context])\nPredict targets from a vector of inputs using model by batching them. Optionally apply function device to batch before passing to model and use context instead of the default Inference.\n\n"
    url = "FastAI@pr-288/ref/FastAI.predictbatch"
    [[input.files]]
    title = "Developing"
    contents = "Developing\nThis guide contains information important when developing FastAI.jl. Concretely:\nhow to set up a local development environment\n\nhow to run the tests\n\nhow to preview the documentation locally\n\n\nSetting up FastAI.jl locally for development\nFork FastAI.jl and add it as a dev dependency. You can fork it from the GitHub repository. Then use Pkg to add the fork to your Julia environment:\nPkg Pkg develop url \nYou should now be able to import FastAI (using FastAI) in Julia. If you are using Revise.jl, any changes you make to its source code will also be reflected in your interactive sessions.\nRunning the tests\nLike any Julia package, you can run the entire test suite in an isolated environment using Pkg.test:\nPkg Pkg test \nWhen developing, however, it can be helpful to repeatedly rerun parts of the tests. FastAI.jl uses ReTest.jl to set up tests which makes it possible to run subsets of tests or only tests that have not previously succeeded.\nFirst, activate the test environment and install its dependencies:\nPkg Pkg activate joinpath Pkg devdir Pkg instantiate \nThen, you can run the test suite or subsets of it:\nReTest ReTest fail ReTest not ReTest pass \nLocal documentation preview\nFastAI.jl uses Pollen.jl as its documentation system, which allows you to preview documentation locally.\nFirst, activate the documentation environment and install its dependencies:\nPkg Pkg activate joinpath Pkg devdir Pkg add Pkg PackageSpec url Pkg PackageSpec url Pkg PackageSpec url rev \nNow you can build the documentation locally, giving you a preview at http://localhost:3000. Using the lazy = true will build pages lazily only once you request them on the website, which reduces the build time when you only care about specific pages.\nPollen Pollen servedocs lazy \nAdding documentation pages files\nDocumentation pages correspond to a Markdown .md or Jupyter Notebook .ipynb file that should be stored in the docs/ folder. If a document should show up in the left sidebar of the docs page, add an entry to FastAI/docs/toc.json.\nJupyter Notebooks should be used when they use resources that are not available on the GitHub CI, like a GPU needed for training. You should run them locally and the outputs will be captured and inserted into the HTML page.\n\nMarkdown documents should be preferred for everything else, as they allow the code examples to be run on the GitHub CI, meaning they'll stay up-to-date unlike a notebook that has to be manually rerun.\n\n\nBoth formats support the Markdown syntax of Publish.jl and in markdown files the cell syntax of Publish.jl can be used to mark code cells. These will be run and the output is inserted into the HTML page.\nLinking to documentation\nFor a new documentation file to be discoverable, you have to add an entry to the nested Markdown list in toc.md, which corresponds to the sidebar in the documentation (updating the sidebar currently requires interrupting and reincluding the file that starts the development server).\nDocumentation pages can also link to each other using standard Markdown link syntax.\nReferencing code symbols\nSymbols like fitonecycle! can be referenced by using the cross-referencing syntax [`fitonecycle!`](#)  which will link to and create a reference page from the symbol's docstrings. It will also be added as an entry on the references page.\n\n"
    url = "FastAI@pr-288/doc/DEVELOPING.md"
    [[input.files]]
    title = "obsslice"
    contents = "obsslice"
    url = "FastAI@pr-288/ref/FastAI.Datasets.obsslice"
    [[input.files]]
    title = "PropagateWrapper"
    contents = "abstract type PropagateWrapper\nDefines the default propagation behavior of a WrapperBlock when an encoding is applied to it.\nPropagation refers to what happens when an encoding is applied to a WrapperBlock. If no encode method is defined for a wrapper block wrapper, encode is instead called on the wrapped block. Propagating the wrapper block means that the block resulting from encoding the wrapped block is rewrapped in wrapper..\nwrapper = Wrapper(block)\n# propagate\nencodedblock(enc, wrapper) = Wrapper(encodedblock(enc, wrapped(wrapper)))\n\n# don't propagate\nencodedblock(enc, wrapper) = encodedblock(enc, wrapped(wrapper))\nThe following wrapping behaviors exist:\nPropagateAlways: Always propagate. This is the default behavior.\n\nPropagateNever: Never propagate\n\nPropagateSameBlock: Only propagate if the wrapped block is unchanged by the encoding\n\n\n\n"
    url = "FastAI@pr-288/ref/FastAI.PropagateWrapper"
    [[input.files]]
    title = "showsamples"
    contents = "showsample([backend], task, sample)\nShow a vector of unprocessed samples for LearningTask task to backend::ShowBackend.\nExamples\ndata blocks loaddataset task data samples data i i task samples task samples \n\n"
    url = "FastAI@pr-288/ref/FastAI.showsamples"
    [[input.files]]
    title = "FastAI/datasets/Datasets.jl"
    contents = "FastAI FastAI ObsView MLDatasets DataDeps FilePathsBase FilePathsBase filename FileIO InlineTest include include include include include __init__ ObsView \n"
    url = "FastAI@pr-288/src/datasets/Datasets.jl"
    [[input.files]]
    title = "encodetarget"
    contents = "encodetarget"
    url = "FastAI@pr-288/ref/FastAI.encodetarget"
    [[input.files]]
    title = "decodey!"
    contents = "decodey!"
    url = "FastAI@pr-288/ref/FastAI.decodey!"
    [[input.files]]
    title = "grandparentname"
    contents = "grandparentname"
    url = "FastAI@pr-288/ref/FastAI.Datasets.grandparentname"
    [[input.files]]
    title = "FastAI/Registries/tasks.jl"
    contents = "_taskregistry name registry Registry id Field String name formatfn x sprint show x name Field String name description computefn row key get row key row id blocks Field Any name description filterfn formatfn b FeatureRegistries code_format _formatblock b category Field String name description description Field String name optional description formatfn FeatureRegistries md_format constructor Field Any name description formatfn FeatureRegistries code_format package Field Module name formatfn FeatureRegistries code_format name loadfn row row constructor description _taskregistry kwargs isempty kwargs filter kwargs \n"
    url = "FastAI@pr-288/src/Registries/tasks.jl"
    [[input.files]]
    title = "testrecipe"
    contents = "testrecipe"
    url = "FastAI@pr-288/ref/FastAI.Datasets.testrecipe"
    [[input.files]]
    title = "FastAI/training/lrfind.jl"
    contents = "losses Any lrs Any estimators Any estimates Any learner dataiter learner data training nsteps startlr endlr divergefactor estimators losses Float64 lrs Float64 bestloss Inf scheduler removecallback! learner modelcheckpoint deepcopy learner model learner model modelcheckpoint params params modelcheckpoint optimizer deepcopy learner optimizer runepoch learner _ i batch zip nsteps dataiter lr startlr endlr startlr i nsteps learner optimizer eta lr state learner batch push! losses state loss push! lrs learner optimizer eta bestloss min state loss bestloss state loss bestloss divergefactor throw isnothing scheduler addcallback! learner scheduler model! learner modelcheckpoint losses lrs estimators beta Any est losses lrs slosses losses est beta grads slosses end slosses end log lrs end lrs end i length lrs lr lrs i end argmax grads i end lr beta Any est losses lrs i length lrs lr lrs i end argmin losses est beta i end lr losses lrs losses lrs losses lrs estimators losses lrs estimators est losses lrs est estimators Base show io IO result io result names typeof est name name est result estimators println io println io pretty_table io hcat names result estimates header tf tf_borderless alignment r l result stdout result io result p UnicodePlots lineplot result lrs result losses height xscale log10 width displaysize io UnicodePlots title! p i estimate enumerate result estimates UnicodePlots lines! p estimate maximum result losses estimate minimum result losses color red UnicodePlots annotate! p estimate maximum result losses string round estimate sigdigits show io p p InlineTest res InlineTest show Base DevNull res xs β res similar xs val i x enumerate xs val val β x β res i val β i res \n"
    url = "FastAI@pr-288/src/training/lrfind.jl"
    [[input.files]]
    title = "FastAI/interpretation/showinterpretable.jl"
    contents = "backend encodings block obs res block backend block encodings block obs isnothing res error block_ obs_ res backend block_ obs_ backend encodings block obss AbstractVector blockobss block backend block encodings block obs obs obss any isnothing blockobss error block_ first first blockobss obss_ last blockobss backend block_ obss_ backend S block B S B hasmethod FastAI Any S B Any f encodings ctx block obs f block encodings nothing f encodings end ctx encodings end block encodings end ctx block obs block obs f encodings ctx blocks Tuple obss Tuple encodings nothing results Tuple f encodings ctx block obs block obs zip blocks obss any isnothing results nothing blocks first results obss_ last results blocks obss_ f encodings ctx title block Pair obs encodings nothing res f encodings ctx block obs isnothing res nothing block_ obs_ res title block_ obs_ \n"
    url = "FastAI@pr-288/src/interpretation/showinterpretable.jl"
    [[input.files]]
    title = "FastAI/interpretation/text.jl"
    contents = "io Any kwargs Any io stdout hlines all alignment l kwargs io hlines hlines alignment alignment kwargs backend backend io io backend title block Pair obs printstyled io title bold println io io backend block obs io backend blocks Tuple obss Tuple header block Pair first block block blocks blocks block Pair last block block block blocks data reshape PrettyTables AnsiTextCell io IOContext io color backend block obs block obs zip blocks obss pretty_table io data header header noheader all isempty header backend kwargs io backend blocks Tuple obsss AbstractVector header block Pair first block block blocks blocks block Pair last block block block blocks rows obss obsss row reshape AnsiTextCell io IOContext io color backend block obs block obs zip blocks obss push! rows row tabledata reduce vcat rows pretty_table io tabledata header header noheader all isempty header backend kwargs io backend block obss AbstractVector io backend block map obs obs obss io block obs print io obs io block obs print io obs io block obs print io obs io block obs sum obs obs softmax obs obs round obs sigdigits plot UnicodePlots barplot block classes obs width compact print IOContext io color plot \n"
    url = "FastAI@pr-288/src/interpretation/text.jl"
    [[input.files]]
    title = "showencodedsamples"
    contents = "showencodedsamples([backend], task, encsamples)\nShow a vector of encoded samples encsamples to backend.\n\n"
    url = "FastAI@pr-288/ref/FastAI.showencodedsamples"
    [[input.files]]
    title = "Continuous"
    contents = "Continuous(size) <: Block\nBlock for collections of numbers. obs is a valid observation if it's length is size and contains Numbers.\n\n"
    url = "FastAI@pr-288/ref/FastAI.Continuous"
    [[input.files]]
    title = "makeavailable"
    contents = "makeavailable"
    url = "FastAI@pr-288/ref/FastAI.Datasets.makeavailable"
    [[input.files]]
    title = "loaddata"
    contents = "loaddata"
    url = "FastAI@pr-288/ref/FastAI.Datasets.loaddata"
    [[input.files]]
    title = "ROOT_URL_TSClassification"
    contents = "ROOT_URL_TSClassification"
    url = "FastAI@pr-288/ref/FastAI.Datasets.ROOT_URL_TSClassification"
    [[input.files]]
    title = "discrlr_optimizer"
    contents = "frozen_optimizer(optim, grouper, model, factor)\nCreate an optimizer that discounts updates parameters which ParamGrouper puts into group 1 by factor.\n\n"
    url = "FastAI@pr-288/ref/FastAI.discrlr_optimizer"
    [[input.files]]
    title = "FastAI/blocks/continuous.jl"
    contents = "size Int block x block size length x eltype x Number block rand block size outblock yblock outblock size yblock size error outblock yblock Losses mse \n"
    url = "FastAI@pr-288/src/blocks/continuous.jl"
    [[input.files]]
    title = "FastAI/training/onecycle.jl"
    contents = "learner nepochs Int maxlr phases learner data training learner data validation wd kwargs nsteps length phases scheduler nepochs nsteps maxlr kwargs wdoptim wd learner optimizer wd learner optimizer learner optimizer wdoptim learner scheduler _ nepochs phase data phases learner phase data optim wd wd optim optim wd i findfirst o o optim os isnothing i wd optim os wd optim os i optim os i end optim Adam optim Adam Adam optim os o Adam o os o os Adam \n"
    url = "FastAI@pr-288/src/training/onecycle.jl"
    [[input.files]]
    title = "getbatch"
    contents = "getbatch(learner[; validation = false, n = nothing])\nGet a batch of data from learner. Take a batch of training data by default or validation data if validation = true. If n take only the first n samples from the batch.\n\n"
    url = "FastAI@pr-288/ref/FastAI.getbatch"
    [[input.files]]
    title = "makebatch"
    contents = "makebatch(task, data, [idxs; context]) -> (xs, ys)\nCreate a batch of encoded data by loading idxs from data container data. Useful for inspection and as input to showbatch. Samples are encoded in context which defaults to Training.\n\n"
    url = "FastAI@pr-288/ref/FastAI.makebatch"
    [[input.files]]
    title = "SHOW_BACKEND"
    contents = "SHOW_BACKEND"
    url = "FastAI@pr-288/ref/FastAI.SHOW_BACKEND"
    [[input.files]]
    title = "showblockinterpretable"
    contents = "showblockinterpretable(backend, encodings, block, obs)\nDecode block successively by applying encodings until a block is gotten that can be shown by backend. Useful to visualize encoded data that is not directly interpretable, for example an Image{2} representing an encoded Image.\nExamples\nencodings block x block encodings block x encodings block x \n\n"
    url = "FastAI@pr-288/ref/FastAI.showblockinterpretable"
    [[input.files]]
    title = "Siamese image similarity"
    contents = "Siamese image similarity\n\nThis tutorial is adapted from this tutorial in fast.ai's documentation. It tries to stay as close to the original as possible, but diverges where the APIs differ.\n\nIn this tutorial, we will see how to deal with a new type of task using the middle layer of the fastai library. The example we will use is a Siamese network, that takes two images and determine if they are of the same class or not. In particular we will see:\nhow to quickly get DataLoaders from a standard PyTorch Datasets\n\nhow to adapt this in a Transform to get some of the show features of fastai\n\nhow to add some new behavior to show_batch/show_results for a custom task\n\nhow to write a custom DataBlock\n\nhow to create your own model from a pretrained model\n\nhow to pass along a custom splitter to Learner to take advantage of transfer learning\n\n\nUPDATE\n\nSetup\n\nSince we'll be implementing some image operations manually, we'll add the Images package:\n\nPkg Pkg add \nPreparing the data\n\nTo make our data ready for training a model, we need to create data iterators for training and validation, for example using DataLoader. Usually, the first step is to create a data container that is then wrapped inside a DataLoader. Unlike in fast.ai, FastAI.jl separates the loading part from the encoding part. In this case, loading means getting pairs of images and encoding includes preprocessing and augmenting them. We'll first create a data container that just loads pairs of images and later show how to apply transforms on top of that.\n\nCairoMakie CairoMakie activate! type \nUsing the low-level API\n\nFirst we'll use datasets to download and untar the dataset and then find all image files.\n\npath joinpath load files path filterfn files \nWe can open the first image and have a look at it. Note that array indices start at 1 in Julia.\n\nimage files \nLet's wrap all the standard preprocessing (resize and conversion to tensor and reordering of the channels) in one helper function. Note some differences to Python here:\nimages by default are 2D-arrays of pixels, so we need to use channelview to get a 3D array with the color dimension expanded\n\npixel values are assumed to be between 0 and 1, so we do not need to divide them by 255\n\n\nFor flexibility, we also separate the loading (loadfile) from the transformations applied to the image.\n\nImages transform_image image sz image_resized imresize convert RGB N0f8 image sz sz a permuteddimsview channelview image_resized \ntransform_image files summary \nWe can see the label of our image is in the filename, before the last _ and some number. We can then use a regex expression to create a label function:\n\nlabel_func path match path label_func files \nNow let's gather all unique labels:\n\nlabels map label_func files length unique labels \nWe could now use mapobs to create a data container from our list of files. It applies a function like loading an image lazily and we can get single observations using getobs and the number of observtions with numobs. It is the same as a torch.utils.data.Dataset. For example, the following example creates a data container with tuples of an image and a category that could be used for image classification\n\ndata files file file label_func file data image label data \nTo create our Siamese datasets, however, we will need to create tuples of images for inputs and the target will be true if the images are of the same class, false otherwise.\n\nFirst we'll shuffle the files and split them into a training and a validation set\n\nRandom idxs shuffle length files cut round Int length idxs trainidxs valididxs idxs cut idxs cut end trainfiles validfiles files trainidxs files valididxs summary trainfiles validfiles \nLet's create a custom data container that returns pairs of indices and a Boolean indicating whether the label is the same. Half of the pairs will have the same label, and half will not. Additionally, during training the other image will be chosen randomly while for the validation the pairs will always be the same.\nWhile you can get far with basic data containers like loadfolderdata and transformations like mapobs and filterobs, sometimes it's simplest to create a custom data container. You just need to implement getobs and numobs for your type, similar to how you would implement __getindex__ and __len__ for a PyTorch Dataset.\n\nSiamesePairs labels same other valid SiamesePairs labels valid ulabels unique labels same Dict label i i l enumerate labels l label label ulabels other Dict label i i l enumerate labels l label label ulabels SiamesePairs labels same other valid si SiamesePairs idx Int rng si valid MersenneTwister idx Random GLOBAL_RNG rand rng idx rand rng si same si labels idx idx rand rng si other si labels idx si SiamesePairs length si labels \nWe can combine this data container that gives us pairs of indices with the files and map the loading and preprocessing functions over it to get a data container that is ready to be passed to a DataLoader.\n\nsiamesedata files valid transformfn identity labels map label_func files si SiamesePairs labels valid valid si obs i j same obs image1 transformfn files i image2 transformfn files j image1 image2 same \ntraindata siamesedata trainfiles transformfn transform_image validdata siamesedata validfiles transformfn transform_image valid \nWe can see that each observation consists of two images and a Boolean:\n\nsummary traindata \nTo use the above data containers in training, we can simply pass them to a DataLoader.\n\ntraindl traindata validdl validdata \n\nNext let's look at how we can extend this example to make use FastAI.jl's data augmentation, visualize our data and more.\n\nUsing the Data Block API\n\nThis is where FastAI.jl's API diverges a bit from fast.ai's. Note how above, we made sure to separate the data container creation and loading from disk from the preprocessing that is applied to every observation. In FastAI.jl, the preprocessing or \"encoding\" is implemented through a learning task. Learning tasks contain any configuration and, beside data processing, have extensible functions for visualizations and model building. One advantage of this separation between loading and encoding is that the data container can easily be swapped out as long as it has observations suitable for the learning task (in this case a tuple of two images and a Boolean). It also makes it easy to export models and all the necessary configuration.\n\nThe easiest way to create learning tasks is using the data block API which should suit the very most of all use cases. It is also possible to directly implement the lower-level LearningTask interface.\n\nThe best way to understand it is to use it, so let's build a learning task for Siamese image similarity. We specify the kinds of input and target data as blocks. Here, we have two 2D images as input ((Image{2}(), Image{2}())) and a binary label (Label([true, false])) as output. We also pass in a tuple of encodings that describe how the data is transformed before being fed to a model. Here ProjectiveTransforms resizes the images to the same size, ImagePreprocessing converts the images to the right format and OneHot one-hot encodes the labels.\n\ntask buffered sharestate buffered \nWe can get a better understanding of the representations the data goes through using describetask:\n\ntask \nWe can reuse all the code above for creating the data container, we just omit the preprocessing function. taskdataloaders constructs training and validation data loaders from data containers by mapping the task encoding over the data containers:\n\ntraindata siamesedata trainfiles valid validdata siamesedata validfiles valid traindl valdl traindata validdata task \nThe above is equivalent to lazily mapping the encoding over the data containers and creating a DataLoader from them.\n\ntraindl sample task sample traindata validdl sample task sample validdata \n\nBy separating the data container preparation from the task-specific data preprocessing and augmentation, we were able to completely reuse the data container creation and quickly add FastAI.jl's data augmentation.\n\nWith the data iterators ready for training, we still need a model to train. For the Siamese similarity setting, a common model architecture is a encoder/feature extractor that is applied to both images and a head that transforms the concatenated image features, resulting in a similar/not similar categorical output.\n\nFlux.jl makes it easy to create models in a similar fashion to PyTorch modules. The forward pass is implemented by overloading the call operator, and the backward pass is automatically generated.\n\nSiameseModel E H encoder E head H SiameseModel m SiameseModel xs1 xs2 m head cat m encoder xs1 m encoder xs2 dims \nWe'll use a XResNet model for the encoder and the same head that is used for classification models. Using Flux.outputsize we can check how many channels the encoder outputs without having to evaluate the model:\n\nMetalhead encoder Models xresnet18 encoder Metalhead ResNet50 pretrain layers end h w ch b encoder \nWe need to double this feature count since the head gets the concatenated features from two images:\n\nhead Models visionhead ch \nmodel SiameseModel encoder head \nLet's test the model works on a batch of training data:\n\nxs ys first traindl ŷs model xs \nWe'll use categorical crossentropy (for logits) as a loss function and ADAM as optimizer:\n\nlossfn logitcrossentropy optimizer \nNow we can create a Learner and start training following the usual process.\n\ncallbacks learner model traindl valdl optimizer lossfn callbacks \nplot learner \nencoder Metalhead ResNet50 pretrain layers end h w ch b encoder head Models visionhead ch model SiameseModel encoder head learner model traindl valdl lossfn callbacks \nlearner \n"
    url = "FastAI@pr-288/doc/docs/notebooks/siamese.ipynb"
    [[input.files]]
    title = "FastAI/datablock/models.jl"
    contents = "inblock outblock inblock outblock inblock \n"
    url = "FastAI@pr-288/src/datablock/models.jl"
    [[input.files]]
    title = "encodeinput!"
    contents = "encodeinput!"
    url = "FastAI@pr-288/ref/FastAI.encodeinput!"
    [[input.files]]
    title = "Changelog"
    contents = "Changelog\nAll notable changes to this project will be documented in this file.\nThe format is based on Keep a Changelog, and this project adheres to Semantic Versioning.\nv0.5 (unreleased)\nChanged\n(BREAKING): Now uses MLUtils.jl to create and load datasets and data containers\nReplaces dependencies MLDataPattern.jl, LearnBase.jl, and DataLoaders.jl\n\nData containers must now implement the Base.getindex/MLUtils.getobs and Base.length/MLUtils.numobs interfaces.\n\nPreviously exported MLDataPattern.datasubset has been replaced by MLUtils.ObsView\n\nDocumentation has been updated appropriately\n\n\n\n(BREAKING): FastAI.Vision now lives in a separate package FastVision that holds all computer vision-related functionality.\n\n(BREAKING): FastAI.Tabular now lives in a separate package FastTabular that holds all tabular data-related functionality.\n\n\nRemoved\n(BREAKING): FastAI.Models submodule. Models submodule of domain libraries, e.g. FastVision.Models should now be used.\n\n\nv0.4.3 (2022/05/14)\nAdded\nFeature registries let you find datasets, data recipes and learning tasks for your projects. It is now easier to search for functionality related to kinds of data and load it. See the updated discovery tutorial\n\n added first support for text datasets, adding the Paragraph block and FastAI.Textual submodule (https://github.com/FluxML/FastAI.jl/pull/207)\n\n\nRemoved\nthe old APIs for registries have been removed and functionality for accessing them (finddatasets, loaddataset) has been deprecated. See the updated docs for how to find functionality using the new feature registries.\n\n\nv0.4.2 (2022/04/30)\nAdded\nCompatibility with FluxTraining.jl v0.3 (https://github.com/FluxML/FastAI.jl/pull/223)\n\n\nv0.4.1\nAdded\nNew documentation frontend based on Pollen.jl: https://fluxml.ai/FastAI.jl/dev/i/\n\nNow supports Flux.jl v0.13 (https://github.com/FluxML/FastAI.jl/pull/202)\n\n\nChanged\nNow has ImageIO.jl as a dependency to ensure that fast jpg loading using JpegTurbo.jl is used\n\n\nv0.4.0 (2022-03-19)\nAdded\nMade block-based learning method more modular. SupervisedMethod now supplants BlockMethod.  PR\ngetencodings and getblocks should now be used to get block information and encodings from a method\n\nSee the [new tutorial training a Variational Autoencoder].\n\nSee also the docstrings for AbstractBlockTask and SupervisedTask\n\n\n\n\nChanged\n(BREAKING): all learning method names have been renamed to task, i.e method* -> task* and Method* -> Task*. Specifically, these exported symbols are affected:\nBlockMethod -> BlockTask,\n\ndescribemethod -> describetask,\n\nmethodmodel -> taskmodel,\n\nmethoddataset -> taskdataset,\n\nmethoddataloaders -> taskdataloaders,\n\nmethodlossfn -> tasklossfn,\n\nfindlearningmethods -> findlearningtasks,\n\nmethodlearner -> tasklearner,\n\nsavemethodmodel -> savetaskmodel,\n\nloadmethodmodel -> loadtaskmodel\n\n\n\nBlockMethod now deprecated in favor of SupervisedMethod\n\n(INTERNAL) domain-specific functionality has moved to submodules FastAI.Vision (computer vision) and FastAI.Tabular (tabular data). Exports of FastAI are not affected.\n\n(INTERNAL) test suite now runs on InlineTest.jl\n\n\nRemoved\nv0.3.0 (2021/12/11)\nAdded\nA new API for visualizing data. See this issue for motivation. This includes:\nHigh-level functions for visualizing data related to a learning method: showsample,  showsamples, showencodedsample, showencodedsamples, showbatch, showprediction, showpredictions, showoutput, showoutputs, showoutputbatch\n\nSupport for multiple backends, including a new text-based show backend that you can use to visualize data in a non-graphical environment. This is also the default unless Makie is imported.\n\nFunctions for showing blocks directly: showblock, showblocks\n\nInterfaces for extension: ShowBackend, showblock!, showblocks!\n\n\n\n\nRemoved\nThe old visualization API incl. all its plot* methods: plotbatch, plotsample, plotsamples, plotpredictions\n\n\n0.2.0 (2021/09/21)\nAdded\nHigh-level API \"FasterAI\"\ndataset recipes\n\nlearning method helpers\n\nFind datasets and learning methods based on Blocks: finddatasets, findlearningmethods\n\nloaddataset for quickly loading data containers from configured recipes\n\n\n\nData container recipes (DatasetRecipe, loadrecipe)\n\nDocumentation setions for FasterAI interfaces:\nDiscovery\n\nBlocks and encodings\n\n\n\nNew interfaces\nblockbackbone creates a default backbone for an input block\n\n\n\nSupport for tabular data along with recipes and learning methods:\nTabular classification tutorial\n\nTabularPreprocessing, TableRow, TableDataset, TabularClassificiationSingle, TabularRegression\n\n\n\n\nChanged\nDocumentation sections to reference FasterAI interfaces:\nREADME\n\nIntroduction\n\nData containers\n\nCombined how-tos on training into a single page\n\n\n\nBreaking changes to methodlearner:\nnow accepts callbacks as kwarg\n\nvaliddata no longer keyword\n\nmodel and backbone now kwargs; isbackbone removed. if neither backbone or model are given, uses blockbackbone for default backbone.\n\nsee updated docstring for details\n\n\n\n\n\n"
    url = "FastAI@pr-288/doc/CHANGELOG.md"
    [[input.files]]
    title = "ROOT_URL_MonashRegression"
    contents = "ROOT_URL_MonashRegression"
    url = "FastAI@pr-288/ref/FastAI.Datasets.ROOT_URL_MonashRegression"
    [[input.files]]
    title = "registerrecipes"
    contents = "registerrecipes"
    url = "FastAI@pr-288/ref/FastAI.Registries.registerrecipes"
    [[input.files]]
    title = "finetune!"
    contents = "finetune!(learner, nepochs[, base_lr = 0.002; kwargs...])\nBehaves like the fastai implementation fastai.Learner.fine_tune.\nKeyword arguments\nfreezeepochs = 1: Number of epochs to train with the backbone completely frozen.\n\ngrouper = FastAI.defaultgrouper(learner.model): ParamGrouper which assigns groups 1 (backbone) or 2 (head) for every parameter in learner.model. The default expects learner.model to be a Chain(backbone, head).\n\nbackbone_factor = 0.1: Factor by which updates to backbone model are discounted during the second phase of training.\n\n\nAny additional keyword arguments are passed to fitonecycle!.\n\n"
    url = "FastAI@pr-288/ref/FastAI.finetune!"
    [[input.files]]
    title = "encode"
    contents = "encode(encoding, context, block, obs)\nencode(encoding, context, blocks, obss)\nencode(encodings, context, blocks, obss)\nApply one or more Encodings to observation(s) obs.\n\n"
    url = "FastAI@pr-288/ref/FastAI.encode"
    [[input.files]]
    title = "loadtaskmodel"
    contents = "loadtaskmodel(path) -> (task, model)\nLoad a trained model along with a task from path that were saved using savetaskmodel.\nJLD2.jl is used for serialization.\n\n"
    url = "FastAI@pr-288/ref/FastAI.loadtaskmodel"
    [[input.files]]
    title = "mockmodel"
    contents = "mockmodel(task)\nGenerate a model compatible with task for testing.\n\nmockmodel(xblock, ŷblock)\nmockmodel(task::AbstractBlockTask)\nCreate a fake model that maps batches of block xblock to batches of block ŷblock. Useful for testing.\n\n"
    url = "FastAI@pr-288/ref/FastAI.mockmodel"
    [[input.files]]
    title = "Datasets"
    contents = "Datasets"
    url = "FastAI@pr-288/ref/FastAI.Datasets"
    [[input.files]]
    title = "TimeSeries Classification"
    contents = "Pkg Pkg activate Pkg instantiate \n\nFastTimeSeries \nTimeSeries Classification\n\ndata blocks load \ngetobs gets us a sample from the TimeSeriesDataset. It returns a tuple with the input time series and the correspodning label.\n\ninput class sample data \nNow we create a learning task for time-series classification. This means using the time-series to predict labels. We will use the TimeSeriesRow block as input and Label block as the target.\n\ntask blocks TSPreprocessing blocks data table \nThe encodings passed in transform samples into formats suitable as inputs and outputs for a model\n\nLet's check that samples from the created data container conform to the blocks of the learning task:\n\ntask blocks sample sample \nTo get an overview of the learning task created, and as a sanity test, we can use describetask. This shows us what encodings will be applied to which blocks, and how the predicted ŷ values are decoded.\n\ntask \nencoded_sample task sample \nVisualization Tools for TimeSeries\n\nsample data \ntask sample \nblocks sample \nTraining\n\nWe will use a StackedLSTM as a backbone model, and a Dense layer at the front for classification. taskmodel knows how to do this by looking at the datablocks used.\n\nbackbone FastTimeSeries Models StackedLSTM \nmodel task backbone \nWe can tasklossfn to get a loss function suitable for our task.\n\nlossfn task \nNext we create a pair of training and validation data loaders. They take care of batching and loading the data in parallel in the background.\n\ntraindl validdl data task \nWe will use an Adam optimzer for this task.\n\noptimizer ADAM \nWe create callbacks to get the accuracy during the training\n\ncallbacks \nWith the addition of an optimizer and a loss function, we can now create a Learner and start training.\n\nlearner model lossfn data traindl validdl optimizer optimizer callbacks callbacks \nlearner \nWe can save the model for later inference using savetaskmodel:\n\ntask learner model force \n\n"
    url = "FastAI@pr-288/doc/docs/notebooks/timeseriesclassification.ipynb"
    [[input.files]]
    title = "setup"
    contents = "setup(Block, data)\nCreate an instance of block type Block from data container data.\nExamples\n\nsetup(Encoding, block, data; kwargs...)\nCreate an encoding using statistics derived from a data container data with observations of block block. Used when some arguments of the encoding are dependent on the dataset. data should be the training dataset. Additional kwargs are passed through to the regular constructor of Encoding.\nExamples\nimages labels blocks loaddataset FastVision FastVision images buffered \ndata block loaddataset FastTabular block data \n\n"
    url = "FastAI@pr-288/ref/FastAI.setup"
    [[input.files]]
    title = "Feature registries in FastAI.jl"
    contents = "ImageShow _show reg show IOContext stdout displaysize reg \nFeature registries in FastAI.jl\n\nDatasets\n\n_show \nWe can get more information on a specific dataset:\n\n_show \nAnd load it, triggering a lazy download:\n\nload \nDataset recipes\n\nOf course, to load datasets into a format that we can work with, FastAI.jl has so-called \"dataset recipes\".\n\n_show \nWe can likewise look at an entry:\n\nrecipe _show \nAnd load it, giving us a ready-to-use data container and blocks:\n\ndata blocks load recipe data \nLearning tasks\n\nFinally, learning tasks can also be listed:\n\n_show \n_show \ntask load blocks size \nlearner task data \nFinding features\n\nAside from listing a big table with features, we can also find entries that are relevant to us.\nFor example, find all the datasets that have already been downloaded:\n\ndownloaded description ismissing _show \nFind all dataset recipes with classification targets:\n\nblocks Any _show \nFind all learning tasks with image inputs:\n\nblocks Any _show \nOutlook\n\nThis work will support other future efforts:\nDomain libraries: make it easy for third-party libraries to contribute features (datasets, recipes, models, tasks, encodings...) and easy for users to discover these features.\n\nNo-code interfaces: having a consistent way to search for features and relating them to relevant Blocks makes it possible to build no-code, dropdown-based interfaces to choose an appropriate dataset, find a learning task, or build a model for a task; and finally, train a model.\n\n\n\n"
    url = "FastAI@pr-288/doc/docs/notebooks/registries.ipynb"
    [[input.files]]
    title = "blockname"
    contents = "blockname(block)\nA short name describing a block that can be used in visualizations and other diagrams.\n\n"
    url = "FastAI@pr-288/ref/FastAI.blockname"
    [[input.files]]
    title = "FastAI/datasets/load.jl"
    contents = "dir pattern splitfn nothing filterfn nothing loadfn nothing data MLDatasets FileDataset identity dir pattern filterfn nothing isempty data data filterfn data splitfn nothing data splitfn data loadfn nothing splitfn nothing data loadfn data data Dict zip keys data map d loadfn d values data data p String splitdir p p String splitdir p f f f f re Regex f re f re Regex f isnothing match re f file AbstractPath string file file String file Val Symbol split file end file String Val FileIO load file \n"
    url = "FastAI@pr-288/src/datasets/load.jl"
    [[input.files]]
    title = "How to log to TensorBoard"
    contents = "How to log to TensorBoard\nTensorBoard is a format and viewer for logs of model training and can be used to inspect and compare the results of training runs. We can log step and epoch metrics, hyperparameters and even visualizations to TensorBoard using FluxTraining.jl's logging callbacks.\nGenerating logs\nTo use logging callbacks, we need to pass a log backend, here TensorBoardBackend. This design allows flexibly supporting other logging backends like Weights and Biases or neptune.ai in the future.\ndir mktempdir backend dir \nThen we create callbacks for logging metrics and hyperparameters to that backend:\nmetricscb backend hparamscb backend \nLike any other callbacks, these can then be passed to a Learner along with other callbacks and we can start training. By using the Metrics callback we can log metrics other than the loss.\ncallbacks metricscb hparamscb data _ task _ learner task data callbacks callbacks learner \nInspecting logs\nTo inspect the logs you will have to install the tensorboard command-line using pip (you can access the command-line in the Julia REPL by pressing ;).\nshell> pip install tensorboard\nAfter this one-time installation, you can run it by pointing it to the log directory created above:\nshell> tensorboard --logdir $dir\nThis should give you an URL that you can open in a browser which should look like this:\n\nNote that you can also open TensorBoard and it will update as the training progresses.\n\n"
    url = "FastAI@pr-288/doc/docs/howto/logtensorboard.md"
    [[input.files]]
    title = "LRFinderResult"
    contents = "LRFinderResult(lrs, losses)\nResult of the learning rate finder lrfind. Use plot to visualize.\n\n"
    url = "FastAI@pr-288/ref/FastAI.LRFinderResult"
    [[input.files]]
    title = "New visualization tools for FastAI.jl"
    contents = "CairoMakie \n\nNew visualization tools for FastAI.jl\n\nbased on blocks\n\nsupport for different backends (currently text and Makie.jl)\n\nhigh-level functions for use with learning tasks\n\n\n\nLearning tasks\n\ndata blocks loaddataset task blocks \nshowsample shows one sample from the data container:\n\nsample data task sample \nshowencodedsample shows a sample after encodings have been applied to it:\n\nx y task sample task x y \nAny encoded block that can't be visualized (here ImageTensor) is decoded until it can be using the task's encodings.\n\nMarkdown task encodings task blocks Markdown parse \nIt is also possible to show a complete (collated) batch using showbatch:\n\nxs ys task data task xs ys \nIf you used a trained model to create outputs, you can compare these to the true data with showoutputbatch:\n\noutputs reduce hcat rand _ task xs ys outputs \nThese works for any learning task that deal with blocks that have a visualization defined. The following definition is all that is needed to add support for the Image block type to the text backend:\nio block data ImageInTerminal imshow io data \n\ndata blocks loaddataset task blocks \nsample data task sample \nshowblock\n\ndata blocks loaddataset sample data blocks sample \nshowblock and showblocks\n\nUsing showblock, arbitrary visualizations can be created:\n\nblocks obs last blocks blocks obs \nshowblocks visualizes multiple observations:\n\nblocks last blocks _ \nMakie\n\nMakie.jl is no longer a required dependency. Instead, Makie.jl visualizations are only loaded if you have it imported (using Requires.jl). This reduces the load time of FastAI.jl significantly.\n\n\ndata blocks loaddataset task blocks \nshowsample shows one sample from the data container:\n\nsample data task sample \nshowencodedsample shows a sample after encodings have been applied to it:\n\nx y task sample task x y \nAny encoded block that can't be visualized (here ImageTensor) is decoded until it can be using the task's encodings.\n\ntask encodings \nIt is also possible to show a complete (collated) batch using showbatch:\n\nxs ys task data task xs ys \nIf you used a trained model to create outputs, you can compare these to the true data with showoutputbatch:\n\noutputs reduce hcat rand _ task xs ys outputs \nThese works for any learning task that deal with blocks that have a visualization defined. The following definition is all that is needed to add support for the Image block type to the text backend:\nio block data ImageInTerminal imshow io data \n\ndata blocks loaddataset task blocks \nsample data task sample \nshowblock\n\ndata blocks loaddataset sample data blocks sample \ndata blocks loaddataset sample data blocks sample \nshowblock and showblocks\n\nUsing showblock, arbitrary visualizations can be created:\n\nblocks obs last blocks blocks obs \nshowblocks visualizes multiple observations:\n\nblocks last blocks _ \n"
    url = "FastAI@pr-288/doc/docs/notebooks/10_26_showblock.ipynb"
    [[input.files]]
    title = "encodedblock"
    contents = "encodedblock(encoding, block)\nencodedblock(encoding, blocks)\nencodedblock(encodings, blocks)\nReturn the block that is obtained by encoding block with encoding E. This needs to be constant for an instance of E, so it cannot depend on the sample or on randomness. The default is to return nothing, meaning the same block is returned and not changed. Encodings that return the same block but change the data (e.g. ProjectiveTransforms) should return block.\n\n"
    url = "FastAI@pr-288/ref/FastAI.encodedblock"
    [[input.files]]
    title = "Quickstart"
    contents = "Quickstart\n\nThis page follows fastai's quickstart page by quickly showing a few learning tasks. More will be added here as they are added to the library.\n\nFastAI.jl's learning tasks all use the same basic steps and code:\ncreate a data container\n\ncreate a learning task\n\ncreate learner\n\nfit the model\n\nmake predictions or view results\n\n\nIn this quick start, we'll show these steps for a wide range of difference applications and datasets. As you'll see, the code in each case is extremely similar, despite the very different models and data being used.\n\nFastVision FastTabular FastMakie Metalhead CairoMakie CairoMakie activate! type \nComputer vision\n\nClassification\n\nSingle-label\n\nLet's train a model to classify images in the ImageNette dataset, a subset of ImageNet with 10 classes. The following lines download the dataset, prepare a data preprocessing pipeline and create a model suitable for classification that is ready to train:\n\ndata blocks load task blocks size learner task data callbacks backbone ResNet layers end \nWith this, we can start training! Let's train for 10 epochs (i.e. iterations through the whole dataset) and a maximum learning rate of 0.004:\n\nlearner \nLet's save the model for later use or deployment:\n\ntask learner model \nAnd now, look at some example model predictions on the validation dataset:\n\ntask learner \nSegmentation\n\nSegmentation is similar to classfication, but instead of assigning one class to an image, we try to classify every single pixel in an image. The CamVid dataset consists of images taken in traffic with pixel labels for various relevant image parts like the street, sky, sidewalk etc.\n\ndata blocks load task blocks learner task data callbacks backbone ResNet layers end \nlearner \ntask learner model \nAs with all other tasks, let's have a look at some model outputs:\n\ntask learner \nTabular data\n\nFastAI.jl also supports training models to understand tabular data.\n\nClassification\n\nLet's train a model on the Adult dataset that classifies, based on the other attributes, whether a person earns a salary below or above 50k. Since the dimensionality of the data and the model are quite small, this will run fast enough on a CPU, so we don't need to use the ToGPU callback.\n\ndata blocks load task blocks data learner task data callbacks \nlearner \nAs with every task, we can visualize some model predictions. Since it fits the data better, we use the ShowText visualization backend here instead of the default ShowMakie used above.\n\ntask learner backend \n"
    url = "FastAI@pr-288/doc/docs/notebooks/quickstart.ipynb"
    [[input.files]]
    title = "LREstimator"
    contents = "abstract type LREstimator\nEstimator for an optimal learning rate. Needs to implement estimatelr.\nSee Steepest and MinDivByTen.\n\n"
    url = "FastAI@pr-288/ref/FastAI.LREstimator"
    [[input.files]]
    title = "testencoding"
    contents = "testencoding(encoding, block[, obs])\nPerforms some tests that the encoding interface is set up properly for encoding and block. Tests that\nobs is a valid instance block\n\nencode returns a valid encodedblock(encoding, block)\n\ndecode returns a valid decodedblock(encoding, encodedblock(encoding, block)) and that the block is identical to block\n\n\n\n"
    url = "FastAI@pr-288/ref/FastAI.testencoding"
    [[input.files]]
    title = "WrapperBlock"
    contents = "WrapperBlock"
    url = "FastAI@pr-288/ref/FastAI.WrapperBlock"
    [[input.files]]
    title = "showbatch"
    contents = "showbatch([backend], task, batch)\nShow a collated batch of encoded samples to backend.\n\n"
    url = "FastAI@pr-288/ref/FastAI.showbatch"
    [[input.files]]
    title = "FastAI/blocks/many.jl"
    contents = "B block B FastAI many obss all parent many obs obs obss FastAI many parent many _ rand FastAI enc ctx many obss map obss obs enc ctx parent many obs FastAI enc ctx many obss map obss obs enc ctx parent many obs enc block FastAI enc block \n"
    url = "FastAI@pr-288/src/blocks/many.jl"
    [[input.files]]
    title = "typify"
    contents = "typify"
    url = "FastAI@pr-288/ref/FastAI.typify"
    [[input.files]]
    title = "isshowable"
    contents = "isshowable"
    url = "FastAI@pr-288/ref/FastAI.isshowable"
    [[input.files]]
    title = "Named"
    contents = "Named(name, block)\nWrapper Block to attach a name to a block. Can be used in conjunction with Only to apply encodings to specific blocks only.\n\n"
    url = "FastAI@pr-288/ref/FastAI.Named"
    [[input.files]]
    title = "default_showbackend"
    contents = "default_showbackend()\nReturn the default ShowBackend to use. If a Makie.jl backend is loaded (i.e. Makie.current_backend[] !== missing), return ShowMakie. Else, return ShowText.\n\n"
    url = "FastAI@pr-288/ref/FastAI.default_showbackend"
    [[input.files]]
    title = "StatefulEncoding"
    contents = "abstract type StatefulEncoding <: Encoding\nEncoding that needs to compute some state from the whole sample, even if it only transforms some of the blocks. This could be random state for stochastic augmentations that needs to be the same for every block that is encoded.\nThe state is created by calling encodestate(encoding, context, blocks, sample) and passed to recursive calls with the keyword argument state. As a result, you need to implement encode, decode, encode!, decode! with a keyword argument state that defaults to the above call.\nSame goes for decode, which should accept a state keyword argument defaulting to decodestate(encoding, context, blocks, sample)\n\n"
    url = "FastAI@pr-288/ref/FastAI.StatefulEncoding"
    [[input.files]]
    title = "ShowBackend"
    contents = "abstract type ShowBackend\nAbstract type for backends that allow showing blocks of data in an interpretable way.\nExtending\nFor a ShowBackend Backend, you should implement the following methods:\ncreatehandle(::Backend) creates a context that blocks of data can be shown to\n\nshowblock!(handle, ::Backend, block::B, obs) shows a block of type B. This needs to be implemented for every block type you want to be able to show\n\nshowblocks!(handle, ::Backend, blocks, obss) shows a collection of blocks\n\n\n\n"
    url = "FastAI@pr-288/ref/FastAI.ShowBackend"
    [[input.files]]
    title = "DatasetLoader"
    contents = "abstract type DatasetLoader\nA DatasetLoader defines how a dataset can made available and loaded. See DataDepLoader as an example.\nA DatasetLoader has to implement the following functions:\nloaddata\n\nmakeavailable\n\nisavailable\n\n\n\n"
    url = "FastAI@pr-288/ref/FastAI.Datasets.DatasetLoader"
    [[input.files]]
    title = "FastAI/FastAI.jl"
    contents = "FastAI Base NamedTuple Reexport FilePathsBase Optimise Optimise handle Events JLD2 jldsave jldopen Markdown PrettyTables Requires Setfield Test UnicodePlots Statistics InlineTest include include include include include include include include include include include include include include include include include include include include include include include include include include include include include include include include include include Models ObsView plot \n"
    url = "FastAI@pr-288/src/FastAI.jl"
    [[input.files]]
    title = "showblocks!"
    contents = "showblocks([backend], block, obss)\nshowblocks!(handle, backend, block, obss)\nShow a vector of observations obss of the same block type.\nExamples\ndata blocks loaddataset samples data i i range blocks samples \nExtending\nThis is used for showing batches of observations, unlike the Tuple variant of showblock! which assumes an observation consists of multiple blocks.\nUsually, a ShowBackend will show an observation in one row with showblock! and showblocks! will show multiple rows.\n\n"
    url = "FastAI@pr-288/ref/FastAI.showblocks!"
    [[input.files]]
    title = "setschedules!"
    contents = "setschedules!(learner, phase, schedules...)\nSet schedules on learner's Scheduler callback so that training resumes from there.\nIf learner does not have a Scheduler callback yet, adds it.\nlearner learner learner \n\n"
    url = "FastAI@pr-288/ref/FastAI.setschedules!"
    [[input.files]]
    title = "taskdataset"
    contents = "taskdataset(data, task, context)\nTransform data container data of samples into a data container of (x, y)-pairs. Maps encodesample(task, context, sample) over the observations in data.\n\n"
    url = "FastAI@pr-288/ref/FastAI.taskdataset"
