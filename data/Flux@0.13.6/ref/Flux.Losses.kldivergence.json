{"attributes":{"kind":"function","backlinks":[{"tag":"sourcefile","title":"Flux/losses/Losses.jl","docid":"Flux@0.13.6/src/losses/Losses.jl"},{"tag":"sourcefile","title":"Flux/losses/functions.jl","docid":"Flux@0.13.6/src/losses/functions.jl"}],"methods":[{"symbol_id":"Flux.Losses.kldivergence","module_id":"Flux.Losses","file":"losses/functions.jl","line":383,"signature":"(::Signature)"}],"package_id":"Flux@0.13.6","title":"kldivergence","symbol_id":"Flux.Losses.kldivergence","exported":true,"module_id":"Flux.Losses"},"tag":"documentation","children":[{"attributes":{"symbol":"Flux.Losses.kldivergence","line":348,"module":"Flux.Losses","file":"losses/functions.jl"},"tag":"docstring","children":[{"attributes":{},"tag":"md","children":[{"attributes":{"lang":""},"tag":"codeblock","children":["kldivergence(ŷ, y; agg = mean, ϵ = eps(ŷ))\n"],"type":"node"},{"attributes":{},"tag":"p","children":["Return the ",{"attributes":{"href":"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence","title":""},"tag":"a","children":["Kullback-Leibler divergence"],"type":"node"}," between the given probability distributions."],"type":"node"},{"attributes":{},"tag":"p","children":["The KL divergence is a measure of how much one probability distribution is different from the other. It is always non-negative, and zero only when both the distributions are equal."],"type":"node"},{"attributes":{},"tag":"h1","children":["Example"],"type":"node"},{"attributes":{"lang":"jldoctest"},"tag":"codeblock","children":["julia> p1 = [1 0; 0 1]\n2×2 Matrix{Int64}:\n 1  0\n 0  1\n\njulia> p2 = fill(0.5, 2, 2)\n2×2 Matrix{Float64}:\n 0.5  0.5\n 0.5  0.5\n\njulia> Flux.kldivergence(p2, p1) ≈ log(2)\ntrue\n\njulia> Flux.kldivergence(p2, p1; agg = sum) ≈ 2log(2)\ntrue\n\njulia> Flux.kldivergence(p2, p2; ϵ = 0)  # about -2e-16 with the regulator\n0.0\n\njulia> Flux.kldivergence(p1, p2; ϵ = 0)  # about 17.3 with the regulator\nInf\n"],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}