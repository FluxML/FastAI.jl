{"attributes":{"kind":"struct","backlinks":[{"tag":"sourcefile","title":"Flux/deprecations.jl","docid":"Flux@0.13.6/src/deprecations.jl"},{"tag":"sourcefile","title":"Flux/Flux.jl","docid":"Flux@0.13.6/src/Flux.jl"},{"tag":"sourcefile","title":"Flux/optimise/Optimise.jl","docid":"Flux@0.13.6/src/optimise/Optimise.jl"},{"tag":"sourcefile","title":"Flux/optimise/optimisers.jl","docid":"Flux@0.13.6/src/optimise/optimisers.jl"}],"methods":[{"symbol_id":"Flux.Optimise.AdaDelta","module_id":"Flux.Optimise","file":"optimise/optimisers.jl","line":383,"signature":"(::Signature)"},{"symbol_id":"Flux.Optimise.AdaDelta","module_id":"Flux.Optimise","file":"optimise/optimisers.jl","line":379,"signature":"(::Signature)"},{"symbol_id":"Flux.Optimise.AdaDelta","module_id":"Flux.Optimise","file":"optimise/optimisers.jl","line":384,"signature":"(::Signature)"}],"package_id":"Flux@0.13.6","title":"AdaDelta","symbol_id":"Flux.Optimise.AdaDelta","exported":true,"module_id":"Flux.Optimise"},"tag":"documentation","children":[{"attributes":{"symbol":"Flux.Optimise.AdaDelta","line":361,"module":"Flux.Optimise","file":"optimise/optimisers.jl"},"tag":"docstring","children":[{"attributes":{},"tag":"md","children":[{"attributes":{"lang":""},"tag":"codeblock","children":["AdaDelta(ρ = 0.9, ϵ = 1.0e-8)\n"],"type":"node"},{"attributes":{},"tag":"p","children":[{"attributes":{"href":"https://arxiv.org/abs/1212.5701","title":""},"tag":"a","children":["AdaDelta"],"type":"node"}," is a version of AdaGrad adapting its learning rate based on a window of past gradient updates. Parameters don't need tuning."],"type":"node"},{"attributes":{},"tag":"h1","children":["Parameters"],"type":"node"},{"attributes":{},"tag":"ul","children":[{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":["Rho (",{"attributes":{},"tag":"code","children":["ρ"],"type":"node"},"): Factor by which the gradient is decayed at each time step."],"type":"node"}],"type":"node"}],"type":"node"},{"attributes":{},"tag":"h1","children":["Examples"],"type":"node"},{"attributes":{"lang":"julia"},"tag":"codeblock","children":[{"attributes":{},"tag":"julia","children":[{"attributes":{},"tag":"=","children":[{"attributes":{},"tag":"Identifier","children":["opt"],"type":"node"},{"attributes":{},"tag":"Whitespace","children":[" "],"type":"node"},{"attributes":{},"tag":"=","children":["="],"type":"node"},{"attributes":{},"tag":"Whitespace","children":[" "],"type":"node"},{"attributes":{},"tag":"call","children":[{"attributes":{"reftype":"symbol","document_id":"Flux@0.13.6/ref/Flux.Optimise.AdaDelta"},"tag":"reference","children":["AdaDelta"],"type":"node"},{"attributes":{},"tag":"(","children":["("],"type":"node"},{"attributes":{},"tag":")","children":[")"],"type":"node"}],"type":"node"}],"type":"node"},{"attributes":{},"tag":"NewlineWs","children":["\n"],"type":"node"},{"attributes":{},"tag":"NewlineWs","children":["\n"],"type":"node"},{"attributes":{},"tag":"=","children":[{"attributes":{},"tag":"Identifier","children":["opt"],"type":"node"},{"attributes":{},"tag":"Whitespace","children":[" "],"type":"node"},{"attributes":{},"tag":"=","children":["="],"type":"node"},{"attributes":{},"tag":"Whitespace","children":[" "],"type":"node"},{"attributes":{},"tag":"call","children":[{"attributes":{"reftype":"symbol","document_id":"Flux@0.13.6/ref/Flux.Optimise.AdaDelta"},"tag":"reference","children":["AdaDelta"],"type":"node"},{"attributes":{},"tag":"(","children":["("],"type":"node"},{"attributes":{},"tag":"Float","children":["0.89"],"type":"node"},{"attributes":{},"tag":")","children":[")"],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}