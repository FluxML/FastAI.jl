{"attributes":{"kind":"function","backlinks":[{"tag":"sourcefile","title":"Flux/train.jl","docid":"Flux@0.13.11/src/train.jl"},{"tag":"sourcefile","title":"Flux/deprecations.jl","docid":"Flux@0.13.11/src/deprecations.jl"},{"tag":"sourcefile","title":"Flux/Flux.jl","docid":"Flux@0.13.11/src/Flux.jl"}],"methods":[{"symbol_id":"Flux.Train.setup","module_id":"Flux.Train","file":"train.jl","line":49,"signature":"(::Signature)"},{"symbol_id":"Flux.Train.setup","module_id":"Flux","file":"deprecations.jl","line":117,"signature":"(::Signature)"}],"package_id":"Flux@0.13.11","title":"setup","symbol_id":"Flux.Train.setup","exported":true,"module_id":"Flux.Train"},"tag":"documentation","children":[{"attributes":{"symbol":"Flux.Train.setup","line":14,"module":"Flux.Train","file":"train.jl"},"tag":"docstring","children":[{"attributes":{},"tag":"md","children":[{"attributes":{"lang":""},"tag":"codeblock","children":["opt_state = setup(rule, model)\n"],"type":"node"},{"attributes":{},"tag":"p","children":["This is a version of ",{"attributes":{},"tag":"code","children":["Optimisers.setup"],"type":"node"},", and is the first step before using [",{"attributes":{},"tag":"code","children":["train!"],"type":"node"},"](",{"attributes":{"id":"ref"},"tag":"citation","children":[],"type":"node"}," Flux.train!). It differs from ",{"attributes":{},"tag":"code","children":["Optimisers.setup"],"type":"node"}," in that it:"],"type":"node"},{"attributes":{},"tag":"ul","children":[{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":["has one extra check for mutability (since Flux expects to mutate the model in-place, while Optimisers.jl is designed to return an updated model)"],"type":"node"}],"type":"node"},{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":["has methods which accept Flux's old optimisers, and convert them. (The old ",{"attributes":{},"tag":"code","children":["Flux.Optimise.Adam"],"type":"node"}," and new ",{"attributes":{},"tag":"code","children":["Optimisers.Adam"],"type":"node"}," are distinct types.)"],"type":"node"}],"type":"node"}],"type":"node"},{"attributes":{"class":"compat"},"tag":"admonition","children":[{"attributes":{},"tag":"admonitiontitle","children":["New"],"type":"node"},{"attributes":{},"tag":"admonitionbody","children":[{"attributes":{},"tag":"p","children":["This function was added in Flux 0.13.9. It was not used by the old \"implicit\" interface, using ",{"attributes":{},"tag":"code","children":["Flux.Optimise"],"type":"node"}," module and ",{"attributes":{"reftype":"symbol","href":"@ref","title":"","document_id":"Flux@0.13.11/ref/Flux.params"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["Flux.params"],"type":"node"}],"type":"node"},"."],"type":"node"}],"type":"node"}],"type":"node"},{"attributes":{},"tag":"h1","children":["Example"],"type":"node"},{"attributes":{"lang":"jldoctest"},"tag":"codeblock","children":["julia> model = Dense(2=>1, leakyrelu; init=Flux.ones32);\n\njulia> opt_state = Flux.setup(Momentum(0.1), model)  # this encodes the optimiser and its state\n(weight = Leaf(Momentum{Float64}(0.1, 0.9), Float32[0.0 0.0]), bias = Leaf(Momentum{Float64}(0.1, 0.9), Float32[0.0]), σ = ())\n\njulia> x1, y1 = [0.2, -0.3], [0.4];  # use the same data for two steps:\n\njulia> Flux.train!(model, [(x1, y1), (x1, y1)], opt_state) do m, x, y\n         sum(abs.(m(x) .- y)) * 100\n       end\n\njulia> model.bias  # was zero, mutated by Flux.train!\n1-element Vector{Float32}:\n 10.190001\n\njulia> opt_state  # mutated by Flux.train!\n(weight = Leaf(Momentum{Float64}(0.1, 0.9), Float32[-2.018 3.027]), bias = Leaf(Momentum{Float64}(0.1, 0.9), Float32[-10.09]), σ = ())\n"],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}