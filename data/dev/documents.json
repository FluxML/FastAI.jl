[{"doctype":"documentation","id":"references/FastAI.withcallbacks","title":"withcallbacks","text":"Run  f  with  callbacks  on  learner  Existing callbacks on  learner  of the same type as in  callbacks  are swapped during the execution of  f "},{"doctype":"documentation","id":"references/FastAI.lrfindtextplot!","title":"lrfindtextplot!","text":""},{"doctype":"documentation","id":"references/Flux.GlobalMaxPool","title":"GlobalMaxPool","text":"Global max pooling layer Transforms w,h,c,b)-shaped input into 1,1,c,b)-shaped output by performing max pooling on the complete w,h)-shaped feature maps See also  MaxPool   GlobalMeanPool "},{"doctype":"documentation","id":"references/FastAI.Datasets.ROOT_URL_FastAI","title":"ROOT_URL_FastAI","text":""},{"doctype":"documentation","id":"references/FastVision.Models.UNetCombineLayer","title":"UNetCombineLayer","text":""},{"doctype":"document","id":"documents/docs/status.md","title":"Status","text":"Status To-Dos x release DLPipelines.jl update its documentation register x release DataAugmentation.jl add onehot-encoding of  MaskMulti s add tests for  PinOrigin fix bounds assignment for  PinOrigin  on  Keypoints x move basic datasets and dataset utilities from DLDatasets.jl to FastAI.jl x image classification datasets y other datasets x add  ImageSegmentation  method training utilities x  fitonecycle x  finetune x  lrfind wait for Flux 12.0 to be release plot recipes for learning rate finder visualizing losses hyperparameters"},{"doctype":"documentation","id":"references/DataAugmentation.tensortoimage","title":"tensortoimage","text":""},{"doctype":"documentation","id":"references/FastAI.ParamGroups","title":"ParamGroups","text":"model paramgroups model paramgroups model weight paramgroups model weight paramgroups rand nothing A logical grouping of parameters in  m  created by  ParamGrouper   grouper  Parameters in  m  are assigned a group that can be queried using  getgroup(paramgroups param  If a parameter is not assigned a group  getgroup  returns  nothing  Examples"},{"doctype":"documentation","id":"references/FluxTraining.AbstractCallback","title":"AbstractCallback","text":"Supertype of  SafeCallback  Callback  When implementing callbacks you should subtype  SafeCallback  instead"},{"doctype":"documentation","id":"references/FastMakie.makeaxis","title":"makeaxis","text":"Create a  Makie.Axis  that block data can be plotted to When plotting block data using  showblock  keyword arguments from  axiskwargs(block  are passed to this function"},{"doctype":"documentation","id":"references/FastAI.propagatedecode","title":"propagatedecode","text":"Whether the wrapper type should be kept after decoding the wrapped block with  encoding "},{"doctype":"documentation","id":"references/FluxTraining.Phases.ValidationPhase","title":"ValidationPhase","text":"A regular validation phase It iterates over batches in  learner.data.validation  and performs a forward pass Throws the following events  EpochBegin   StepBegin   LossBegin   StepEnd   EpochEnd  Throws the following events in this order EpochBegin  when an epoch starts StepBegin  when a step starts LossBegin  after the forward pass but before loss calculation StepEnd  when a step ends and EpochEnd  when an epoch ends It writes the following step state to  learner.state  grouped by the event from which on it is available StepBegin  xs  and  ys  encoded input and target batch LossBegin  ŷs  model output StepEnd  loss  loss"},{"doctype":"documentation","id":"references/DataAugmentation.showpolygon!","title":"showpolygon!","text":""},{"doctype":"documentation","id":"references/FluxTraining.iterpairs","title":"iterpairs","text":"Iterators over the Cartesian product of  a  with itself skipping any pairs  a b  where  a  b "},{"doctype":"documentation","id":"references/FluxTraining.StopOnNaNLoss","title":"StopOnNaNLoss","text":"Stops the training when a NaN loss is encountered This callback is added by default to every  Learner  unless you pass in  usedefaultcallbacks  false "},{"doctype":"documentation","id":"references/Flux.crosscor_dims","title":"crosscor_dims","text":""},{"doctype":"documentation","id":"references/FastAI.WrapperBlock","title":"WrapperBlock","text":""},{"doctype":"document","id":"documents/docs/notebooks/imagesegmentation.ipynb","title":"Image segmentation","text":"Metalhead CairoMakie CairoMakie activate! type dir load readdir dir classes readlines open joinpath dir images joinpath dir filterfn loadfn masks joinpath dir filterfn loadfn f f classes data images masks image mask sample data image view mask task classes task blocks sample sample xs ys task data task xs ys task backbone Metalhead ResNet layers end model task backbone lossfn task traindl validdl data task optimizer learner model lossfn data traindl validdl optimizer callbacks learner task learner n Image segmentation In the  quickstart section  you saw a short example of how to train an image segmentation model In this tutorial we'll recreate that using the mid-level APIs In image segmentation instead of assigning a class to a whole image as in  image classification  we want to classify each pixel of an image We'll use the CamVid dataset which contains street images with every pixel annotated as one of 32 classes Inside the dataset folder we find  images  a subfolder with all the input images  labels  a subfolder containing the segmentation masks saved as images and  codes.txt  which contains the class names Every line in  codes.txt  corresponds to one class so let's load it To create our data containers we'll use  loadfolderdata  which creates data containers from folder contents The function  loadfn  is applied to the file paths and we use  loadfile  to  loadmask  to get images and masks Now we can get an observation Each mask is an array with elements the same type as our classes here  String  but efficiently stored using integers under the hood Next we need to create a learning task for image segmentation This means using images to predict masks so we'll use the  Image  and  Mask  blocks as input and target Since the dataset is 2D we'll use 2-dimensional blocks The encodings passed in transform samples into formats suitable as inputs and outputs for a model and also allow decoding model outputs to get back to our target format an array of class labels for every pixel Let's check that samples from the created data container conform to the blocks of the learning task We can ascertain the encodings work as expected by creating a batch of encoded data and visualizing it Here the segmentation masks are color-coded and overlayed on top of the image We can use  describetask  to get more information about learning tasks created through the data block API We see which representations our data goes through and which encodings transform which parts With a  task  and a matching data container the only thing we need before we can create a  Learner  is a backbone architecture to build the segmentation model from We'll use a slightly modified ResNet but you can use any convolutional architecture We'll use  taskmodel  to construct the model from the backbone Since we want mask outputs the intermediate feature representation needs to be scaled back up Based on the  Block s we built our task from  taskmodel  knows that it needs to build a mapping  ImageTensor{2  OneHotTensor{2  and constructs a U-Net model In a similar vein  tasklossfn  creates a loss function suitable for comparing model outputs and encoded targets Next we turn our data container into training and validation data loaders that will iterate over batches of encoded data and construct a  Learner  Note that we could also have used  tasklearner  which is a shorthand that calls  taskdataloaders  and  taskmodel  for us Now let's train the model And look at the results on a batch of validation data The data block API can also be used for 3D segmentation but the tutorial for that is still in the works"},{"doctype":"documentation","id":"references/FastAI.Registries.DATARECIPES","title":"DATARECIPES","text":""},{"doctype":"documentation","id":"references/Flux.nfan","title":"nfan","text":"For a layer characterized by dimensions  dims  return a tuple  fan_in fan_out  where  fan_in  is the number of input neurons connected to an output one and  fan_out  is the number of output neurons connected to an input one This function is mainly used by weight initializers e.g  kaiming_normal   Flux.kaiming_normal Examples"},{"doctype":"documentation","id":"references/FastTabular.RECIPES","title":"RECIPES","text":""},{"doctype":"documentation","id":"references/Flux.isleaflike","title":"isleaflike","text":""},{"doctype":"documentation","id":"references/FastAI.predictbatch","title":"predictbatch","text":"Predict  targets  from a vector of  inputs  using  model  by batching them Optionally apply function  device  to batch before passing to  model  and use  context  instead of the default  Inference "},{"doctype":"documentation","id":"references/FluxTraining.LinearRunner","title":"LinearRunner","text":""},{"doctype":"documentation","id":"references/FastAI.mockinput","title":"mockinput","text":"Generate a random  input  compatible with  task "},{"doctype":"documentation","id":"references/Flux.ConvTranspose","title":"ConvTranspose","text":"Standard convolutional transpose layer  filter  is a tuple of integers specifying the size of the convolutional kernel while  in  and  out  specify the number of input and output channels Note that  pad=SamePad  here tries to ensure  size(output,d  size(x,d  stride  Parameters are controlled by additional keywords with defaults  init=glorot_uniform  and  bias=true  See also  Conv  for more detailed description of keywords Examples Constructs a ConvTranspose layer with the given weight and bias Accepts the same keywords and has the same defaults as  ConvTranspose(k::NTuple{N,Integer ch::Pair{<:Integer,<:Integer σ    ConvTranspose Examples"},{"doctype":"documentation","id":"references/Flux.rand32","title":"rand32","text":"Return an  Array{Float32  of the given  size  filled like  rand  or  randn  When the size is not provided  rand32(rng::AbstractRNG  returns a function"},{"doctype":"documentation","id":"references/Flux.Data","title":"Data","text":""},{"doctype":"documentation","id":"references/FluxTraining.setlearningrate!","title":"setlearningrate!","text":""},{"doctype":"documentation","id":"references/FastVision.ImageClassificationSingle","title":"ImageClassificationSingle","text":"Learning task for single-label image classification Images are resized to  size  and classified into one of  classes  Use  ImageClassificationMulti  for the multi-class setting Keyword arguments computestats  false  Whether to compute image statistics on dataset  data  or use default ImageNet stats aug_projections   DataAugmentation.Identity  augmentation to apply during  ProjectiveTransforms  resizing and cropping aug_image   DataAugmentation.Identity  pixel-level augmentation to apply during  ImagePreprocessing C  RGB{N0f8  Color type images are converted to before further processing Use  Gray{N0f8  for grayscale images"},{"doctype":"documentation","id":"references/MLUtils.labelmap","title":"labelmap","text":""},{"doctype":"documentation","id":"references/Flux.CUDAint","title":"CUDAint","text":""},{"doctype":"documentation","id":"references/DataAugmentation.normalize!","title":"normalize!","text":""},{"doctype":"documentation","id":"references/FluxTraining.RunFirst","title":"RunFirst","text":"Return  RunFirst(cb1/cb2  from  resolveconflict cb1 cb2  to indicate that one of the callbacks should always run before the other"},{"doctype":"documentation","id":"references/FastAI.showblocksinterpretable","title":"showblocksinterpretable","text":"Multi-sample version  showblockinterpretable "},{"doctype":"documentation","id":"references/Flux.testmode!","title":"testmode!","text":"Set a layer or model's test mode see below Using  auto  mode will treat any gradient computation as training Note  if you manually set a model into test mode you need to manually place it back into train mode during training phase Possible values include false  for training true  for testing auto  or  nothing  for Flux to detect the mode automatically"},{"doctype":"documentation","id":"references/MLUtils.JoinedData","title":"JoinedData","text":""},{"doctype":"documentation","id":"references/MLUtils.epseltype","title":"epseltype","text":""},{"doctype":"documentation","id":"references/FastAI.setwrapped","title":"setwrapped","text":""},{"doctype":"documentation","id":"references/FluxTraining.ToDevice","title":"ToDevice","text":"Moves model and step data to a device using  movedatafn  for step data and  movemodelfn  for the model For example  ToDevice(Flux.gpu Flux.gpu  moves them to a GPU if available See  ToGPU  By default only moves  step.xs  and  step.ys  but this can be extended to other state by implementing  on(::StepBegin MyCustomPhase ToDevice learner "},{"doctype":"documentation","id":"references/MLUtils.Datasets.make_poly","title":"make_poly","text":"Generates a noisy response for a polynomial of degree  length(coef  using the vector  x  as input and adding  noise  f_randn(length(x  to the result The vector  coef  contains the coefficients for the terms of the polynome The first element of  coef  denotes the coefficient for the term with the highest degree while the last element of  coef  denotes the intercept"},{"doctype":"documentation","id":"references/FastAI.PropagateWrapper","title":"PropagateWrapper","text":"Defines the default propagation behavior of a  WrapperBlock  when an encoding is applied to it Propagation refers to what happens when an encoding is applied to a  WrapperBlock  If no  encode  method is defined for a wrapper block  wrapper   encode  is instead called on the wrapped block Propagating the wrapper block means that the block resulting from encoding the wrapped block is rewrapped in  wrapper  The following wrapping behaviors exist PropagateAlways  Always propagate This is the default behavior PropagateNever  Never propagate PropagateSameBlock  Only propagate if the wrapped block is unchanged by the encoding"},{"doctype":"documentation","id":"references/FastAI.CONTEXTS","title":"CONTEXTS","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.obsslices","title":"obsslices","text":""},{"doctype":"documentation","id":"references/FastVision.Models.xresnet18","title":"xresnet18","text":""},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Image","title":"Image","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.apply!","title":"apply!","text":""},{"doctype":"document","id":"documents/docs/howto/augmentvision.md","title":"How to augment vision data","text":"CairoMakie CairoMakie activate! type data blocks load task blocks xs ys task data fill task xs ys task2 blocks augmentations xs2 ys2 task2 data fill task2 xs2 ys2 task3 blocks augmentations augmentations xs3 ys3 task3 data fill task3 xs3 ys3 How to augment vision data Data augmentation is important to train models with good generalization ability especially when the size of your dataset is limited FastAI.jl gives you high-level helpers to use data augmentation in vision learning tasks but also allows directly using  DataAugmentation.jl  the underlying data augmentation library By default the only augmentation that will be used in computer vision tasks is a random crop meaning that after images keypoints and masks are resized to a similar size a random portion will be cropped during training We can demonstrate this on the image classification task Most learning tasks let you pass additional augmentations as keyword arguments For example  ImageClassification  takes the  aug_projection  and  aug_image  arguments FastAI.jl provides the  augs_projection  helper to quickly construct a set of projective data augmentations Likewise there is an  augs_lighting  helper that adds contrast and brightness augmentation"},{"doctype":"documentation","id":"references/FastVision.DimSize","title":"DimSize","text":""},{"doctype":"documentation","id":"references/FastAI.describetask","title":"describetask","text":""},{"doctype":"documentation","id":"references/FastAI.fitonecycle!","title":"fitonecycle!","text":"Fit  learner  for  nepochs  using a one-cycle learning rate schedule The learning rate starts at  lrmax/div  for  pct_start*nepochs  epochs rising to  lrmax  and then goes down to  lrmax/div_final  over the remaining duration Keyword arguments wd  0  weight decay pct_start  0.25  Percentage of time spent raising the learning rate div  25  Starting learning rate is  lr_max/div div_final  1e5  Ending learning rate is  lr_max/div_final"},{"doctype":"documentation","id":"references/FastTabular.gettransformdict","title":"gettransformdict","text":"The helper functions defined below can be used for quickly constructing a dictionary which will be required for creating various tabular transformations available in DataAugmentation.jl These functions assume that the table in the TableDataset object td has Tables.jl columnaccess interface defined"},{"doctype":"documentation","id":"references/FastVision.RECIPES","title":"RECIPES","text":""},{"doctype":"documentation","id":"references/FastAI.encodetarget","title":"encodetarget","text":""},{"doctype":"documentation","id":"references/Flux.trainmode!","title":"trainmode!","text":"Set a layer of model's train mode see below Symmetric to  testmode  i.e  trainmode!(m mode  testmode!(m mode  Note  if you manually set a model into train mode you need to manually place it into test mode during testing phase Possible values include true  for training false  for testing auto  or  nothing  for Flux to detect the mode automatically"},{"doctype":"documentation","id":"references/FastAI.shouldbatch","title":"shouldbatch","text":"Define whether encoded samples for a learning task should be batched The default is  true "},{"doctype":"documentation","id":"references/FastAI.frozen_optimizer","title":"frozen_optimizer","text":"Create an optimizer that only updates parameters which  ParamGrouper  puts into group  2 "},{"doctype":"documentation","id":"references/FluxTraining.Check","title":"Check","text":""},{"doctype":"documentation","id":"references/MLUtils.AbstractDataContainer","title":"AbstractDataContainer","text":""},{"doctype":"document","id":"documents/docs/background/blocksencodings.md","title":"Blocks and encodings","text":"RGB enc data rand RGB summary data encdata enc data summary encdata data_ enc encdata enc enc enc enc enc enc enc nothing enc enc task data joinpath load filterfn loadfn sample data x y task sample summary x summary y x y task encodings task sample sample summary x summary y task task task encodings Blocks and encodings Unstructured notes on blocks and encodings Blocks A  block  describes the meaning of a piece of data in the context of a learning task For example for supervised learning tasks there is an input block and a target block and we want to learn to predict targets from inputs Learning to predict a cat/dog label  Label([\"cat dog  from 2D images  Image{2  is a supervised image classification task A block is not a piece of data itself Instead it describes the meaning of a piece of data in a context That a piece of data is a block can be checked using  checkblock  block data  A piece of data for the  Label  block above needs to be one of the labels so  checkblock(Label([\"cat dog cat  true  but  checkblock(Label([\"cat dog cat  false  We can say that a data container is compatible with a learning task if every observation in it is a valid sample of the sample block of the learning task The sample block for supervised tasks is  sampleblock  inputblock targetblock  so  sample  getobs(data i  from a compatible data container implies that  checkblock(sampleblock sample  This also means that any data stored in blocks must not depend on individual samples we can store the names of possible classes inside the  Label  block because they are the same across the whole dataset Data pipelines We can use blocks to formalize the data processing pipeline During  training  we want to create pairs of data  x y  s.t  output  model(x  and  loss  lossfn(output y  In terms of blocks that means  model  is a function  x  output  and the loss function maps  outputblock yblock  loss  Usually  input target  x y  and instead we have an encoding step that transforms a sample into representations suitable to train a model on i.e  encode  sample  x y  For the above image classification example we have  sampleblock  Image{2 Label([\"cat dog  but we cannot put raw images into a model and get out a class Instead the image is converted to an array that includes the color dimension and its values are normalized and the class label is one-hot encoded So  xblock  ImageTensor{2  and  yblock  OneHotTensor{0  Hence to do training we need a sample encoding function  Image{2 Label  ImageTensor{2 OneHotTensor{0 During  inference  we have an input and want to use a trained model to predict a target i.e  input  target  The model is again a mapping  xblock  outputblock  so we can build the transformation with an encoding step that encodes the input and a decoding step that takes the model output back into a target This gives us predict  input  target  decodeoutput ∘ model ∘ encodeinput  where encodeinput  input  x model  x  y decodeoutput  y  target In the classification example we have written in blocks  predict  Image{2  Label  and hence  encodeinput  Image{2  ImageTensor{2  and  decodeoutput  OneHotTensor{0  Label Where do we draw the line between model and data processing In general the encoding and decoding steps are  non-learnable  transformations while the model is a  learnable  transformation Encodings Encodings  are reversible transformations that model the non-learnable parts encoding and decoding of the data pipeline What an encoding does depends on what block is passed in Most encodings only transform specific blocks For example the  ImagePreprocessing  encoding maps blocks  Image{N  ImageTensor{N  but leaves other blocks unchanged Encodings are called with  encode  and  decode  which take in the block and the data The actual encoding and decoding takes in an additional context argument which can be specialized on to implement different behavior for e.g training and validation Using an encoding to encode and then decode must be block-preserving i.e if for an encoding  encode  Block1  Block2  then  decode  Block2  Block1  To see the resulting block of applying an encoding to a block we can use  encodedblock  and  decodedblock  You can use  testencoding  to test these invariants to make sure an encoding is implemented properly for a specific block The default implementations of  encodedblock  and  decodedblock  is to return  nothing  indicating that it doesn't transform the data This is overwritten for blocks for which  encode  and  decode  are implemented to indicate that the data is transformed Using  encodedblockfilled(block data  will replace returned  nothing s with the unchanged block Encodings can be applied to tuples of blocks The default behavior is to apply the encoding to each block separately Applying a tuple of encodings will encode the data by applying one encoding after the other When decoding the order is reversed Block learning tasks BlockTask  creates a learning task from blocks and encodings You define the sample block recall for supervised tasks this is a tuple of input and target and a sequence of encodings that are applied to all blocks The below example defines the same learning task as  ImageClassificationSingle  does The first two encodings only change  Image  and the last changes only  Label  so it's simple to understand Now  encode  expects a sample and just runs the encodings over that giving us an encoded input  x  and an encoded target  y  This is equivalent to Image segmentation looks almost the same except we use a  Mask  block as target We're also using  OneHot  here because it also has an  encode  task for  Mask s For this task  ProjectiveTransforms  will be applied to both the  Image  and the  Mask  using the same random state for cropping and augmentation The easiest way to understand how encodings are applied to each block is to use  describetask  and  describeencodings  which print a table of how each encoding is applied successively to each block Rows where a block is  bolded  indicate that the data was transformed by that encoding The above tables make it clear what happens during training encoding a sample and inference encoding an input and decoding an output The more general form  describeencodings  takes in encodings and blocks directly and can be useful for building an understanding of how encodings apply to some blocks Notes Since most encodings just operate on a small number of blocks and keep the rest unchanged applying them to all blocks is usually not a problem When it is because you want some encoding to apply to a specific block only you can use  Named  and  Only  to get around it"},{"doctype":"documentation","id":"references/FluxTraining.throttle","title":"throttle","text":"Throttle  Event  type for  callback  so that it is triggered either only every  freq th time  or every  seconds  seconds Examples If you want to only sporadically log metrics  LogMetrics  or images  LogVisualization   throttle  can be used as follows Every 10 steps Or every 5 seconds"},{"doctype":"documentation","id":"references/FluxTraining.Callbacks","title":"Callbacks","text":""},{"doctype":"documentation","id":"references/FluxTraining.LearningRate","title":"LearningRate","text":"Hyperparameter for the optimizer's learning rate See  Scheduler  and  hyperparameter scheduling "},{"doctype":"documentation","id":"references/DataAugmentation.corners","title":"corners","text":""},{"doctype":"documentation","id":"references/FluxTraining.Metric","title":"Metric","text":"cb device name cb expensivemetric P Implementation of  AbstractMetric  that can be used with the  Metrics  callback Arguments Positional metricfn(ŷs ys  should return a number Keyword statistic  is a  OnlineStats.Statistic  that is updated after every step The default is  OnlineStats.Mean name  is used for printing device  is a function applied to  ŷs  and  ys  before passing them to  metricfn  The default is  Flux.cpu  so that the callback works if  metricfn  doesn't support arrays from other device types If for example  metricfn  works on  CurArray s you can pass  device  Flux.gpu  phase  Phase  a sub)type of  Phase  that restricts for which phases the metric is computed Examples Metric(accuracy Metric(Flux.mse device  gpu name  Mean Squared Error Metric(Flux.mae device  gpu If a metric is expensive to compute and you don't want it to slow down the training phase you can compute it on the validation phase only"},{"doctype":"documentation","id":"references/MLUtils.shuffleobs","title":"shuffleobs","text":"Return a subset of  data  that spans all observations but has the order of the observations shuffled The values of  data  itself are not copied Instead only the indices are shuffled This function calls  obsview  to accomplish that which means that the return value is likely of a different type than  data  The optional parameter  rng  allows one to specify the random number generator used for shuffling This is useful when reproducible results are desired By default uses the global RNG See  Random  in Julia's standard library for more info For this function to work the type of  data  must implement  numobs  and  getobs  See  ObsView  for more information"},{"doctype":"documentation","id":"references/Flux.modules","title":"modules","text":"Return an iterator over non-leaf objects that can be reached by recursing  m  over the children given by  functor  Useful for applying a function e.g a regularizer over specific modules or subsets of the parameters e.g the weights but not the biases Examples"},{"doctype":"documentation","id":"references/FastAI.tuplemap","title":"tuplemap","text":""},{"doctype":"documentation","id":"references/FastAI.default_showbackend","title":"default_showbackend","text":"Return the default  ShowBackend  to use If a Makie.jl backend is loaded i.e  Makie.current_backend  missing  return  ShowMakie  Else return  ShowText "},{"doctype":"documentation","id":"references/FluxTraining.setupoptimstate","title":"setupoptimstate","text":""},{"doctype":"documentation","id":"references/DataAugmentation.setdata","title":"setdata","text":""},{"doctype":"documentation","id":"references/FastAI.OneHotLabel","title":"OneHotLabel","text":""},{"doctype":"documentation","id":"references/DataAugmentation.Sequence","title":"Sequence","text":"Transform  that applies multiple  transformations  after each other You should not use this explicitly Instead use  compose "},{"doctype":"documentation","id":"references/FastAI.showencodedsample","title":"showencodedsample","text":"Show an encoded sample  encsample  to  backend "},{"doctype":"documentation","id":"references/FastAI.showencodedsamples","title":"showencodedsamples","text":"Show a vector of encoded samples  encsamples  to  backend "},{"doctype":"documentation","id":"references/Flux.Losses.mae","title":"mae","text":"Return the loss corresponding to mean absolute error Example"},{"doctype":"document","id":"documents/sysimage/README.md","title":"Building a sysimage","text":"Building a sysimage Building a sysimage for FastAI.jl and its dependecies can greatly reduce the amount of time waiting for code to compile when using it to train models This folder contains a script to build such a sysimage"},{"doctype":"documentation","id":"references/Flux.kaiming_normal","title":"kaiming_normal","text":"Return an  Array{Float32  of the given  size  containing random numbers taken from a normal distribution standard deviation  gain  sqrt(fan_in  using  nfan   Flux.nfan This method is described in 1 and also known as He initialization Examples References 1 He Kaiming et al Delving deep into rectifiers Surpassing human-level performance on imagenet classification  Proceedings of the IEEE international conference on computer vision  2015"},{"doctype":"documentation","id":"references/Flux.GRU","title":"GRU","text":"Gated Recurrent Unit  layer Behaves like an RNN but generally exhibits a longer memory span over sequences This implements the variant proposed in v1 of the referenced paper The integer arguments  in  and  out  describe the size of the feature vectors passed as input and as output That is it accepts a vector of length  in  or a batch of vectors represented as a  in x B  matrix and outputs a vector of length  out  or a batch of vectors of size  out x B  This constructor is syntactic sugar for  Recur(GRUCell(a  and so GRUs are stateful Note that the state shape can change depending on the inputs and so it is good to  reset  the model between inference calls if the batch size changes See the examples below See  this article  for a good overview of the internals Examples Batch size changes Failing to call  reset  when the input batch size changes can lead to unexpected behavior See the example in  RNN "},{"doctype":"documentation","id":"references/Flux.FluxCPUAdaptor","title":"FluxCPUAdaptor","text":""},{"doctype":"documentation","id":"references/MLUtils.randobs","title":"randobs","text":"Pick a random observation or a batch of  n  random observations from  data  For this function to work the type of  data  must implement  numobs  and  getobs "},{"doctype":"documentation","id":"references/FluxTraining.stateaccess","title":"stateaccess","text":"model step xs ys Return a named tuple determining what learner state  callback  can access The default is    the empty named tuple meaning no state can be accessed Implementations of  stateaccess  should always return the least permissions possible Extending For example the  ToGPU  callback needs to write both the model and the batch data so its  stateaccess  implementation is When defining  stateaccess  be careful that you do return a  NamedTuple   x  Read  is one but  x  Read  without the comma is parsed as an assignment with value  Read  Defines what  Learner  state is accessed when calling  sethyperparameter  and  gethyperparameter  This is needed so that  Scheduler  can access the state"},{"doctype":"documentation","id":"references/DataAugmentation.fmap","title":"fmap","text":""},{"doctype":"documentation","id":"references/FastAI.showbatch","title":"showbatch","text":"Show a collated batch of encoded samples to  backend "},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Graph","title":"Graph","text":""},{"doctype":"documentation","id":"references/Flux.LayerNorm","title":"LayerNorm","text":"A  normalisation layer  designed to be used with recurrent hidden states The argument  size  should be an integer or a tuple of integers In the forward pass the layer normalises the mean and standard deviation of the input then applies the elementwise activation  λ  The input is normalised along the first  length(size  dimensions for tuple  size  and along the first dimension for integer  size  The input is expected to have first dimensions size equal to  size  If  affine=true  it also applies a learnable shift and rescaling using the  Scale  layer See also  BatchNorm   InstanceNorm   GroupNorm  and  normalise  Examples"},{"doctype":"documentation","id":"references/FastAI.withfields","title":"withfields","text":"Replace fields on  x  with given keyword arguments run  f  and then restore the fields  x  needs to be a  mutable struct  Every keyword argument is a mapping  field value  or  field setfn value   setfn!(x val  will be used to set the field if as in the first case none is given  setfield  is used"},{"doctype":"documentation","id":"references/MLUtils.filterobs","title":"filterobs","text":"data data fdata > data fdata Return a subset of data container  data  including all indices  i  for which  f(getobs(data i  true "},{"doctype":"documentation","id":"references/Flux.Optimise.skip","title":"skip","text":"cb loss Call  Flux.skip  in a callback to indicate when a callback condition is met This will trigger the train loop to skip the current data point and not update with the calculated gradient Examples"},{"doctype":"documentation","id":"references/FluxTraining.Events","title":"Events","text":""},{"doctype":"documentation","id":"references/MLUtils.normalise","title":"normalise","text":"Normalise the array  x  to mean 0 and standard deviation 1 across the dimension(s given by  dims  Per default  dims  is the last dimension ϵ  is a small additive factor added to the denominator for numerical stability"},{"doctype":"document","id":"documents/docs/notebooks/quickstart.ipynb","title":"Quickstart","text":"Metalhead CairoMakie CairoMakie activate! type data blocks load task blocks size learner task data callbacks backbone ResNet layers end learner task learner model task learner data blocks load task blocks learner task data callbacks backbone ResNet layers end learner task learner model task learner data blocks load task blocks data learner task data callbacks learner task learner backend Quickstart This page follows fastai's  quickstart page  by quickly showing a few learning tasks More will be added here as they are added to the library FastAI.jl's learning tasks all use the same basic steps and code create a  data container create a learning task create learner fit the model make predictions or view results In this quick start we'll show these steps for a wide range of difference applications and datasets As you'll see the code in each case is extremely similar despite the very different models and data being used Computer vision Classification Single-label Let's train a model to classify images in the ImageNette dataset a subset of ImageNet with 10 classes The following lines download the dataset prepare a data preprocessing pipeline and create a model suitable for classification that is ready to train With this we can start training Let's train for  10  epochs i.e iterations through the whole dataset and a maximum learning rate of  0.004  Let's save the model for later use or deployment And now look at some example model predictions on the validation dataset Segmentation Segmentation is similar to classfication but instead of assigning one class to an image we try to classify every single pixel in an image The  CamVid dataset  consists of images taken in traffic with pixel labels for various relevant image parts like the street sky sidewalk etc As with all other tasks let's have a look at some model outputs Tabular data FastAI.jl also supports training models to understand tabular data Classification Let's train a model on the  Adult dataset  that classifies based on the other attributes whether a person earns a salary below or above 50k Since the dimensionality of the data and the model are quite small this will run fast enough on a CPU so we don't need to use the  ToGPU  callback As with every task we can visualize some model predictions Since it fits the data better we use the  ShowText  visualization backend here instead of the default  ShowMakie  used above"},{"doctype":"documentation","id":"references/FastAI.mocktarget","title":"mocktarget","text":"Generate a random  target  compatible with  task "},{"doctype":"documentation","id":"references/Flux.use_cuda","title":"use_cuda","text":""},{"doctype":"documentation","id":"references/FluxTraining.metricname","title":"metricname","text":""},{"doctype":"documentation","id":"references/FastAI.listdecodeblocks","title":"listdecodeblocks","text":""},{"doctype":"documentation","id":"references/FastTabular.sigmoidrange","title":"sigmoidrange","text":""},{"doctype":"documentation","id":"references/FastVision.Models.UNetBlock","title":"UNetBlock","text":"Given convolutional module  m  that halves the spatial dimensions and outputs  k_in  filters create a module that upsamples the spatial dimensions and then aggregates features via  a skip connection"},{"doctype":"documentation","id":"references/Flux.Optimise.AdaBelief","title":"AdaBelief","text":"opt opt The  AdaBelief  optimiser is a variant of the well-known Adam optimiser Parameters Learning rate  η  Amount by which gradients are discounted before updating the weights Decay of momentums  β::Tuple  Exponential decay for the first β1 and the second β2 momentum estimate Examples"},{"doctype":"documentation","id":"references/Flux.RNN","title":"RNN","text":"julia r tanh julia r state size julia r rand Float32 size julia r state size julia r rand Float32 size julia r state size julia r rand Float32 size The most basic recurrent layer essentially acts as a  Dense  layer but with the output fed back into the input each time step The arguments  in  and  out  describe the size of the feature vectors passed as input and as output That is it accepts a vector of length  in  or a batch of vectors represented as a  in x B  matrix and outputs a vector of length  out  or a batch of vectors of size  out x B  This constructor is syntactic sugar for  Recur(RNNCell(a  and so RNNs are stateful Note that the state shape can change depending on the inputs and so it is good to  reset  the model between inference calls if the batch size changes See the examples below Examples Batch size changes Failing to call  reset  when the input batch size changes can lead to unexpected behavior See the following example"},{"doctype":"documentation","id":"references/FastVision","title":"FastVision","text":"Data blocks encodings and more for computer vision The most important  Block  is  Image N  Blocks Image N  an  N dimensional color image Mask N  an  N dimensional categorical mask Keypoints N  a fixed number of  N dimensional keypoints Encodings OneHot  is implemented for  Mask s ImagePreprocessing  prepares an  Image  for training by normalizing and expanding the color channels KeypointPreprocessing  prepares  Keypoints  for training by normalizing them"},{"doctype":"documentation","id":"references/FluxTraining.History","title":"History","text":""},{"doctype":"documentation","id":"references/FluxTraining.Scheduler","title":"Scheduler","text":"es length learner data training lrschedule ParameterSchedulers Step λ γ step_sizes scheduler lrschedule Callback for hyperparameter scheduling Takes pairs of  HyperParameter  types and  ParameterSchedulers.jl schedules  See  the tutorial  for more information Example"},{"doctype":"documentation","id":"references/Flux.Optimise.AdaDelta","title":"AdaDelta","text":"opt opt AdaDelta  is a version of AdaGrad adapting its learning rate based on a window of past gradient updates Parameters don't need tuning Parameters Rho  ρ  Factor by which the gradient is decayed at each time step Examples"},{"doctype":"documentation","id":"references/FluxTraining.Checkpointer","title":"Checkpointer","text":"Saves  learner.model  to  folder  after every  AbstractTrainingPhase  Use  FluxTraining loadmodel  to load a model"},{"doctype":"documentation","id":"references/Flux.gate","title":"gate","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.batchsize","title":"batchsize","text":""},{"doctype":"documentation","id":"references/MLUtils.eachobsparallel","title":"eachobsparallel","text":"Construct a data iterator over observations in container  data  It uses available threads as workers to load observations in parallel leading to large speedups when threads are available To ensure that the active Julia session has multiple threads available check that  Threads.nthreads  1  You can start Julia with multiple threads with the  t n  option If your data loading is bottlenecked by the CPU it is recommended to set  n  to the number of physical CPU cores Arguments data  a data container that implements  getindex/getobs  and  length/numobs buffer  false  whether to use inplace data loading with  getobs  Only use this if you need the additional performance and  getobs  is implemented for  data  Setting  buffer  true  means that when using the iterator an observation is only valid for the current loop iteration You can also pass in a preallocated  buffer  getobs(data 1  executor  Folds.ThreadedEx  task scheduler You may specify a different task scheduler which can be any  Folds.Executor  channelsize  Threads.nthreads  the number of observations that are prefetched Increasing  channelsize  can lead to speedups when per-observation processing time is irregular but will cause higher memory usage"},{"doctype":"documentation","id":"references/Flux.frequencies","title":"frequencies","text":""},{"doctype":"documentation","id":"references/FastVision.ImageTensor","title":"ImageTensor","text":"Block for N+1-dimensional arrays representing an N-dimensional image with the color channels expanded"},{"doctype":"documentation","id":"references/FastAI.Datasets.ROOT_URL_TSClassification","title":"ROOT_URL_TSClassification","text":""},{"doctype":"documentation","id":"references/FastAI.encode","title":"encode","text":"Apply one or more  Encoding s to observation(s  obs "},{"doctype":"documentation","id":"references/FastAI.OneHotTensorMulti","title":"OneHotTensorMulti","text":""},{"doctype":"documentation","id":"references/DataAugmentation.MaskBinary","title":"MaskBinary","text":"mask rand Bool mask An  N dimensional binary mask Examples"},{"doctype":"documentation","id":"references/FastVision.Models","title":"Models","text":""},{"doctype":"documentation","id":"references/FastAI.smoothvalues","title":"smoothvalues","text":"Apply exponential smoothing with parameter  β  to vector  xs "},{"doctype":"documentation","id":"references/DataAugmentation.AbstractCrop","title":"AbstractCrop","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.FastAIDataset","title":"FastAIDataset","text":""},{"doctype":"documentation","id":"references/MLUtils.ofeltype","title":"ofeltype","text":""},{"doctype":"documentation","id":"references/FluxTraining.phasedataiter","title":"phasedataiter","text":""},{"doctype":"documentation","id":"references/FastAI.Validation","title":"Validation","text":"A context for applying data transformations during validation/testing  Encoding s and  LearningTask s can dispatch on this when certain transformations like random augmentations should not be applied during validation only in training"},{"doctype":"document","id":"documents/docs/notebooks/10_26_showblock.ipynb","title":"New visualization tools for FastAI.jl","text":"CairoMakie data blocks loaddataset task blocks sample data task sample x y task sample task x y Markdown task encodings task blocks Markdown parse xs ys task data task xs ys outputs reduce hcat rand _ task xs ys outputs io block data ImageInTerminal imshow io data data blocks loaddataset task blocks sample data task sample data blocks loaddataset sample data blocks sample blocks obs last blocks blocks obs blocks last blocks _ data blocks loaddataset task blocks sample data task sample x y task sample task x y task encodings xs ys task data task xs ys outputs reduce hcat rand _ task xs ys outputs io block data ImageInTerminal imshow io data data blocks loaddataset task blocks sample data task sample data blocks loaddataset sample data blocks sample data blocks loaddataset sample data blocks sample blocks obs last blocks blocks obs blocks last blocks _ New visualization tools for FastAI.jl based on blocks support for different backends currently text and Makie.jl high-level functions for use with learning tasks Learning tasks showsample  shows one sample from the data container showencodedsample  shows a sample after encodings have been applied to it Any encoded block that can't be visualized here  ImageTensor  is decoded until it can be using the task's encodings It is also possible to show a complete collated batch using  showbatch  If you used a trained model to create outputs you can compare these to the true data with  showoutputbatch  These works for any learning task that deal with blocks that have a visualization defined The following definition is all that is needed to add support for the  Image  block type to the text backend showblock showblock  and  showblocks Using  showblock  arbitrary visualizations can be created showblocks  visualizes multiple observations Makie Makie.jl is no longer a required dependency Instead Makie.jl visualizations are only loaded if you have it imported using Requires.jl This reduces the load time of FastAI.jl significantly showsample  shows one sample from the data container showencodedsample  shows a sample after encodings have been applied to it Any encoded block that can't be visualized here  ImageTensor  is decoded until it can be using the task's encodings It is also possible to show a complete collated batch using  showbatch  If you used a trained model to create outputs you can compare these to the true data with  showoutputbatch  These works for any learning task that deal with blocks that have a visualization defined The following definition is all that is needed to add support for the  Image  block type to the text backend showblock showblock  and  showblocks Using  showblock  arbitrary visualizations can be created showblocks  visualizes multiple observations"},{"doctype":"documentation","id":"references/FastAI.Block","title":"Block","text":"A block describes the meaning of a piece of data in the context of a learning task For example for supervised learning tasks there is an input and a target and we want to learn to predict targets from inputs Learning to predict a cat/dog label from 2D images is a supervised image classification task that can be represented with the  Block s  Image{2  and  Label([\"cat dog  Block s are used in virtually every part of the high-level interfaces from data processing over model creation to visualization Extending Consider the following when subtyping  Block  A block Does not hold observation data itself Instead they are used in conjunction with data to annotate it with some meaning If it has any fields they should be metadata that cannot be derived from the data itself and is constant for every sample in the dataset For example  Label  holds all possible classes which are constant for the learning problem Interfaces There are many interfaces that can be implemented for a  Block  See the docstrings of each function for more info about how to implement it checkblock block obs  check whether an observation is a valid block mockblock block  randomly generate an observation blocklossfn predblock yblock  loss function for comparing two blocks blockmodel inblock outblock backbone  construct a task-specific model blockbackbone inblock  construct a backbone model that takes in specific data showblock block obs  visualize an observation"},{"doctype":"documentation","id":"references/Flux.Losses.poisson_loss","title":"poisson_loss","text":"Return how much the predicted distribution  ŷ  diverges from the expected Poisson distribution  y  calculated as  sum(ŷ  y  log.(ŷ  size(y 2  More information "},{"doctype":"documentation","id":"references/Flux.Losses.msle","title":"msle","text":"The loss corresponding to mean squared logarithmic errors calculated as The  ϵ  term provides numerical stability Penalizes an under-estimation more than an over-estimatation Example"},{"doctype":"documentation","id":"references/Flux.loadleaf!","title":"loadleaf!","text":""},{"doctype":"documentation","id":"references/FastVision.Models.linbndrop","title":"linbndrop","text":""},{"doctype":"documentation","id":"references/FastAI.SupervisedTask","title":"SupervisedTask","text":"A  AbstractBlockTask  learning task for the supervised task of learning to predict a  target  given an  input   encodings  are applied to samples before being input to the model Model outputs are decoded using those same encodings to get a target prediction In addition to the blocks defined by  AbstractBlockTask   getblocks(::SupervisedTask  defines the following blocks By default the model output is assumed to be an encoded target but the  ŷblock  keyword argument to overwrite this blocks.input  An unencoded input and the first element in the tuple  sample  input target blocks.target  An unencoded target and the second element in the tuple  sample  input target blocks.pred  A prediction Usually the same as  blocks.target  but may differ if a custom  ŷblock  is specified A  SupervisedTask  also enables some additional functionality encodeinput encodetarget showprediction   showpredictions"},{"doctype":"documentation","id":"references/FluxTraining.epochvalue","title":"epochvalue","text":""},{"doctype":"documentation","id":"references/FastVision.Models.AdaptiveConcatPool","title":"AdaptiveConcatPool","text":""},{"doctype":"documentation","id":"references/Flux.Losses.compute_beta_and_grad_kernel","title":"compute_beta_and_grad_kernel","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.stop","title":"stop","text":"cb Call  Flux.stop  in a callback to indicate when a callback condition is met This will trigger the train loop to stop and exit Examples"},{"doctype":"documentation","id":"references/FastAI.Training","title":"Training","text":"A context for applying data transformations during training  Encoding s and  LearningTask s can dispatch on this when certain transformations like random augmentations should only be applied during training"},{"doctype":"documentation","id":"references/DataAugmentation.RandomCrop","title":"RandomCrop","text":""},{"doctype":"documentation","id":"references/FastAI","title":"FastAI","text":""},{"doctype":"documentation","id":"references/Flux.GRUv3","title":"GRUv3","text":"Gated Recurrent Unit  layer Behaves like an RNN but generally exhibits a longer memory span over sequences This implements the variant proposed in v3 of the referenced paper The arguments  in  and  out  describe the size of the feature vectors passed as input and as output That is it accepts a vector of length  in  or a batch of vectors represented as a  in x B  matrix and outputs a vector of length  out  or a batch of vectors of size  out x B  This constructor is syntactic sugar for  Recur(GRUv3Cell(a  and so GRUv3s are stateful Note that the state shape can change depending on the inputs and so it is good to  reset  the model between inference calls if the batch size changes See the examples below See  this article  for a good overview of the internals Examples Batch size changes Failing to call  reset  when the input batch size changes can lead to unexpected behavior See the example in  RNN "},{"doctype":"documentation","id":"references/DataAugmentation.Rotate","title":"Rotate","text":"tfm Rotate 2D spatial data around the center by an angle chosen at uniformly from γ γ an angle given in degrees You can also pass any  Distributions.Sampleable  from which the angle is selected Examples"},{"doctype":"documentation","id":"references/FastTabular.TableDataset","title":"TableDataset","text":""},{"doctype":"documentation","id":"references/FastAI.Registries.blocktypesmatch","title":"blocktypesmatch","text":""},{"doctype":"documentation","id":"references/Flux.GroupNorm","title":"GroupNorm","text":"Group Normalization  layer chs  is the number of channels the channel dimension of your input For an array of N dimensions the  N-1 th index is the channel dimension G  is the number of groups along which the statistics are computed The number of channels must be an integer multiple of the number of groups channels  should be the size of the channel dimension in your data see below Given an array with  N  2  dimensions call the  N-1 th the channel dimension For  WHCN  images it's the usual channel dimension If  affine=true  it also applies  a shift and a rescale to the input through to learnable per-channel bias  β  and scale  γ  parameters If  track_stats=true  accumulates mean and var statistics in training phase that will be used to renormalize the input in test phase Examples"},{"doctype":"document","id":"documents/docs/notebooks/tabularclassification.ipynb","title":"Tabular Classification","text":"Tables path joinpath load data path Parquet read_parquet parquet_path splitdata row row row salary data cat cont data target salary cat filter isequal target cat catdict data cat inputblock cat cont catdict targetblock unique data table target task inputblock targetblock inputblock data task2 cat cont catdict data outputblock task splitdata x task splitdata model task cardinalities collect map col length catdict col cat ovdict Dict workclass education Symbol overrides collect map col col keys ovdict ovdict col nothing cat embedszs _get_emb_sz cardinalities overrides catback embedszs backbone categorical catback continuous length cont model task backbone learner task splitdata backbone backbone callbacks learner Tabular Classification Tabular Classification involves having a categorical column as the target Here we'll use the adult sample dataset from fastai and try to predict whether the salary is above 50K or not making this a binary classification task We can quickly download and get the path of any dataset from fastai by using  datasets  Once we have the path we'll load the data in a  TableDataset  By default if we pass in just the path to  TableDataset  the data is loaded in a  DataFrame  but we can use any package for accessing our data and pass an object satisfying the  Tables.jl  interface to it In case our data was present in a different format for eg parquet it could be loaded into a data container as follows mapobs  is used here to split our target column from the rest of the row in a lazy manner so that each observation consists of a row of inputs and a target variable To create a learning task for tabular classification task we need an input block an output block and the encodings to be performed on the data The input block here is a  TableRow  which contains information about the nature of the columns ie categorical or continuous along with an indexable collection mapping categorical column names to a collection with distinct classes in that column We can get this mapping by using the  gettransformationdict  task with  DataAugmentation.Categorify  The outblock block used is  Label  for single column classification and the unique classes have to passed to it This is followed by the encodings which needs to be applied on our input and output blocks For the input block we have used the  gettransforms  function here to get a standard bunch of transformations to apply but this can be easily customized by passing in any tabular transformation from DataAugmentation.jl or a composition of those to  TabularPreprocessing  In addition to this we have just one-hot encoded the outblock In case our initial problem wasn't a classification task and we had a continuous target column we would need to perform tabular regression To create a learning task suitable for regression we use a  Continuous  block for representing our target column This can be done even with multiple continuous target columns by just passing the number of columns in  Continuous  For example the task here could be used for 3 targets To get an overview of the learning task created and as a sanity test we can use  describetask  This shows us what encodings will be applied to which blocks and how the predicted ŷ values are decoded getobs  gets us a row of data from the  TableDataset  which we encode here This gives us a tuple with the input and target The input here is again a tuple containing the categorical values which have been label encoded or categorified and the continuous values which have been normalized and any missing values have been filled To get a model suitable for our learning task we can use  taskmodel  which constructs a suitable model based on the target block Of course you can also create a custom backbone using the functions present in  FastAI.Models  We can then pass a named tuple  categorical   continuous    to  taskmodel  to replace the default backbone To directly get a  Learner  suitable for our task and data we can use the  tasklearner  function This creates both batched data loaders and a model for us Once we have a  Learner  we can call  fitonecycle  on it to train it for the desired number of epochs"},{"doctype":"documentation","id":"references/DataAugmentation.OneOfProjective","title":"OneOfProjective","text":""},{"doctype":"documentation","id":"references/DataAugmentation.BufferedThreadsafe","title":"BufferedThreadsafe","text":""},{"doctype":"documentation","id":"references/FluxTraining.accesses","title":"accesses","text":"Enumerate all valid state accesses of permissions of kind  perm  accesses((x  Read Read  x   accesses((x  Read Write  "},{"doctype":"documentation","id":"references/DataAugmentation.NormalizeRow","title":"NormalizeRow","text":"cols col1 col2 col3 row zip cols item row cols normdict Dict col1 col2 tfm normdict col1 col2 tfm item Normalizes the values of a row present in  TabularItem  for the columns specified in  cols  using  dict  which contains the column names as dictionary keys and the mean and standard deviation tuple present as dictionary values Example"},{"doctype":"documentation","id":"references/Flux.Optimise.update!","title":"update!","text":"Perform an update step of the parameters  ps  or the single parameter  p  according to optimizer  opt   and the gradients  gs  the gradient  g  As a result the parameters are mutated and the optimizer's internal state may change The gradient could be mutated as well"},{"doctype":"documentation","id":"references/FastAI.Only","title":"Only","text":"Wrapper that applies the wrapped  encoding  to a  block  if  fn(block  true  Instead of a function you can also pass in a type of block  BlockType  or the  name  of a  Named  block"},{"doctype":"documentation","id":"references/FastVision.Keypoints","title":"Keypoints","text":"A block representing an array of size  sz  filled with keypoints of type  SVector{N "},{"doctype":"documentation","id":"references/DataAugmentation.imagetotensor!","title":"imagetotensor!","text":""},{"doctype":"documentation","id":"references/Flux.SamePad","title":"SamePad","text":"Passed as an option to convolutional layers and friends this causes the padding to be chosen such that the input and output sizes agree on the first  N  dimensions the kernel or window when  stride==1  When  stride≠1  the output size equals  ceil(input_size/stride  See also  Conv   MaxPool  Examples"},{"doctype":"documentation","id":"references/Flux.Optimise.AdaGrad","title":"AdaGrad","text":"opt opt AdaGrad  optimizer It has parameter specific learning rates based on how frequently it is updated Parameters don't need tuning Parameters Learning rate  η  Amount by which gradients are discounted before updating the weights Examples"},{"doctype":"documentation","id":"references/FluxTraining.PropDict","title":"PropDict","text":"Like a  Dict{Symbol  but attribute syntax can be used to access values"},{"doctype":"documentation","id":"references/Flux.Optimise.Optimiser","title":"Optimiser","text":"Combine several optimisers into one each optimiser produces a modified gradient that will be fed into the next and this is finally applied to the parameter as usual"},{"doctype":"documentation","id":"references/FastAI.Registries.DATASETS","title":"DATASETS","text":""},{"doctype":"documentation","id":"references/DataAugmentation.getwrapped","title":"getwrapped","text":""},{"doctype":"documentation","id":"references/FastAI.showoutput","title":"showoutput","text":"Show a model output to  backend  If an encoded sample  encsample  is also given show it next to the output"},{"doctype":"documentation","id":"references/FluxTraining.fit!","title":"fit!","text":"learner learner traindl valdl Train  learner  for  nepochs  of training and validation each Use data iterators that are passed in If none are given use  learner.data.training  and  learner.data.validation  Examples"},{"doctype":"documentation","id":"references/FluxTraining.SanityCheckException","title":"SanityCheckException","text":""},{"doctype":"documentation","id":"references/FluxTraining.UnsafeCallback","title":"UnsafeCallback","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets","title":"Datasets","text":""},{"doctype":"documentation","id":"references/FluxTraining.Permission","title":"Permission","text":""},{"doctype":"documentation","id":"references/Flux.calc_padding","title":"calc_padding","text":""},{"doctype":"documentation","id":"references/DataAugmentation.normalize","title":"normalize","text":""},{"doctype":"documentation","id":"references/Flux.OneHotMatrix","title":"OneHotMatrix","text":"These are constructed by  onehot  and  onehotbatch  Parameter  I  is the type of the underlying storage and  T  its eltype"},{"doctype":"documentation","id":"references/FluxTraining.removecallback!","title":"removecallback!","text":"Remove the first callback of type  C  from  learner  and return it If there is none return  nothing "},{"doctype":"documentation","id":"references/FastVision.Models.unetlayers","title":"unetlayers","text":""},{"doctype":"documentation","id":"references/DataAugmentation.testapply","title":"testapply","text":"Test  apply  invariants of  tfm  on  item  or item type  I  With a constant  randstate  parameter  apply  should always return the same result"},{"doctype":"documentation","id":"references/FluxTraining.addcallback!","title":"addcallback!","text":"Adds  callback  to  learner  and updates the dependency graph"},{"doctype":"documentation","id":"references/FastAI.showblocks","title":"showblocks","text":"data blocks loaddataset samples data i i range blocks samples Show a vector of observations  obss  of the same  block  type Examples Extending This is used for showing batches of observations unlike the  Tuple  variant of  showblock  which assumes an observation consists of multiple blocks Usually a  ShowBackend  will show an observation in one row with  showblock  and  showblocks  will show multiple rows"},{"doctype":"documentation","id":"references/FluxTraining.CheckDataIteratorTrain","title":"CheckDataIteratorTrain","text":""},{"doctype":"documentation","id":"references/DataAugmentation.MaskMulti","title":"MaskMulti","text":"mask rand mask An  N dimensional multilabel mask with labels  classes  Examples"},{"doctype":"documentation","id":"references/FluxTraining.Events.BackwardBegin","title":"BackwardBegin","text":"Event  called between calculating loss and calculating gradients"},{"doctype":"documentation","id":"references/FastAI.Datasets.TSClassificationDataset","title":"TSClassificationDataset","text":""},{"doctype":"documentation","id":"references/FastVision.Mask","title":"Mask","text":"Block for an N-dimensional categorical mask  obs  is valid for  Mask{N T}(classes  if it is an N-dimensional array with every element in  classes "},{"doctype":"documentation","id":"references/FastAI.encodeinput","title":"encodeinput","text":""},{"doctype":"documentation","id":"references/MLUtils.getobs","title":"getobs","text":"Return the observations corresponding to the observation-index  idx  Note that  idx  can be any type as long as  data  has defined  getobs  for that type If  data  does not have  getobs  defined then this function falls back to  data[idx  Authors of custom data containers should implement  Base.getindex  for their type instead of  getobs   getobs  should only be implemented for types where there is a difference between  getobs  and  Base.getindex  such as multi-dimensional arrays The returned observation(s should be in the form intended to be passed as-is to some learning algorithm There is no strict interface requirement on how this actual data must look like Every author behind some custom data container can make this decision themselves The output should be consistent when  idx  is a scalar vs vector See also  getobs  and  numobs"},{"doctype":"documentation","id":"references/DataAugmentation.offsetcropbounds","title":"offsetcropbounds","text":"Calculate offset bounds for a crop of size  sz  For every dimension i where  sz[i  length(indices[i  offsets the crop by  offsets[i  times the difference between the two"},{"doctype":"documentation","id":"references/Flux.Losses.logitcrossentropy","title":"logitcrossentropy","text":"Return the cross entropy calculated by This is mathematically equivalent to  crossentropy(softmax(ŷ y  but is more numerically stable than using functions  crossentropy  and  softmax  separately See also  binarycrossentropy   logitbinarycrossentropy   label_smoothing  Example"},{"doctype":"documentation","id":"references/FastAI.listencodeblocks","title":"listencodeblocks","text":""},{"doctype":"documentation","id":"references/FastAI.LabelMulti","title":"LabelMulti","text":"block block block targets block targets block Block  for a categorical label in a multi-class context  data  is valid for  Label(classes  if  data ∈ classes  Examples You can use  setup  to create a  Label  instance from a data container containing possible classes"},{"doctype":"documentation","id":"references/FluxTraining.LogHistograms","title":"LogHistograms","text":"Callback that logs histograms of model weights to  LoggerBackend s  backends  every  freq  steps If histograms should be logged every step pass  freq  nothing"},{"doctype":"documentation","id":"references/FluxTraining.shouldrun","title":"shouldrun","text":""},{"doctype":"documentation","id":"references/Flux.Chain","title":"Chain","text":"Collects multiple layers  functions to be called in sequence on a given input Supports indexing and slicing  m[2  or  m[1:end-1  and if names are given  m[:name  m[1  etc Examples For large models there is a special type-unstable path which can reduce compilation times This can be used by supplying a vector of layers  Chain([layer1 layer2   This feature is somewhat experimental beware"},{"doctype":"documentation","id":"references/FluxTraining.init!","title":"init!","text":"Initialize a callback Default is to do nothing Extending To extend for a callback implement  init!(cb::MyCallback learner   init  can set up internal state of a callback that depends on  learner  and can also initialize shared callback state in  learner.cbstate  Just like  on  event handlers the state access permissions must be correctly defined using  stateaccess  to do so init  must also be idempotent i.e running it twice on the same  Learner  should have the same effect as runnning it once"},{"doctype":"documentation","id":"references/FastAI.decodedblockfilled","title":"decodedblockfilled","text":""},{"doctype":"documentation","id":"references/FluxTraining.setfieldperm!","title":"setfieldperm!","text":""},{"doctype":"documentation","id":"references/FluxTraining.getfieldperm","title":"getfieldperm","text":""},{"doctype":"documentation","id":"references/Flux.GRUCell","title":"GRUCell","text":""},{"doctype":"documentation","id":"references/FluxTraining.CHECKS","title":"CHECKS","text":""},{"doctype":"documentation","id":"references/MLUtils","title":"MLUtils","text":""},{"doctype":"documentation","id":"references/FastTabular.TableClassificationRecipe","title":"TableClassificationRecipe","text":""},{"doctype":"documentation","id":"references/FastMakie.blockaxis","title":"blockaxis","text":""},{"doctype":"documentation","id":"references/FastVision.ProjectiveTransforms","title":"ProjectiveTransforms","text":"Encoding for spatial data that resizes blocks to a common size  sz  and applies projective augmentations Encodes all spatial blocks preserving the block type Image{N    Image{N Mask{N    Mask{N Keypoints{N    Keypoints{N The behavior differs based on the  context  of encoding Training  Resizes the data so the smallest side equals a side length in  sz  while keeping the aspect ratio Applies  augmentations  Crops a random  sz sized portion of the data Validation  Resizes the data so the smallest side equals a side length in  sz  while keeping the aspect ratio Crops a  sz sized portion from the center Inference  Resizes the data so the smallest side equals a side length in  sz  while keeping the aspect ratio Note that in this context the data does not have size  sz  since no cropping happens and aspect ratio is preserved ProjectiveTransforms  is not limited to 2D data and works on 3D data as well Note however that some transformations in  augs_projection  rotation warping flipping are 2D only so  augs_projection  cannot be used for 3D data Keyword arguments augmentations DataAugmentation.Transform   Identity  Projective augmentation to apply during training See  augs_projection  buffered  true  Whether to use inplace transformations Reduces memory usage sharestate  true  Whether to use the same random state and bounds for all blocks in a sample"},{"doctype":"documentation","id":"references/FastVision.Models.visionhead","title":"visionhead","text":""},{"doctype":"documentation","id":"references/FluxTraining.Metrics","title":"Metrics","text":"cb cb device device Callback that tracks metrics during training You can pass any number of  metrics  with every argument being an  AbstractMetric  like  Metric  or a function  f(ŷs ys  val This callback is added by default to every  Learner  unless you pass in  usedefaultcallbacks  false  A metric tracking  learner.lossfn   Loss  is included by default The computed metrics can be access in  learner.cbstate.metricsstep  and  learner.cbstate.metricsepoch  for steps and epochs respectively Examples Track  accuracy  Pass in  Metric s"},{"doctype":"documentation","id":"references/Flux.Losses.hinge_loss","title":"hinge_loss","text":"Return the  hinge_loss loss  given the prediction  ŷ  and true labels  y  containing 1 or 1 calculated as  sum(max.(0 1  ŷ  y  size(y 2  See also  squared_hinge_loss"},{"doctype":"documentation","id":"references/FastAI.test_task_show","title":"test_task_show","text":"Test suite that tests that all learning task-related  show  functions work for  backend Keyword arguments sample  mockblock(getblocks(task  Sample data to use for tests output  mockblock(getblocks(task).ŷ  Model output data to use for tests"},{"doctype":"documentation","id":"references/Flux.check_use_cuda","title":"check_use_cuda","text":""},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Loggable","title":"Loggable","text":"Abstract type for data that  LoggerBackend s can log See  subtypes(FluxTraining.Loggables.Loggable  and  LoggerBackend"},{"doctype":"documentation","id":"references/MLUtils.batchsize","title":"batchsize","text":"Return the fixed size of each batch in  data "},{"doctype":"documentation","id":"references/DataAugmentation.projectionbounds","title":"projectionbounds","text":""},{"doctype":"documentation","id":"references/FluxTraining.Read","title":"Read","text":""},{"doctype":"documentation","id":"references/Flux.reshape_cell_output","title":"reshape_cell_output","text":""},{"doctype":"documentation","id":"references/FastAI.AbstractBlockTask","title":"AbstractBlockTask","text":"task task Abstract supertype for learning tasks that derive their functionality from  Block s and  Encoding s These learning tasks require you only to specify blocks and encodings by defining which blocks of data show up at which stage of the pipeline Generally a subtype will have a field  blocks  of type  NamedTuple  that contains this information and a field  encodings  of encodings that are applied to samples They can be accessed with  getblocks  and  getencodings  respectively For example  SupervisedTask  represents a learning task where each sample consists of an input and a target To implement a new  AbstractBlockTask  either use the helper  BlockTask  simpler or subtype  AbstractBlockTask  allows customization through dispatch Blocks and interfaces To support different learning task interfaces a  AbstractBlockTask s blocks need to contain different blocks Below we list first block names with descriptions and afterwards relevant interface functions and which blocks are required to use them Blocks Each name corresponds to a key of the named tuple  blocks  getblocks(task  A block is referred to with  blocks.$name  and an instance of data from a block is referred to as  name  blocks.sample  The most important block representing one full observation of unprocessed data Data containers used with a learning task should have compatible observations i.e  checkblock(blocks.sample data[i  blocks.x  Data that will be fed into the model i.e neglecting batching  model(x  should work blocks.ŷ  Data that is output by the model i.e neglecting batching  checkblock(blocks.ŷ model(x blocks.y  Data that is compared to the model output using a loss function i.e  lossfn(ŷ y blocks.encodedsample  An encoded version of  blocks.sample  Will usually correspond to  encodedblockfilled(getencodings(task blocks.sample  Interfaces/functionality and required blocks Core encode task ctx sample  requires  sample  Also enables use of  taskdataset   taskdataloaders decode task ctx encodedsample  requires  encodedsample decodeypred task ctx ŷ  requires  ŷ decodey task ctx y  requires  y Training taskmodel task  requires  x   ŷ tasklossfn task  requires  y   ŷ Visualization showsample   showsamples  require  sample showencodedsample   showencodedsamples   showbatch  require  encodedsample showsample   showsamples  require  sample showoutput   showoutputs   showoutputbatch  require  ŷ   encodedsample Testing mockmodel task  requires  x   ŷ mocksample task  requires  sample"},{"doctype":"document","id":"documents/docs/howto/findfunctionality.md","title":"How to find functionality","text":"entry load entry How to find functionality For some kinds of functionality FastAI.jl provides feature registries that allow you to search for and use features The following registries currently exist datasets  to download and unpack datasets datarecipes  to load datasets into  data containers  that are compatible with a learning task and learningtasks  to find learning tasks that are compatible with a dataset Domain packages Functionality is registered by domain packages such as  FastVision  and  FastTabular  You need to import the respective packages to be able to find their functionality in their registry To load functionality Get an entry using its ID And load it Datasets Data recipes Learning tasks"},{"doctype":"documentation","id":"references/Flux.Optimise.Momentum","title":"Momentum","text":"opt opt Gradient descent optimizer with learning rate  η  and momentum  ρ  Parameters Learning rate  η  Amount by which gradients are discounted before updating the weights Momentum  ρ  Controls the acceleration of gradient descent in the prominent direction in effect damping oscillations Examples"},{"doctype":"documentation","id":"references/Flux.Parallel","title":"Parallel","text":"Create a layer which passes an input array to each path in  layers  before reducing the output with  connection  Called with one input  x  this is equivalent to  connection([l(x for l in layers  If called with multiple inputs one is passed to each layer thus  Parallel f g)(x y  f(x  g(y  Like  Chain  its sub-layers may be given names using the keyword constructor These can be accessed by indexing  m[1  m[:name  is the first layer See also  SkipConnection  which is  Parallel  with one  identity  and  Maxout  which reduces by broadcasting  max  Examples"},{"doctype":"documentation","id":"references/FastAI.LREstimator","title":"LREstimator","text":"Estimator for an optimal learning rate Needs to implement  estimatelr  See  Steepest  and  MinDivByTen "},{"doctype":"documentation","id":"references/FluxTraining.getcallback","title":"getcallback","text":"Find callback of type  C  in  learner s callbacks and return it If there is none return  nothing "},{"doctype":"documentation","id":"references/FastAI.decodewhile","title":"decodewhile","text":"Decode  block  by successively applying  encodings  to decode in reverse order until  f(block  false "},{"doctype":"documentation","id":"references/Flux.flatten","title":"flatten","text":"Reshape arbitrarly-shaped input into a matrix-shaped output preserving the size of the last dimension See also  unsqueeze  Examples"},{"doctype":"documentation","id":"references/Flux.truncated_normal","title":"truncated_normal","text":"Return an  Array{Float32  of the given  size  where each element is drawn from a truncated normal distribution The numbers are distributed like  filter(x  lo<=x<=hi mean  std  randn(100  The values are generated by sampling a Uniform(0 1  rand  and then applying the inverse CDF of the truncated normal distribution This method works best when  lo ≤ mean ≤ hi  Examples"},{"doctype":"documentation","id":"references/Flux.conv_transpose_dims","title":"conv_transpose_dims","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.loadfolderdata","title":"loadfolderdata","text":""},{"doctype":"document","id":"documents/docs/notebooks/training.ipynb","title":"How to train a model","text":"Metalhead CairoMakie data blocks load task blocks learner task data callbacks finderresult learner Makie plot finderresult Metalhead data blocks load task blocks backbone Metalhead ResNet layers end learner task data callbacks learner data blocks loaddataset task blocks learner task data backbone Metalhead ResNet50 pretrain layers end callbacks learner How to train a model Finding a learning rate Using a good learning rate is important for a balance between model convergence and training speed but finding one isn't always easy FastAI.jl includes a learning rate finder that runs a mock training run with increasing learning rates to find a good one You can use it with  lrfind  We can also use the Makie backend to show the results of the learning rate finder Training a model from scratch When using randomly intialized models like you can use  fitonecycle  to train Finetuning a pretrained model When finetuning a pretrained model it is recommended to use  finetune  which uses a warmup schedule to train the newly initiliazed head more quickly than the pretrained backbone"},{"doctype":"documentation","id":"references/FastAI.MinDivByTen","title":"MinDivByTen","text":"Estimate the optimal learning rate to be value at the minimum loss divided by 10"},{"doctype":"documentation","id":"references/FastVision.ImageKeypointRegression","title":"ImageKeypointRegression","text":"Learning task for regressing a set of  nkeypoints  keypoints from images Images are resized to  size  and a class is predicted for every pixel"},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Value","title":"Value","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.ClipNorm","title":"ClipNorm","text":"Clip gradients when their L2 norm exceeds  thresh "},{"doctype":"document","id":"documents/docs/tutorials/learningmethod.md","title":"learningmethod","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.makeavailable","title":"makeavailable","text":""},{"doctype":"documentation","id":"references/Flux.f64","title":"f64","text":"Converts the  eltype  of model's parameters to  Float64  Recurses into structs marked with  functor "},{"doctype":"documentation","id":"references/FluxTraining.CheckModelLossStep","title":"CheckModelLossStep","text":""},{"doctype":"documentation","id":"references/FastAI.createhandle","title":"createhandle","text":"backend backend block obs backend block obs Creates a context to which blocks of data can be shown using the mutating functions  showblock  and  showblocks  It is called internally when using  showblock  or  showblocks "},{"doctype":"documentation","id":"references/FastTabular.tabular_continuous_backbone","title":"tabular_continuous_backbone","text":""},{"doctype":"documentation","id":"references/FastAI.taskmodel","title":"taskmodel","text":"Construct a model for  task  from a backbone architecture for example by attaching a task-specific head model"},{"doctype":"documentation","id":"references/FastAI.encodeinput!","title":"encodeinput!","text":""},{"doctype":"documentation","id":"references/DataAugmentation.Image","title":"Image","text":"Images imagedata rand RGB item imagedata item imagedata rand Float32 item imagedata item Item representing an N-dimensional image with element type T Examples If  T  is not a color the image will be interpreted as grayscale"},{"doctype":"documentation","id":"references/FluxTraining.print_epoch_table","title":"print_epoch_table","text":""},{"doctype":"documentation","id":"references/DataAugmentation.denormalize!","title":"denormalize!","text":""},{"doctype":"documentation","id":"references/FastAI.Label","title":"Label","text":"block block block targets block targets block Block  for a categorical label in a single-class context  data  is valid for  Label(classes  if  data ∈ classes  See  LabelMulti  for the multi-class setting where an observation can belong to multiple classes Examples You can use  setup  to create a  Label  instance from a data container containing possible classes"},{"doctype":"documentation","id":"references/FluxTraining.Protected","title":"Protected","text":""},{"doctype":"documentation","id":"references/FastAI.encodedblock","title":"encodedblock","text":"Return the block that is obtained by encoding  block  with encoding  E  This needs to be constant for an instance of  E  so it cannot depend on the sample or on randomness The default is to return  nothing  meaning the same block is returned and not changed Encodings that return the same block but change the data e.g  ProjectiveTransforms  should return  block "},{"doctype":"documentation","id":"references/FastVision.plotimage","title":"plotimage","text":""},{"doctype":"documentation","id":"references/FastAI.Named","title":"Named","text":"Wrapper  Block  to attach a name to a block Can be used in conjunction with  Only  to apply encodings to specific blocks only"},{"doctype":"documentation","id":"references/FluxTraining.edgesrunafter","title":"edgesrunafter","text":"Return a vector of  Edge s representing dependencies defined by  runafter "},{"doctype":"documentation","id":"references/FastAI.ShowMakie","title":"ShowMakie","text":"ax Makie Axis block B obs A backend for showing block data that uses  Makie.jl  figures for visualization Keyword arguments  kwargs  are passed to the constructed  Figure s Implementing a  Block  visualization As with other  ShowBackend  implementing a visualization for a block type  B  AbstractBlock  requires you to implement  showblock  For  ShowMakie  the first argument is a  Makie.Axis  i.e you have to implement The axis is created by  FastMakie.makeaxis  The default options will result in an axis cleaned of all decorations To customize it implement  FastMakie.axiskwargs(block::B  See the docstring of  makeaxis  for available options"},{"doctype":"documentation","id":"references/FastAI.taskdataloaders","title":"taskdataloaders","text":"traindl validdl data task traindl validdl traindata validdata task shuffle traindl validdl data task parallel buffered Create training and validation  DataLoader s from two data containers  traindata valdata  If only one container  data  is passed splits it into two with  pctgvalid  of the data going into the validation split Arguments Positional batchsize  16 Keyword shuffle  true  Whether to shuffle the training data container validbsfactor  2  Factor to multiply batchsize for validation data loader with validation batches can be larger since no GPU memory is needed for the backward pass All remaining keyword arguments are passed to  DataLoader  Examples Basic usage Explicit validation data container and no shuffling of training container Customizing the  DataLoader"},{"doctype":"documentation","id":"references/MLUtils.stack","title":"stack","text":"Concatenate the given array of arrays  xs  into a single array along the given dimension  dims  See also  stack  and  batch  Examples"},{"doctype":"documentation","id":"references/FluxTraining.resolveconflict","title":"resolveconflict","text":"Define a conflict resolution strategy for resolving a write/write conflict between two callbacks The default is  NotDefined  which will result in an error and a message to implement this method To implement dispatch on the callback types that you which to resolve in any order and return one of the following Unresolvable   if the callbacks must not be used together RunFirst cb  if one of the callbacks needs to run first or NoConflict   if the callbacks may run together in any order"},{"doctype":"documentation","id":"references/FluxTraining.stepvalue","title":"stepvalue","text":""},{"doctype":"documentation","id":"references/Flux.outputsize","title":"outputsize","text":"Calculate the size of the output from model  m  given the size of the input Obeys  outputsize(m size(x  size(m(x  for valid input  x  Keyword  padbatch=true  is equivalent to using  inputsize 1  and returns the final size including this extra batch dimension This should be faster than calling  size(m(x  It uses a trivial number type which should work out of the box for custom layers If  m  is a  Tuple  or  Vector  its elements are applied in sequence like  Chain(m  Examples For model or layer  m  accepting multiple arrays as input this returns  size(m((x y   given  size_x  size(x  etc Examples Notice that  Chain  only accepts multiple arrays as a tuple while  Parallel  also accepts them as multiple arguments  outputsize  always supplies the tuple"},{"doctype":"documentation","id":"references/DataAugmentation.Reflect","title":"Reflect","text":"tfm Reflect 2D spatial data around the center by an angle chosen at uniformly from γ γ an angle given in degrees You can also pass any  Distributions.Sampleable  from which the angle is selected Examples"},{"doctype":"documentation","id":"references/MLUtils.joinobs","title":"joinobs","text":"data1 data2 jdata joinumobs data1 data2 jdata Concatenate data containers  datas "},{"doctype":"documentation","id":"references/FastAI.Registries.datarecipes","title":"datarecipes","text":"datasetid blocks Any info Show a registry of available dataset recipes A dataset recipe defines how to load a dataset into a suitable format for use with a learning task Pass in filters as keyword arguments to look at a subset See also  finding functionality   datasets  and  learningtasks  For more information on registries see  FeatureRegistries.jl  Examples Show all available dataset recipes Show all recipes for datasets that have image in their name Show all data recipes usable for classification tasks that is where the target block is a  Label  Get an explanation of fields in the dataset recipe registry"},{"doctype":"documentation","id":"references/FastVision.Models.convxlayer","title":"convxlayer","text":""},{"doctype":"documentation","id":"references/FluxTraining.ProtectedException","title":"ProtectedException","text":""},{"doctype":"documentation","id":"references/FluxTraining.Events.StepEnd","title":"StepEnd","text":"Event  called at the end of a batch"},{"doctype":"documentation","id":"references/FastAI.mockblock","title":"mockblock","text":"Randomly generate an instance of  block  It always holds that  checkblock(block mockblock(block  true "},{"doctype":"documentation","id":"references/DataAugmentation.boundsof","title":"boundsof","text":"Find bounding index ranges for points  ps "},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Audio","title":"Audio","text":""},{"doctype":"documentation","id":"references/FastVision.KeypointPreprocessing","title":"KeypointPreprocessing","text":"Scale a  Keypoints  block falling in a rectangle of  bounds  so that they lie between 1 and 1"},{"doctype":"documentation","id":"references/Flux.onecold","title":"onecold","text":"Roughly the inverse operation of  onehot  or  onehotbatch  This finds the index of the largest element of  y  or each column of  y  and looks them up in  labels  If  labels  are not specified the default is integers  1:size(y,1   the same operation as  argmax(y dims=1  but sometimes a different return type Examples"},{"doctype":"documentation","id":"references/FastAI.ShowText","title":"ShowText","text":"A backend for showing block data using text for REPL use Text is displayed to  io  and  kwargs  are keyword arguments for  PrettyTables.pretty_table  which is used to display collections of blocks"},{"doctype":"documentation","id":"references/DataAugmentation.itemdata","title":"itemdata","text":"Access the data wrapped in  item  or a tuple of items"},{"doctype":"documentation","id":"references/FastVision.Models.UNetMiddleBlock","title":"UNetMiddleBlock","text":""},{"doctype":"documentation","id":"references/MLUtils.ones_like","title":"ones_like","text":"Create an array with the given element type and size based upon the given source array  x  All element of the new array will be set to 1 The second and third arguments are both optional defaulting to the given array's eltype and size The dimensions may be specified as an integer or as a tuple argument See also  zeros_like  and  fill_like  Examples"},{"doctype":"documentation","id":"references/FastAI.PropagateSameBlock","title":"PropagateSameBlock","text":"Propagate a wrapper type only if the encoded block is same ignoring any wrappers See  propagate  for more information"},{"doctype":"documentation","id":"references/MLUtils.splitobs","title":"splitobs","text":"julia at julia at train test X at train val test X at train test X y at shuffle Xtrain Ytrain train Compute the indices for two or more disjoint subsets of the range  1:n  with splits given by  at  Examples Split the  data  into multiple subsets proportional to the value(s of  at  If  shuffle=true  randomly permute the observations before splitting Supports any datatype implementing the  numobs  and  getobs  interfaces Examples"},{"doctype":"documentation","id":"references/FastAI.Datasets.loaddata","title":"loaddata","text":""},{"doctype":"documentation","id":"references/MLUtils.Datasets.make_spiral","title":"make_spiral","text":"Generates  n  noisy responses for a spiral with two labels Uses the radius angle and scaling arguments to space the points in 2D space and adding  noise  f_randn(n  to the response"},{"doctype":"documentation","id":"references/FluxTraining.accuracy","title":"accuracy","text":""},{"doctype":"documentation","id":"references/DataAugmentation.ResizePadDivisible","title":"ResizePadDivisible","text":""},{"doctype":"documentation","id":"references/FastAI.SHOW_BACKEND","title":"SHOW_BACKEND","text":""},{"doctype":"documentation","id":"references/FastVision.ImagePreprocessing","title":"ImagePreprocessing","text":"Encodes  Image s by converting them to a common color type  C  expanding the color channels and normalizing the channel values Additionally apply pixel-level augmentations passed in as  augmentations  during  Training  Encodes Image{N    ImageTensor{N Keyword arguments augmentations DataAugmentation.Transform  Augmentation to apply to every image before preprocessing See  augs_lighting buffered  true  Whether to use inplace transformations Reduces memory usage means::SVector  IMAGENET_MEANS  mean value of each color channel stds::SVector  IMAGENET_STDS  standard deviation of each color channel C::Type{<:Colorant  RGB{N0f8  color type to convert images to T::Type{<:Real  Float32  element type of output"},{"doctype":"documentation","id":"references/FastAI.showblock!","title":"showblock!","text":"Show block of data to an existing context  handle  using  backend  See  showblock  for examples Extending Every  ShowBackend  should implement the following versions of this method showblock!(handle backend block::Block obs  to show a single block of obs should be implemented for every block type you want to show showblock!(handle backend blocks::Tuple obss::Tuple  to show several blocks that belong to the same observation Optionally you can also implement showblock!(handle backend pair::Pair obs  where  title block  pair  gives the name for a block If this is not implemented for a backend then calling it will default to the untitled method"},{"doctype":"documentation","id":"references/FluxTraining.FitException","title":"FitException","text":"Abstract types for exceptions that can be thrown during fitting to change its control flow See  CancelStepException   CancelEpochException   CancelFittingException "},{"doctype":"document","id":"documents/docs/notebooks/how_to_visualize.ipynb","title":"How to visualize data","text":"CairoMakie CairoMakie activate! type task model dir joinpath load data dir filterfn loadfn idxs rand data samples data i i idxs xs ys task data idxs ŷs model xs task samples task xs ys task xs ŷs ys How to visualize data Visualizing the data we're working with is indispensible both to check that data pipelines are set up correctly and to check the predictions of a trained model For visualization the  Makie.jl  plotting package is used which requires you to  install a plotting backend  Learning tasks define how the data is visualized allowing you to use the following functions for visualization showsample   showsamples  Visualize an unprocessed sample usually a pair of inputs and targets or a vector of samples showencodedsample   showbatch  Visualize processed model input and output  x y  or a batch of  xs  and  ys  showprediction    showpredictions  Compare a model output with the ground truth To add support for these to a learning task you have to implement the plotting interface for a block  showblock  Let's look at an example using the Cat/Dog classifier from  the saving and loading tutorial  First we load a vector of unprocessed samples a batch of training data and the corresponding model outputs Then we can visualize the data with the functions listed above"},{"doctype":"documentation","id":"references/MLUtils.leavepout","title":"leavepout","text":"julia train_idx val_idx train val X p Compute the train/validation assignments for  k ≈ n/size  repartitions of  n  observations and return them in the form of two vectors The first vector contains the index-vectors for the training subsets and the second vector the index-vectors for the validation subsets respectively Each validation subset will have either  size  or  size+1  observations assigned to it The following code snippet generates the index-vectors for  size  2  Each observation is assigned to the validation subset once and only once Thus a union over all validation index-vectors reproduces the full range  1:n  Note that there is no random assignment of observations to subsets which means that adjacent observations are likely to be part of the same validation subset Repartition a  data  container using a k-fold strategy where  k  is chosen in such a way that each validation subset of the resulting folds contains roughly  p  observations Defaults to  p  1  which is also known as leave-one-out partitioning The resulting sequence of folds is returned as a lazy iterator Only data subsets are created That means no actual data is copied until  getobs  is invoked See kfolds  for a related function"},{"doctype":"documentation","id":"references/FastAI.Steepest","title":"Steepest","text":"Estimate the optimal learning rate to be where the gradient of the loss is the steepest i.e the decrease is largest"},{"doctype":"documentation","id":"references/MLUtils.batch","title":"batch","text":"Batch the arrays in  xs  into a single array with an extra dimension If the elements of  xs  are tuples named tuples or dicts the output will be of the same type See also  unbatch  Examples"},{"doctype":"documentation","id":"references/FastAI.setup","title":"setup","text":"images labels blocks loaddataset images buffered data block loaddataset block data Create an instance of block type  Block  from data container  data  Examples Create an encoding using statistics derived from a data container  data  with observations of block  block  Used when some arguments of the encoding are dependent on the dataset  data  should be the training dataset Additional  kwargs  are passed through to the regular constructor of  Encoding  Examples"},{"doctype":"documentation","id":"references/Flux.Losses.squared_hinge_loss","title":"squared_hinge_loss","text":"Return the squared hinge_loss loss given the prediction  ŷ  and true labels  y  containing 1 or 1 calculated as  sum((max.(0 1  ŷ  y)).^2  size(y 2  See also  hinge_loss"},{"doctype":"documentation","id":"references/Flux.Dense","title":"Dense","text":"Create a traditional fully connected layer whose forward pass is given by The input  x  should be a vector of length  in  or batch of vectors represented as an  in × N  matrix or any array with  size(x,1  in  The out  y  will be a vector  of length  out  or a batch with  size(y  out size(x)[2:end Keyword  bias=false  will switch off trainable bias for the layer The initialisation of the weight matrix is  W  init(out in  calling the function given to keyword  init  with default  glorot_uniform   Flux.glorot_uniform The weight matrix and/or the bias vector of length  out  may also be provided explicitly Examples"},{"doctype":"documentation","id":"references/MLUtils.undersample","title":"undersample","text":"X rand Y X_bal Y_bal X Y size X_bal length Y_bal sum Y_bal sum Y_bal data DataFrame i data i data DataFrame nrow data Generate a class-balanced version of  data  by subsampling its observations in such a way that the resulting number of observations will be the same number for every class This way all classes will have as many observations in the resulting data set as the smallest class has in the given original  data  The convenience parameter  shuffle  determines if the resulting data will be shuffled after its creation if it is not shuffled then all the observations will be in their original order Defaults to  false  For this function to work the type of  data  must implement  numobs  and  getobs  For example the following code allows  undersample  to work on a  DataFrame  Note that if  data  is a tuple then it will be assumed that the last element of the tuple contains the targets See  ObsView  for more information on data subsets See also  oversample "},{"doctype":"documentation","id":"references/DataAugmentation.CropFrom","title":"CropFrom","text":""},{"doctype":"documentation","id":"references/DataAugmentation.testapply!","title":"testapply!","text":"Test  apply  invariants With a constant  randstate  parameter  apply  should always return the same result Given a different item than was used to create the buffer the buffer's data should be modified"},{"doctype":"documentation","id":"references/DataAugmentation.PadDivisible","title":"PadDivisible","text":""},{"doctype":"documentation","id":"references/FluxTraining.Phases.AbstractTrainingPhase","title":"AbstractTrainingPhase","text":"An abstract type for phases where parameter updates are being made This exists so callbacks can dispatch on it and work with custom training phases The default implementation for supervised tasks is  TrainingPhase "},{"doctype":"documentation","id":"references/FastVision.Models.runtests","title":"runtests","text":"Equivalent to  ReTest.retest(FastVision.Models pattern kwargs  This function is defined automatically in any module containing a  testset  possibly nested within submodules"},{"doctype":"documentation","id":"references/DataAugmentation.testitem","title":"testitem","text":"Create an instance of an item with type  TItem  If it has spatial bounds should return an instance with bounds with ranges 1:16 1:16"},{"doctype":"documentation","id":"references/MLUtils.eachobs","title":"eachobs","text":"X rand x X typeof x Vector Float64 size x x X typeof x Matrix Float64 size x x y X Y Return an iterator over  data  Supports the same arguments as  DataLoader  The  batchsize  default is  1  here while it is  1  for  DataLoader  Examples"},{"doctype":"documentation","id":"references/FluxTraining.GarbageCollect","title":"GarbageCollect","text":"Every  nsteps  steps forces garbage collection Use this if you get memory leaks from for example parallel data loading Performs an additional C-call on Linux systems that can sometimes help"},{"doctype":"documentation","id":"references/FastTabular.EncodedTableRow","title":"EncodedTableRow","text":"Block for processed rows having a tuple of M categorical and N continuous value collections"},{"doctype":"documentation","id":"references/FluxTraining.LogVisualization","title":"LogVisualization","text":"Logs images created by  visfn(learner.step  to  backends  every  freq  steps"},{"doctype":"documentation","id":"references/FastAI.blockmodel","title":"blockmodel","text":"From a  backbone  model construct a model suitable for learning a mapping from  inblock  to  outblock "},{"doctype":"documentation","id":"references/FluxTraining.Events.Event","title":"Event","text":"Abstract type for events that callbacks can hook into"},{"doctype":"documentation","id":"references/FluxTraining.setindexperm!","title":"setindexperm!","text":""},{"doctype":"documentation","id":"references/DataAugmentation.adjustcontrast","title":"adjustcontrast","text":""},{"doctype":"documentation","id":"references/FluxTraining.CancelFittingException","title":"CancelFittingException","text":"Throw during fitting to cancel it"},{"doctype":"documentation","id":"references/DataAugmentation.ImageToTensor","title":"ImageToTensor","text":"Images image rand RGB tfm tfm image Expands an  Image{N T  of size  sz  to an  ArrayItem{N+1  with size  sz ch  where  ch  is the number of color channels of  T  Supports  apply  Examples"},{"doctype":"documentation","id":"references/Flux.plateau","title":"plateau","text":"Return a function that internally counts by one when  abs(distance(last_score f  min_dist  where  last_score  holds the last value of  f  If the count is greater than or equal to  width  the function returns  true  otherwise it returns  false  The count is reset when  abs(distance(last_score f  min_dist  Examples"},{"doctype":"documentation","id":"references/FastAI.wrapped","title":"wrapped","text":""},{"doctype":"documentation","id":"references/FluxTraining.hasconflict","title":"hasconflict","text":""},{"doctype":"documentation","id":"references/FastAI.Registries","title":"Registries","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.isavailable","title":"isavailable","text":""},{"doctype":"documentation","id":"references/FastAI.group","title":"group","text":""},{"doctype":"documentation","id":"references/DataAugmentation.apply","title":"apply","text":"Apply  tfm  to an  item  or a tuple  items "},{"doctype":"documentation","id":"references/FluxTraining.LogHyperParams","title":"LogHyperParams","text":"Callback that logs hyperparameters to one or more  LoggerBackend s See also  LoggerBackend   Loggables.Loggable   log_to   TensorBoardBackend Example"},{"doctype":"documentation","id":"references/Flux.Optimise.train!","title":"train!","text":"Uses a  loss  function and training  data  to improve the model's parameters according to a particular optimisation rule  opt  For each  d in data  first the gradient of the  loss  is computed like this Here  pars  is produced by calling  Flux.params  on your model Or just on the layers you want to train like  train!(loss params(model[1:end-2 data opt  This is the implicit style of parameter handling Then this gradient is used by optimizer  opt  to update the paramters The optimiser should be from the  Flux.Optimise  module Different optimisers can be combined using  Flux.Optimise.Optimiser  This training loop iterates through  data  once You can use  epochs  to do this several times or use for instance  Iterators.repeat  to make a longer  data  iterator Callbacks Callbacks  are given with the keyword argument  cb  For example this will print training every 10 seconds using  Flux.throttle  The callback can call  Flux.stop  to interrupt the training loop Multiple callbacks can be passed to  cb  as array"},{"doctype":"documentation","id":"references/FluxTraining.Events.BackwardEnd","title":"BackwardEnd","text":"Event  called between calculating gradients and updating parameters"},{"doctype":"documentation","id":"references/DataAugmentation.FlipY","title":"FlipY","text":""},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Histogram","title":"Histogram","text":""},{"doctype":"documentation","id":"references/FastVision.Models.iterlayers","title":"iterlayers","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.AdaMax","title":"AdaMax","text":"opt opt AdaMax  is a variant of Adam based on the ∞-norm Parameters Learning rate  η  Amount by which gradients are discounted before updating the weights Decay of momentums  β::Tuple  Exponential decay for the first β1 and the second β2 momentum estimate Examples"},{"doctype":"documentation","id":"references/FastAI.decodey!","title":"decodey!","text":""},{"doctype":"documentation","id":"references/FluxTraining.CheckIteratesTuples","title":"CheckIteratesTuples","text":""},{"doctype":"documentation","id":"references/Flux.zeros32","title":"zeros32","text":"Return an  Array{Float32  of the given  size "},{"doctype":"documentation","id":"references/Flux.Losses.siamese_contrastive_loss","title":"siamese_contrastive_loss","text":"Return the  contrastive loss  which can be useful for training Siamese Networks It is given by Specify  margin  to set the baseline for distance at which pairs are dissimilar"},{"doctype":"documentation","id":"references/Flux.flip","title":"flip","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.pathparent","title":"pathparent","text":""},{"doctype":"documentation","id":"references/FluxTraining.sethyperparameter!","title":"sethyperparameter!","text":"Sets hyperparameter  H  to  value  on  learner  returning the modified learner"},{"doctype":"documentation","id":"references/FastAI.Datasets.recipeblocks","title":"recipeblocks","text":"Tuple Return the  Block   types  for the data container that recipe type  TRecipe  creates Does not return  Block  instances as the exact configuration may not be known until the dataset is being loaded Examples"},{"doctype":"documentation","id":"references/FastAI.encodesample","title":"encodesample","text":""},{"doctype":"documentation","id":"references/FastVision.checksize","title":"checksize","text":""},{"doctype":"documentation","id":"references/FastTabular.TabularRegression","title":"TabularRegression","text":"Learning task for tabular regression Continuous columns are normalized and missing values are filled categorical columns are label encoded taking into account any missing values which might be present  blocks  should be an input and target block  TableRow Continuous  Construct learning task with  classes  to classify into and a  TableDataset   tabledata  The column names can be passed in or guessed from the data The regression target is a vector of  n  values"},{"doctype":"documentation","id":"references/Flux.Optimise.NAdam","title":"NAdam","text":"opt opt NAdam  is a Nesterov variant of Adam Parameters don't need tuning Parameters Learning rate  η  Amount by which gradients are discounted before updating the weights Decay of momentums  β::Tuple  Exponential decay for the first β1 and the second β2 momentum estimate Examples"},{"doctype":"documentation","id":"references/FastVision.Models.convx","title":"convx","text":""},{"doctype":"documentation","id":"references/FluxTraining.testlearner","title":"testlearner","text":"Construct a  Learner  with a simple optimization problem This learner should be used in tests that require training a model e.g for callbacks"},{"doctype":"documentation","id":"references/FastAI.Datasets.loadrecipe","title":"loadrecipe","text":"Load a recipe from a path Return a data container  data  and concrete  blocks "},{"doctype":"document","id":"documents/docs/notebooks/textclassification.ipynb","title":"Text Classification","text":"data blocks load println data println blocks task TextClassificationSingle blocks data task encodings input target data encoded_input encoded_output task input target println encoded_input println encoded_output encoding_1 Textual Sanitize sanitized_data encoding_1 Paragraph input encoding_2 Textual Tokenize tokenized_data encoding_2 Paragraph sanitized_data vocab Textual EmbedVocabulary data encoding_3 Textual EmbedVocabulary vocab vocab vocab vector_data encoding_3 Textual Tokens tokenized_data Text Classification We'll use the  IMDB  dataset for this task This is a dataset for binary sentiment classification containing 25,000 highly polarized movie reviews for training and 25,000 for testing There is additional unlabeled data for use as well Each sample is a review this'll be our input data The output is the sentiment of the input either positive or negative The task consists of encodings that needs to be applied to the input data and output data The encodings for the input data are as follows Sanitize  Involves text cleaning steps like case trimming remove punctuations removing stop words and some fastai specific preprocessing steps xxbos xxup etc Tokenize  Tokenizing the text into words EmbedVocabulary  Embedding the words into a vector space This step constructs the vocabulary for the training data and returns the vector embedding for the input data Let us now look at each step of the above encoding process Sanitize The sanitized input data will have no stop words no punctuations and no case Along with those it'll also contain some fastai specific tokens like xxbos beginning of the sentence xxup the next word if uppercase in the original text xxmaj the first letter is uppercase in the original text etc Tokenize Tokenize the sanitized input data EmbedVocabulary This step is the most important step in the encoding process It constructs the vocabulary for the training data and returns the vector embedding for the input data"},{"doctype":"documentation","id":"references/Flux.Losses.logaddexp","title":"logaddexp","text":"logaddexp(a b Adds log-space  a  and  b  such that the result equals  log(exp(a)+exp(b"},{"doctype":"documentation","id":"references/Flux.InstanceNorm","title":"InstanceNorm","text":"Instance Normalization  layer  channels  should be the size of the channel dimension in your data see below Given an array with  N  2  dimensions call the  N-1 th the channel dimension For  WHCN  images it's the usual channel dimension InstanceNorm  computes the mean and variance for each  D_1×...×D_{N-2}×1×1  input slice and normalises the input accordingly If  affine=true  it also applies  a shift and a rescale to the input through to learnable per-channel bias  β  and scale  γ  parameters If  track_stats=true  accumulates mean and var statistics in training phase that will be used to renormalize the input in test phase Warning  the defaults for  affine  and  track_stats  used to be  true  in previous Flux versions  v0.12 Examples"},{"doctype":"documentation","id":"references/FluxTraining.epoch!","title":"epoch!","text":"Train  learner  for one epoch on  dataiter  Iterates through  dataiter  and  step s for each batch/item If no data iterator is passed in use  learner.data[phasedataiter(phase  Extending The default implementation iterates over every batch in  dataiter  and calls  step  for each This behavior can be overloaded by implementing  epoch!(learner MyPhase dataiter  If you're implementing a custom  epoch  method it is recommended you make use of  runepoch  to get begin and end events as well as proper handling of  CancelEpochException s See the default implementation for reference"},{"doctype":"documentation","id":"references/FluxTraining.setcallbacks!","title":"setcallbacks!","text":"Set  learner s callbacks to  callbacks  removing all current callbacks"},{"doctype":"documentation","id":"references/FastTabular.TabularClassificationSingle","title":"TabularClassificationSingle","text":"Learning task for single-label tabular classification Continuous columns are normalized and missing values are filled categorical columns are label encoded taking into account any missing values which might be present The target value is predicted from  classes   blocks  should be an input and target block  TableRow Label  Construct learning task with  classes  to classify into and a  TableDataset   tabledata  The column names can be passed in or guessed from the data"},{"doctype":"document","id":"documents/DEVELOPING.md","title":"Developing","text":"Pkg Pkg develop url Pkg Pkg test Pkg Pkg activate joinpath Pkg devdir Pkg instantiate ReTest ReTest fail ReTest not ReTest pass Pkg Pkg activate joinpath Pkg devdir Pkg add Pkg PackageSpec url Pkg PackageSpec url Pkg PackageSpec url rev Pollen Pollen servedocs lazy Developing This guide contains information important when developing FastAI.jl Concretely how to set up a local development environment how to run the tests how to preview the documentation locally Setting up FastAI.jl locally for development Fork FastAI.jl and add it as a  dev  dependency  You can fork it from  the GitHub repository  Then use  Pkg  to add the fork to your Julia environment You should now be able to import FastAI  using FastAI  in Julia If you are using  Revise.jl  any changes you make to its source code will also be reflected in your interactive sessions Running the tests Like any Julia package you can run the entire test suite in an isolated environment using  Pkg.test  When developing however it can be helpful to repeatedly rerun parts of the tests FastAI.jl uses  ReTest.jl  to set up tests which makes it possible to run subsets of tests or only tests that have not previously succeeded First activate the test environment and install its dependencies Then you can run the test suite or subsets of it Local documentation preview FastAI.jl uses  Pollen.jl  as its documentation system which allows you to preview documentation locally First activate the documentation environment and install its dependencies Now you can build the documentation locally giving you a preview at  http://localhost:3000  Using the  lazy  true  will build pages lazily only once you request them on the website which reduces the build time when you only care about specific pages Adding documentation pages files Documentation pages correspond to a Markdown  md  or Jupyter Notebook  ipynb  file that should be stored in the  docs  folder If a document should show up in the left sidebar of the docs page add an entry to  FastAI/docs/toc.json  Jupyter Notebooks should be used when they use resources that are not available on the GitHub CI like a GPU needed for training You should run them locally and the outputs will be captured and inserted into the HTML page Markdown documents should be preferred for everything else as they allow the code examples to be run on the GitHub CI meaning they'll stay up-to-date unlike a notebook that has to be manually rerun Both formats support the  Markdown syntax of Publish.jl  and in markdown files the  cell syntax of Publish.jl  can be used to mark code cells These will be run and the output is inserted into the HTML page Linking to documentation For a new documentation file to be discoverable you have to add an entry to the nested Markdown list in  toc.md  which corresponds to the sidebar in the documentation  updating the sidebar currently requires interrupting and reincluding the file that starts the development server  Documentation pages can also link to each other using standard Markdown link syntax Referencing code symbols Symbols like  fitonecycle  can be referenced by using the cross-referencing syntax  fitonecycle   which will link to and create a reference page from the symbol's docstrings It will also be added as an entry on the references page"},{"doctype":"documentation","id":"references/Flux.activations","title":"activations","text":"Like calling a  Chain  but saves the result of each layer as an output Examples"},{"doctype":"documentation","id":"references/DataAugmentation.imagetotensor","title":"imagetotensor","text":""},{"doctype":"documentation","id":"references/FluxTraining.Unresolvable","title":"Unresolvable","text":"Return from  resolveconflict  to indicate that two callbacks are incompatible and cannot be used together"},{"doctype":"documentation","id":"references/FastAI.Datasets.pathname","title":"pathname","text":""},{"doctype":"documentation","id":"references/FastVision.imagestats","title":"imagestats","text":"Compute the color channel-wise means and standard deviations of all pixels  image  is converted to color type  C  e.g  RGB{N0f8   Gray{N0f8  before statistics are calculated"},{"doctype":"documentation","id":"references/DataAugmentation.RandomResizeCrop","title":"RandomResizeCrop","text":""},{"doctype":"documentation","id":"references/Flux.rng_from_array","title":"rng_from_array","text":"Create an instance of the RNG most appropriate for  x  The current defaults are x isa CuArray   CUDA.default_rng  else x isa AbstractArray  or no  x  provided Julia version is  1.7  Random.GLOBAL_RNG Julia version is  1.7  Random.default_rng"},{"doctype":"documentation","id":"references/FluxTraining.testbatches","title":"testbatches","text":""},{"doctype":"documentation","id":"references/FastAI.Many","title":"Many","text":"Many  indicates that you can variable number of instances for  block  Consider a bounding box detection task where there may be any number of targets in an image and this number varies for different samples The blocks  Image{2 BoundingBox{2  imply that there is exactly one bounding box for every image which is not the case Instead you would want to use  Image{2 Many(BoundingBox{2 "},{"doctype":"documentation","id":"references/FluxTraining.Phases.AbstractValidationPhase","title":"AbstractValidationPhase","text":"An abstract type for phases where no parameter updates are being made This exists so callbacks can dispatch on it and work with custom validation phases The default implementation for supervised tasks is  ValidationPhase "},{"doctype":"documentation","id":"references/FastVision.Models.PixelShuffle","title":"PixelShuffle","text":"Pixel shuffle layer that upscales height and width of  x  by  scale  Has reduced checkerboard artifacts compared to  ConvTranspose Introduced in  Real-Time Single Image and Video Super-Resolution Using   an EfficientSub-Pixel Convolutional Neural Network "},{"doctype":"documentation","id":"references/Flux.randn32","title":"randn32","text":"Return an  Array{Float32  of the given  size  filled like  rand  or  randn  When the size is not provided  rand32(rng::AbstractRNG  returns a function"},{"doctype":"documentation","id":"references/FluxTraining.NotDefined","title":"NotDefined","text":"The default implementation of  resolveconflict  If a conflict is detected this ensures an error message is printed"},{"doctype":"documentation","id":"references/FastTabular.TableRegressionRecipe","title":"TableRegressionRecipe","text":""},{"doctype":"documentation","id":"references/FluxTraining.findconflicts","title":"findconflicts","text":""},{"doctype":"documentation","id":"references/FluxTraining.Loggables.Text","title":"Text","text":""},{"doctype":"documentation","id":"references/Flux.underscorise","title":"underscorise","text":""},{"doctype":"documentation","id":"references/DataAugmentation.AbstractArrayItem","title":"AbstractArrayItem","text":"Abstract type for all  Item s that wrap an  N dimensional array with element type  T "},{"doctype":"documentation","id":"references/DataAugmentation.NormalizeIntensity","title":"NormalizeIntensity","text":""},{"doctype":"documentation","id":"references/DataAugmentation.ArrayItem","title":"ArrayItem","text":"An item that contains an array"},{"doctype":"documentation","id":"references/MLUtils.flatten","title":"flatten","text":"Reshape arbitrarly-shaped input into a matrix-shaped output preserving the size of the last dimension See also  unsqueeze  Examples"},{"doctype":"documentation","id":"references/FastAI.Datasets.DESCRIPTIONS","title":"DESCRIPTIONS","text":""},{"doctype":"documentation","id":"references/MLUtils.chunk","title":"chunk","text":"Split  x  into  n  parts The parts contain the same number of elements except possibly for the last one that can be smaller If  x  is an array  dims  can be used to specify along which dimension to split defaults to the last dimension Examples"},{"doctype":"documentation","id":"references/FastAI.lrfind","title":"lrfind","text":"Run the learning rate finder Exponentially increases the learning rate from a very low value to a very high value and uses the losses to estimate an optimal learning rate Return a  LRFinderResult  Keyword arguments nsteps  100  maximum number of steps to run the learning rate finder for startlr  1e-7  minimum learning rate endlr  10  maximum learning rate divergefactor  stop finder early if loss goes higher than lowest loss times this factor estimators  Steepest MinDivByTen  list of  LREstimator s"},{"doctype":"documentation","id":"references/DataAugmentation","title":"DataAugmentation","text":""},{"doctype":"documentation","id":"references/DataAugmentation.WarpAffine","title":"WarpAffine","text":"A three-point affine warp calculated by randomly moving 3 corners of an item Similar to a random translation shear and rotation"},{"doctype":"documentation","id":"references/MLUtils.unstack","title":"unstack","text":"Unroll the given  xs  into an array of arrays along the given dimension  dims  See also  stack  and  unbatch  Examples"},{"doctype":"documentation","id":"references/MLUtils.group_counts","title":"group_counts","text":"Count the number of times that each element of  x  appears See also  group_indices Examples"},{"doctype":"documentation","id":"references/FastAI.PropagateSameWrapper","title":"PropagateSameWrapper","text":"Propagate a wrapper type only if the encoded block is the exact same including any wrappers See  propagate  for more information"},{"doctype":"documentation","id":"references/FluxTraining.runstep","title":"runstep","text":"Run  stepfn  inside the context of a step Calls  stepfn(handle state  where  handle(e  can be called to dispatch events and  state  is a  PropDict  which step data gradients and losses can be written to Return  state  Takes care of dispatching  StepBegin  and  StepEnd  events as well as handling  CancelStepException s"},{"doctype":"document","id":"documents/docs/learning_methods.md","title":"Custom learning tasks","text":"Colors data load filterfn loadfn images targets data classes unique targets ImageClassification classes size task ImageClassification classes getresizecrop context sz sz getresizecrop context sz sz getresizecrop context sz sz task ImageClassification context image tfm getresizecrop context task size RGB Float32 tfm image sample image class data x task image summary x task ImageClassification class idx findfirst isequal class task classes v Float32 length task classes v idx v task ImageClassification ctx input target task ctx input task ctx target y task class task ImageClassification ypred task classes argmax ypred task y class traindl valdl data task model length task classes optimizer lossfn learner model lossfn data traindl valdl optimizer task ImageClassification backbone h w outch b backbone inblock nchannels head outch length task classes backbone head Custom learning tasks This tutorial explains the low-level interface behind  BlockTask s and how to use it to create your custom learning tasks without the data block interface In the  quickstart  section you've already seen a learning task in action  BlockTask  The learning task abstraction powers FastAI.jl's high-level interface allowing you to make training models for a task simple  BlockTask  is a particularly convenient and composable interface for creating learning tasks and should be preferred for most use cases However to get a look behind the scenes in this tutorial we'll use the lower-level learning task interface to implement our own version of an image classification learning task You're encouraged to follow along in a REPL or notebook This tutorial can also serve as a template for implementing a custom learning task for your own project A learning task describes how we need to process data so we can train a model for some task In our case the task we want to solve is to classify an image The task defines what kind of data we need here pairs of images and class labels That alone however isn't enough to train a model since we can't just throw an image in any format into a model and get a class out Almost always the input data needs to be processed in some way before it is input to a model we call this  encoding  and the same goes for the model outputs we call this  decoding  So let's say we have an image and a trained model How do we make a prediction First we encode the image run it through the model and then decode the output Similarly how we can use a pair of image and class to train a model We encode both run the encoded input through the model and then compare the output with the encoded class using a  loss function  The result tells us how we'll need to update the weights of the model to improve its performance In essence the learning task interface allows us to implement these steps and derive useful functionality from it like training and evaluating models Later we'll also cover some optional interfaces that allow us to define other parts of a deep learning project Datasets Before we get started let's load up a  data container  that we can test our code on as we go It's always a good idea to interactively test your code Since we'll be implementing a task for image classification the observations in our data container will of course have to be pairs of images and classes We'll use one of the many image classification datasets available from the fastai dataset repository I'll use ImageNette but you can use any of the datasets listed in  FastAI.Datasets.DATASETS_IMAGECLASSIFICATION  The way the interface is built allows you to easily swap out the dataset you're using We'll also collect the unique class names Implementation Learning task struct Now let's get to it The first thing we need to do is to create a  LearningTask  struct The  LearningTask   struct  should contain all the configuration needed for encoding and decoding the data We'll keep it simple here and include a list of the classes and the image dimensions input to the model Now we can create an instance of it though of course it can't do anything yet Encoding and decoding There are 3 tasks we need to define before we can use our learning task to train models and make predictions encodesample  which encodes an image and a class encodeinput  will encode an image so it can be input to a model decodeypred  decodes a model output into a class label Note These functions always operate on  single  images and classes even if we want to pass batches to the model later on While it's not the focus of this tutorial let's give a quick recap of how the data is encoded and decoded for image classification Images are cropped to a common size so they can be batched converted to a 3D array with dimensions height width color channels and normalized Classes are encoded as one-hot vectors teaching the model to predict a confidence distribution over all classes To decode a predicted one-hot vector we can simply find the index with the highest value and look up the class label Each of the tasks also takes a  context FastAI.Context  argument which allows it to behave differently during training validation and inference We'll make use of that to choose a different image crop for each situation During training we'll use a random crop for augmentation while during validation a center crop will ensure that any metrics we track are the same every epoch During inference we won't crop the image so we don't lose any information Inputs We implement  encodeinput  using  DataAugmentation.jl  Feel free to look at  its documentation  we won't focus on it here If we test this out on an image it should give us a 3D array of size  128 128 3  and indeed it does Outputs encodetarget  is much simpler The same goes for the decoding step Training And that's all we need to start training models There are some optional interfaces that make that even easier but let's use what we have for now With our  LearningTask  defined we can use  taskdataloaders  to turn a dataset into a set of training and validation data loaders that can be thrown into a training loop Now with a makeshift model an optimizer and a loss function we can create a  Learner  From here you're free to start training using   fit  or  fitonecycle  These tasks are also enough to use  predict  and  predictbatch  once you've trained a model Additional interfaces Training interface We can implement some additional tasks to make our life easier Specifically let's implement every task needed to use  tasklearner  tasklossfn  return a loss function  lossfn(ys ys  comparing a batch of model outputs and encoded targets taskmodel  from a backbone construct a model suitable for the task Let's start with the loss function We want to compare two one-hot encoded categorical variables for which categorical cross entropy is the most commonly used loss function For the model we'll assume we're getting a convolutional feature extractor passed in as a backbone so its output will be of size height width channels batch size  Flux.outputsize  can be used to calculate the output size of arbitrary models without having to evaluate the model We'll use it to check the number of output channels of the backbone Then we add a global pooling layer and some dense layers on top to get a classification output"},{"doctype":"documentation","id":"references/DataAugmentation.makebuffer","title":"makebuffer","text":"Create a buffer  buf  that can be used in a call to  apply!(buf tfm item  Default to  buffer  apply(tfm item  You only need to implement this if the default  apply(tfm item  isn't enough See  apply(tfm::Sequence item  for an example of this"},{"doctype":"documentation","id":"references/MLUtils.NamedTupleData","title":"NamedTupleData","text":""},{"doctype":"documentation","id":"references/Flux.reset!","title":"reset!","text":"Reset the hidden state of a recurrent layer back to its original value Assuming you have a  Recur  layer  rnn  this is roughly equivalent to Examples"},{"doctype":"documentation","id":"references/FastAI.Datasets.DatasetRecipe","title":"DatasetRecipe","text":"data blocks recipe args kwargs A recipe that contains configuration for loading a data container Calling it with a path returns a data container and the blocks that each sample is made of Examples For example implementations see  FastVision.ImageFolders  Extending Interface loadrecipe DatasetRecipe args kwargs  data blocks  This loads a data container the  Block s that each observation corresponds to For most recipes the only argument beside the recipe is a path to a folder on disk recipeblocks Type{DatasetRecipe  TBlocks  The type of  blocks  returned by  loadrecipe  Should be as specific as possible Used for discovery Invariants Given the following must hold ∀i ∈ 1:numobs(data checkblock(blocks data[i  i.e  data  must be a data container of observations that are valid  blocks  numobs(data ≥ 1  i.e there is at least one observation if the data was loaded without error"},{"doctype":"documentation","id":"references/Flux.Zeros","title":"Zeros","text":""},{"doctype":"documentation","id":"references/MLUtils.zeros_like","title":"zeros_like","text":"Create an array with the given element type and size based upon the given source array  x  All element of the new array will be set to 0 The second and third arguments are both optional defaulting to the given array's eltype and size The dimensions may be specified as an integer or as a tuple argument See also  ones_like  and  fill_like  Examples"},{"doctype":"documentation","id":"references/Flux.throttle","title":"throttle","text":"Return a function that when invoked will only be triggered at most once during  timeout  seconds Normally the throttled function will run as much as it can without ever going more than once per  wait  duration but if you'd like to disable the execution on the leading edge pass  leading=false  To enable execution on the trailing edge pass  trailing=true "},{"doctype":"documentation","id":"references/FluxTraining.reset!","title":"reset!","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.loadfile","title":"loadfile","text":"Load a file from disk into the appropriate format"},{"doctype":"documentation","id":"references/FastAI.decodedblock","title":"decodedblock","text":"Return the block that is obtained by decoding  block  with encoding  E  This needs to be constant for an instance of  E  so it cannot depend on the sample or on randomness The default is to return  nothing  meaning the same block is returned and not changed Encodings that return the same block but change the data when decoding should return  block "},{"doctype":"documentation","id":"references/FastAI.decodeŷ!","title":"decodeŷ!","text":""},{"doctype":"document","id":"documents/docs/notebooks/vae.ipynb","title":"Variational autoencoders","text":"SVector Gray CairoMakie path load data path filterfn loadfn data EmbeddingTask block encodings sample block encodedsample x y ŷ encodings sample blocks sample x y ŷ encodedsample blocks encodings task EmbeddingTask means SVector stds SVector C Gray Float32 buffered x task data task x td data task td BATCHSIZE dl _ data task BATCHSIZE pctgval dataiter collect dl xs dataiter print size xs VAE E D encoder E decoder D VAE vae VAE xs μ logσ² vae encoder xs zs sample_latent μ logσ² x̄s vae decoder zs x̄s μ logσ² Random randn! Statistics mean sample_latent μ AbstractArray T logσ² AbstractArray T T μ exp logσ² randn! similar logσ² βELBO x x̄ μ logσ² β reconstruction_error mean sum x̄ x dims kl_divergence mean sum μ exp logσ² logσ² dims reconstruction_error β kl_divergence SIZE Din prod SIZE Dhidden Dlatent encoder Din Dhidden relu tuple Dhidden Dlatent Dhidden Dlatent decoder Dlatent Dhidden relu Dhidden Din sigmoid xs reshape xs SIZE model VAE encoder decoder VAETrainingPhase VAETrainingPhase learner phase VAETrainingPhase learner phase xs state gs gradient learner μ logσ² learner model encoder state xs state zs sample_latent μ logσ² state x̄s learner model decoder state zs state loss learner lossfn state xs state x̄s μ logσ² state loss learner optimizer learner gs VAETrainingPhase cb learner learner step xs cb movedatafn learner step xs learner model βELBO callbacks learner learner phases VAETrainingPhase dataiter xs task data rand data ypreds _ model xs task xs ypreds Variational autoencoders So far we've covered many examples of how to train models in a supervised fashion However there are many applications of neural networks outside the supervised regime In this tutorial we'll implement and train a variational autoencoder VAE to embed and generate images from the MNIST dataset You'll learn how to implement custom Flux.jl models write a custom training loop generate new images and visualize them Setting up the data First we load the MNIST dataset Since we're not using it for supervised learning we only need the input images and don't load the labels We get a data container where every observation is an image Next we need to define a learning task that will handle data encoding and decoding as well as visualization for us So far we've used  SupervisedTask  a lot which assumes there is an input that is fed to the model and a corresponding target output Since we want to do unsupervised learning we'll instead create a custom learning task using  BlockTask  It defines what kind of data we'll have at each step in the data pipeline for example  x  is a model input and  ŷ  a model output See  AbstractBlockTask  for more info With this helper defined we can create a learning task for our task  Image{2  is the kind of data we want to learn with and  ImagePreprocessing  makes sure to encode and decode these images so they can be used to train a model With the learning task set up we can use  encode  to get samples ready to be input to a model and all  show  functions to visualize data at various points of the pipeline For later training the last thing we need to do with the data is to create a data iterator over batches of encoded samples Since the dataset comfortably fits into memory we preload it all at once by using  collect  on the  DataLoader  This saves us having to reload each image again every epoch With that we have a data iterator of batches that we can use in a training loop just by iterating over it Next we need to construct a model and define a loss function so it can be optimized Modelling The variational autoencoder consists of two parts an encoder and a decoder The encoder takes in data and outputs parameters of a probability distribution These are then used to sample latent vectors which are fed into the decoder to produce new samples A loss function ELBO rewards the model if the outputs are similar to the inputs The challenge is that the latent space is of much lower dimensionality than the data space so the model needs to learn to compress the information contained in the data If you're interested in more mathematical background on VAEs and the loss function Lilian Weng has written a  great write-up on autoencoders  The architecture looks like this Diagram of VAE architecture We define the Variational Autoencoder model as a new type that wraps an encoder and decoder model and define the forward pass and loss function as regular Julia functions Next we define the encoder and decoder models by composing basic Flux.jl layers  Dlatent  is the size of the latent space and controls how much the model has to compress the information Feel free to try out smaller or larger numbers and see how the quality of the generated images changes Custom training loop When dealing with a unconvential learning scheme we usually need to write a custom training loop FastAI.jl is build on top of FluxTraining.jl which allows you to write custom training loops with very little boilerplate while retaining compatibility with its extensive callback system In fact the built-in training loops for supervised learning are defined in just the same way as we will We just defined our own training phase All that is required to take advantage of the FastAI.jl framework is to define the  FluxTraining.step  function We'll use a utility  FluxTraining.runstep  to reduce the boilerplate involved in handling callback events and state  runstep s first argument is a function with inputs  handle state   handle  can be used to dispatch events which callbacks can react to  state  holds data generated on each call to  step  like the batch gradients and loss These are also accessible to callbacks for example to calculate metrics We also give  runstep  the initial step state which just contains our batch Since the step state is a little different from the supervised case there are no targets  ys  we also overwrite the default task for the  ToDevice  callback for our training phase Training Now we can put the pieces together creating a  Learner  To override the default supervised training loops we pass our custom training phase and the data iterator we want to run it on to  fitonecycle  Finally we can visualize how well inputs are reconstructed"},{"doctype":"documentation","id":"references/FastAI.mocksample","title":"mocksample","text":"Generate a random  sample  compatible with  task "},{"doctype":"documentation","id":"references/DataAugmentation.Buffered","title":"Buffered","text":""},{"doctype":"documentation","id":"references/FastAI.describeencodings","title":"describeencodings","text":""},{"doctype":"documentation","id":"references/Flux.RNNCell","title":"RNNCell","text":""},{"doctype":"documentation","id":"references/FluxTraining.Loggables.File","title":"File","text":""},{"doctype":"documentation","id":"references/Flux.Losses.ctc_loss","title":"ctc_loss","text":"ctc_loss(ŷ y Computes the connectionist temporal classification loss between  ŷ  and  y  ŷ  must be a classes-by-time matrices i.e each row represents a class and each column represents a time step Additionally the  logsoftmax  function will be applied to  ŷ  so  ŷ  must be the raw activation values from the neural network and not for example the activations after being passed through a  softmax  activation function  y  must be a 1D array of the labels associated with  ŷ  The blank label is assumed to be the last label category in  ŷ  so it is equivalent to  size(ŷ 1  Used for sequence-to-sequence classification problems such as speech recognition and handwriting recognition where the exact time-alignment of the output e.g letters is not needed to solve the problem See  Graves et al 2006  or  Graves 2012  for mathematical details"},{"doctype":"documentation","id":"references/FastTabular.removecol","title":"removecol","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.testrecipe","title":"testrecipe","text":""},{"doctype":"documentation","id":"references/FastAI.Registries.TASKS","title":"TASKS","text":""},{"doctype":"documentation","id":"references/FluxTraining.log_parameters","title":"log_parameters","text":""},{"doctype":"documentation","id":"references/FluxTraining.model!","title":"model!","text":""},{"doctype":"documentation","id":"references/FluxTraining.CallbackCondition","title":"CallbackCondition","text":"Supertype for conditions to use with  ConditionalCallback  To implement a  CallbackCondition  implement  shouldrun MyCondition event phase  See  FrequencyThrottle   TimeThrottle  and  throttle "},{"doctype":"documentation","id":"references/Flux.Optimise.AdamW","title":"AdamW","text":"opt opt AdamW  is a variant of Adam fixing as in repairing its weight decay regularization Parameters Learning rate  η  Amount by which gradients are discounted before updating the weights Decay of momentums  β::Tuple  Exponential decay for the first β1 and the second β2 momentum estimate decay  Decay applied to weights during optimisation Examples"},{"doctype":"documentation","id":"references/DataAugmentation.Transform","title":"Transform","text":"Abstract supertype for all transformations"},{"doctype":"documentation","id":"references/Flux.kaiming_uniform","title":"kaiming_uniform","text":"Return an  Array{Float32  of the given  size  containing random numbers drawn from a uniform distribution on the interval  x x  where  x  gain  sqrt(3/fan_in  using  nfan   Flux.nfan This method is described in 1 and also known as He initialization Examples References 1 He Kaiming et al Delving deep into rectifiers Surpassing human-level performance on imagenet classification  Proceedings of the IEEE international conference on computer vision  2015"},{"doctype":"documentation","id":"references/MLUtils.randn_like","title":"randn_like","text":"Create an array with the given element type and size based upon the given source array  x  All element of the new array will be set to a random value drawn from a normal distribution The last two arguments are both optional defaulting to the given array's eltype and size The dimensions may be specified as an integer or as a tuple argument The default random number generator is used unless a custom one is passed in explicitly as the first argument See also  Base.randn  and  rand_like  Examples"},{"doctype":"documentation","id":"references/Flux.MaxPool","title":"MaxPool","text":"Max pooling layer which replaces all pixels in a block of size  window  with one Expects as input an array with  ndims(x  N+2  i.e channel and batch dimensions after the  N  feature dimensions where  N  length(window  By default the window size is also the stride in each dimension The keyword  pad  accepts the same options as for the  Conv  layer including  SamePad  See also  Conv   MeanPool   AdaptiveMaxPool   GlobalMaxPool  Examples"},{"doctype":"documentation","id":"references/Flux.Optimise.batchmemaybe","title":"batchmemaybe","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.AbstractOptimiser","title":"AbstractOptimiser","text":""},{"doctype":"documentation","id":"references/Flux.cpu","title":"cpu","text":"Moves  m  onto the CPU the opposite of  gpu  Recurses into structs marked  functor "},{"doctype":"documentation","id":"references/FastVision.Models.make_layer","title":"make_layer","text":""},{"doctype":"documentation","id":"references/Flux.Conv","title":"Conv","text":"Standard convolutional layer  filter  is a tuple of integers specifying the size of the convolutional kernel  in  and  out  specify the number of input and output channels Image data should be stored in WHCN order width height channels batch In other words a 100×100 RGB image would be a  100×100×3×1  array and a batch of 50 would be a  100×100×3×50  array This has  N  2  spatial dimensions and needs a kernel size like  5,5  a 2-tuple of integers To take convolutions along  N  feature dimensions this layer expects as input an array with  ndims(x  N+2  where  size(x N+1  in  is the number of input channels and  size(x ndims(x  is as always the number of observations in a batch Then filter  should be a tuple of  N  integers Keywords  stride  and  dilation  should each be either single integer or a tuple with  N  integers Keyword  pad  specifies the number of elements added to the borders of the data array It can be a single integer for equal padding all around a tuple of  N  integers to apply the same padding at begin/end of each spatial dimension a tuple of  2*N  integers for asymmetric padding or the singleton  SamePad  to calculate padding such that  size(output,d  size(x,d  stride  possibly rounded for each spatial dimension Keyword  groups  is expected to be an  Int  It specifies the number of groups to divide a convolution into Keywords to control initialization of the layer init   Function used to generate initial weights Defaults to  glorot_uniform  bias   The initial bias vector is all zero by default Trainable bias can be disabled entirely by setting this to  false  or another vector can be provided such as  bias  randn(Float32 out  See also  ConvTranspose   DepthwiseConv   CrossCor  Examples Constructs a convolutional layer with the given weight and bias Accepts the same keywords and has the same defaults as  Conv(k::NTuple{N,Integer ch::Pair{<:Integer,<:Integer σ    Conv"},{"doctype":"documentation","id":"references/FastAI.estimatelr","title":"estimatelr","text":"Estimate the optimal learning rate using  losses  and  lrs "},{"doctype":"documentation","id":"references/FluxTraining.SafeCallback","title":"SafeCallback","text":""},{"doctype":"documentation","id":"references/FastAI.assigngroups!","title":"assigngroups!","text":""},{"doctype":"documentation","id":"references/DataAugmentation.CenterCrop","title":"CenterCrop","text":""},{"doctype":"documentation","id":"references/FastTabular.getcoltypes","title":"getcoltypes","text":"Returns the categorical and continuous columns present in a  TableDataset "},{"doctype":"document","id":"documents/docs/setup.md","title":"Setup","text":"Pkg Pkg add Pkg Pkg add CairoMakie Setup FastAI.jl is a  Julia  package You can download Julia from the  official website  You can install FastAI.jl like any other Julia package using the REPL as follows Visualization  Aside from text-based visualizations FastAI.jl also defines  Makie.jl  plotting recipes to visualize data If you want to use them you'll have to install and one of the Makie.jl backends  CairoMakie.jl   GLMakie.jl  or  WGLMakie.jl  and load the package Colab  If you don't have access to a GPU or want to try out FastAI.jl without installing Julia try out  this FastAI.jl Colab notebook  We're working on adding a Launch Colab button to every documentation page based off a notebook file but for now you can copy the code over manually Threaded data loading  To make use of multi-threaded data loading you need to start Julia with multiple threads either with the  t auto  commandline flag or by setting the environment variable  JULIA_NUM_THREADS  See the  IJulia.jl documentation  for instructions on setting these for Jupyter notebook kernels"},{"doctype":"documentation","id":"references/Flux.BatchNorm","title":"BatchNorm","text":"julia Statistics julia xs rand julia m julia m julia isapprox std m xs atol std xs std m xs Batch Normalization  layer  channels  should be the size of the channel dimension in your data see below Given an array with  N  dimensions call the  N-1 th the channel dimension For a batch of feature vectors this is just the data dimension for  WHCN  images it's the usual channel dimension BatchNorm  computes the mean and variance for each  D_1×...×D_{N-2}×1×D_N  input slice and normalises the input accordingly If  affine=true  it also applies  a shift and a rescale to the input through to learnable per-channel bias β and scale γ parameters After normalisation elementwise activation  λ  is applied If  track_stats=true  accumulates mean and var statistics in training phase that will be used to renormalize the input in test phase Use  testmode  during inference Examples"},{"doctype":"documentation","id":"references/FastVision.plotmask","title":"plotmask","text":""},{"doctype":"documentation","id":"references/Flux.DepthwiseConv","title":"DepthwiseConv","text":"Return a depthwise convolutional layer that is a  Conv  layer with number of groups equal to the number of input channels See  Conv  for a description of the arguments Examples"},{"doctype":"documentation","id":"references/FluxTraining.TestModel","title":"TestModel","text":""},{"doctype":"documentation","id":"references/FastAI.StatefulEncoding","title":"StatefulEncoding","text":"Encoding that needs to compute some state from the whole sample even if it only transforms some of the blocks This could be random state for stochastic augmentations that needs to be the same for every block that is encoded The state is created by calling  encodestate(encoding context blocks sample  and passed to recursive calls with the keyword argument  state  As a result you need to implement  encode   decode   encode   decode  with a keyword argument  state  that defaults to the above call Same goes for  decode  which should accept a  state  keyword argument defaulting to  decodestate(encoding context blocks sample"},{"doctype":"documentation","id":"references/FastAI.Registries.learningtasks","title":"learningtasks","text":"package blocks Any info Show a registry of available learning tasks Pass in filters as keyword arguments to look at a subset See also  finding functionality   datasets  and  datarecipes  For more information on registries see  FeatureRegistries.jl  Examples Show all available learning tasks Show all computer vision tasks Show all classification tasks i.e where the target block is a  Label  Get an explanation of fields in the learning task registry"},{"doctype":"documentation","id":"references/DataAugmentation.FillMissing","title":"FillMissing","text":"cols col1 col2 col3 row zip cols item row cols fmdict Dict col1 col2 tfm fmdict col1 col2 tfm item Fills the missing values of a row present in  TabularItem  for the columns specified in  cols  using  dict  which contains the column names as dictionary keys and the value to fill the column with present as dictionary values Example"},{"doctype":"documentation","id":"references/FluxTraining.HyperParameter","title":"HyperParameter","text":"A hyperparameter is any state that influences the training and is not a parameter of the model Hyperparameters can be scheduled using the  Scheduler  callback"},{"doctype":"documentation","id":"references/FluxTraining.savemodel","title":"savemodel","text":""},{"doctype":"documentation","id":"references/DataAugmentation.Polygon","title":"Polygon","text":"StaticArrays points SVector SVector SVector SVector item points item Item wrapper around  Keypoints  Examples"},{"doctype":"documentation","id":"references/DataAugmentation.threepointwarpaffine","title":"threepointwarpaffine","text":"Calculate an affine  CoordinateTransformations.LinearMap  from 3 source points to 3 destination points Adapted from   CoordinateTransformations.jl#30 "},{"doctype":"documentation","id":"references/FluxTraining.CancelStepException","title":"CancelStepException","text":"learner phase _ xs ys batches learner phase xs ys _ state isnan state loss throw Throw during fitting to cancel the currently running step This prematurely ends the current step without throwing an error Must be thrown inside the context of  runstep  Examples"},{"doctype":"documentation","id":"references/DataAugmentation.Normalize","title":"Normalize","text":"Images image rand RGB tfms tfms image Normalizes the last dimension of an  AbstractArrayItem{N  Supports  apply  Examples Preprocessing a 3D image with 3 color channels"},{"doctype":"documentation","id":"references/FastAI.decodey","title":"decodey","text":""},{"doctype":"documentation","id":"references/Flux.PixelShuffle","title":"PixelShuffle","text":"Pixel shuffling layer with upscale factor  r  Usually used for generating higher resolution images while upscaling them See  NNlib.pixel_shuffle  Examples"},{"doctype":"documentation","id":"references/DataAugmentation.FromOrigin","title":"FromOrigin","text":""},{"doctype":"document","id":"documents/docs/interfaces.md","title":"Interfaces","text":"Interfaces FastAI.jl provides many interfaces that allow extending its functionality Learning task interfaces Learning tasks form the core of FastAI.jl's high-level API See  this tutorial  for a motivation and introduction Functions for the learning task interfaces always dispatch on a  LearningTask  A  LearningTask  defines everything that needs to happen to turn an input into a target and much more  LearningTask  should be a  struct  containing configuration Core interface Enables training and prediction Prerequisite for other optional learning task interfaces Required tasks encode  or both  encodeinput  and  encodetarget  decodeŷ Optional tasks shouldbatch Enables use of taskdataset taskdataloaders predict predictbatch Plotting interface For visualizing observations and predictions using  Makie.jl  Training interface Convenience for creating  Learner s Required methods tasklossfn taskmodel Enables use of tasklearner Testing interface Automatically test interfaces Required tasks mockmodel mocksample  or both  mockinput  and  mocktarget Enables use of checktask_core Callback interface See the  FluxTraining.jl tutorial  Data container interface"},{"doctype":"documentation","id":"references/Flux.Embedding","title":"Embedding","text":"A lookup table that stores embeddings of dimension  out  for a vocabulary of size  in  This layer is often used to store word embeddings and retrieve them using indices The input to the layer can be either a vector of indexes or the corresponding onehot encoding  Flux.OneHotArray Examples"},{"doctype":"documentation","id":"references/FastAI.Datasets.DATASETS_IMAGECLASSIFICATION","title":"DATASETS_IMAGECLASSIFICATION","text":""},{"doctype":"documentation","id":"references/FastAI.encodesample!","title":"encodesample!","text":""},{"doctype":"documentation","id":"references/FluxTraining.Events.StepBegin","title":"StepBegin","text":"Event  called at the beginning of a batch"},{"doctype":"documentation","id":"references/FluxTraining.AbstractMetric","title":"AbstractMetric","text":"Abstract type for metrics passed to  Metrics  For most use cases you should use  Metric  the standard implementation Interface If  Metric  doesn't fit your use case you can create a new subtype of  AbstractMetric  and implement the following methods to make it compatible with  Metrics  reset metric step metric learner stepvalue metric epochvalue metric metricname metric"},{"doctype":"documentation","id":"references/MLUtils.Datasets.load_iris","title":"load_iris","text":"Loads the first 150 observations from the Iris flower data set introduced by Ronald Fisher 1936 The 4 by 150 matrix  X  contains the numeric measurements in which each individual column denotes an observation The vector  y  contains the class labels as strings The vector  names  contains the names of the features i.e rows of  X  1 Fisher Ronald A The use of multiple measurements in taxonomic problems Annals of eugenics 7.2 1936 179-188"},{"doctype":"documentation","id":"references/FastTabular.gettransforms","title":"gettransforms","text":"Returns a composition of basic tabular transformations constructed for the given TableDataset"},{"doctype":"documentation","id":"references/DataAugmentation.showgrid","title":"showgrid","text":""},{"doctype":"documentation","id":"references/DataAugmentation.project","title":"project","text":"Project  item  using projection  P  and crop to  indices  if given"},{"doctype":"documentation","id":"references/FluxTraining.formataccess","title":"formataccess","text":""},{"doctype":"documentation","id":"references/FastAI.tasklossfn","title":"tasklossfn","text":"Default loss function to use when training models for  task "},{"doctype":"documentation","id":"references/FluxTraining.Loggables","title":"Loggables","text":""},{"doctype":"documentation","id":"references/FastVision.augs_lighting","title":"augs_lighting","text":"Helper to create a set of lighting transformations for image data With probability  p  applies  AdjustBrightness intensity  and  AdjustContrast intensity "},{"doctype":"documentation","id":"references/MLUtils.obsview","title":"obsview","text":"Returns a lazy view of the observations in  data  that correspond to the given  indices  No data will be copied except of the indices It is similar to constructing an  ObsView  but returns a  SubArray  if the type of  data  is  Array  or  SubArray  Furthermore this function may be extended for custom types of  data  that also want to provide their own subset-type In case  data  is a tuple the constructor will be mapped over its elements That means that the constructor returns a tuple of  ObsView  instead of a  ObsView  of tuples If instead you want to get the subset of observations corresponding to the given  indices  in their native type use  getobs  See  ObsView  for more information"},{"doctype":"documentation","id":"references/Flux.Losses.dice_coeff_loss","title":"dice_coeff_loss","text":"Return a loss based on the dice coefficient Used in the  V-Net  image segmentation architecture Similar to the F1_score Calculated as"},{"doctype":"documentation","id":"references/FastTabular.linbndrop","title":"linbndrop","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.ClipValue","title":"ClipValue","text":"Clip gradients when their absolute value exceeds  thresh "},{"doctype":"documentation","id":"references/DataAugmentation.BoundingBox","title":"BoundingBox","text":"StaticArrays points SVector SVector item points item Item wrapper around  Keypoints  Examples"},{"doctype":"documentation","id":"references/DataAugmentation.AbstractItem","title":"AbstractItem","text":"Abstract supertype for all items To implement items subtype either  Item  to create a new item or  ItemWrapper  to wrap an existing item"},{"doctype":"documentation","id":"references/MLUtils.numobs","title":"numobs","text":"Return the total number of observations contained in  data  If  data  does not have  numobs  defined then this function falls back to  length(data  Authors of custom data containers should implement  Base.length  for their type instead of  numobs   numobs  should only be implemented for types where there is a difference between  numobs  and  Base.length  such as multi-dimensional arrays See also  getobs"},{"doctype":"documentation","id":"references/FastAI.propagate","title":"propagate","text":"Whether the wrapper type should be kept after encoding the wrapped block with  encoding "},{"doctype":"documentation","id":"references/Flux.Losses.focal_loss","title":"focal_loss","text":"Return the  focal_loss  which can be used in classification tasks with highly imbalanced classes It down-weights well-classified examples and focuses on hard examples The input ŷ is expected to be normalized i.e  softmax  output The modulating factor  γ  controls the down-weighting strength For  γ  0  the loss is mathematically equivalent to  Losses.crossentropy  Example See also  Losses.binary_focal_loss  for binary not one-hot labels"},{"doctype":"documentation","id":"references/FastMakie","title":"FastMakie","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.call","title":"call","text":""},{"doctype":"documentation","id":"references/DataAugmentation.FromRandom","title":"FromRandom","text":""},{"doctype":"documentation","id":"references/Flux.SkipConnection","title":"SkipConnection","text":"Create a skip connection which consists of a layer or  Chain  of consecutive layers and a shortcut connection linking the block's input to the output through a user-supplied 2-argument callable The first argument to the callable will be propagated through the given  layer  while the second is the unchanged skipped input The simplest ResNet\"-type connection is just  SkipConnection(layer   Here is a more complicated example See also  Parallel   Maxout "},{"doctype":"documentation","id":"references/FastAI.axiskwargs","title":"axiskwargs","text":""},{"doctype":"documentation","id":"references/FluxTraining.CustomCallback","title":"CustomCallback","text":"cb learner println A callback that runs  f(learner  every time an event of type  Event  during a phase of type in  Phase  If  f  needs to access learner state pass  access  a named tuple in the same form as  stateaccess  Instead of using  CustomCallback  it is recommended to properly implement a  Callback  Examples We can get a quick idea of when a new epoch starts as follows"},{"doctype":"documentation","id":"references/DataAugmentation.onehot","title":"onehot","text":""},{"doctype":"document","id":"documents/docs/introduction.md","title":"Introduction","text":"data blocks load task blocks learner task data callbacks learner task learner data blocks load image class sample data class image blocks task blocks task classes learner task data callbacks dls data task model task lossfn task learner model dls lossfn learner task learner Introduction This tutorial explains the qickstart examples and some core abstractions FastAI.jl is built on On the  quickstart page  we showed how to train models on common tasks in a few lines of code like these Each of the five lines encapsulates one part of the deep learning pipeline to give a high-level API while still allowing customization Let's have a closer look Dataset This line downloads and loads the  ImageNette  image classification dataset a small subset of ImageNet with 10 different classes  data  is a  data container  that can be used to load individual observations here of images and the corresponding labels We can use  getobs(data i  to load the  i th observation and  numobs  to find out how many observations there are blocks  describe the format of the data that you want to use for learning For supervised training tasks they are a tuple of  inputblock targetblock  Since we want to do image classification the input block is  Image{2  representing a 2-dimensional image and the target block is  Label(classes  representing the class the image belongs to Learning task The next line defines a learning task which encapsulates the data preprocessing pipeline and other logic related to the task  ImageClassificationSingle  is a simple wrapper around  BlockTask  which takes in blocks and data processing steps so-called  encodings  Using it we can replace the above line with Based on the blocks and encodings the learning task can derive lots of functionality data processing visualization constructing task-specific models from a backbone creating a loss function Learner Next we create a  Learner  that encapsulates everything needed for training including parallelized training and validation data loaders using  taskdataloaders a loss function using  tasklossfn a task-specific model using  taskmodel The customizable expanded version of the code looks like this At this step we can also pass in any number of  callbacks  to customize the training Here  ToGPU  ensures an available GPU is used and  Metrics  adds additional metrics to track during training Training Training now is quite simple You have several options for high-level training schedules lrfind  to run a learning rate finder finetune  for when you're using a pretrained backbone fitonecycle  for when you're training a model from scratch Visualization Finally the last line visualizes the predictions of the trained model It takes some samples from the training data loader runs them through the model and decodes the outputs How each piece of data is visualized is also inferred through the blocks in the learning task"},{"doctype":"documentation","id":"references/Flux.Optimise.Nesterov","title":"Nesterov","text":"opt opt Gradient descent optimizer with learning rate  η  and Nesterov momentum  ρ  Parameters Learning rate  η  Amount by which gradients are discounted before updating the weights Nesterov momentum  ρ  Controls the acceleration of gradient descent in the prominent direction in effect damping oscillations Examples"},{"doctype":"documentation","id":"references/DataAugmentation.ProjectiveTransform","title":"ProjectiveTransform","text":"Abstract supertype for projective transformations See  Projective transformations "},{"doctype":"documentation","id":"references/FastAI.showblock","title":"showblock","text":"Show a block or blocks of obs to  backend  ShowBackend  block  can be a  Block  a tuple of  block s or a  Pair  of  title  block "},{"doctype":"document","id":"documents/docs/notebooks/timeseriesclassification.ipynb","title":"TimeSeries Classification","text":"data blocks load input class sample data task blocks TSPreprocessing blocks data table task blocks sample sample task encoded_sample task sample sample data task sample blocks sample TimeSeries Classification getobs  gets us a sample from the TimeSeriesDataset It returns a tuple with the input time series and the correspodning label Now we create a learning task for time-series classification This means using the time-series to predict labels We will use the  TimeSeriesRow  block as input and  Label  block as the target The encodings passed in transform samples into formats suitable as inputs and outputs for a model Let's check that samples from the created data container conform to the blocks of the learning task To get an overview of the learning task created and as a sanity test we can use describetask This shows us what encodings will be applied to which blocks and how the predicted ŷ values are decoded Visualization Tools for TimeSeries"},{"doctype":"documentation","id":"references/Flux.orthogonal","title":"orthogonal","text":"Return an  Array{Float32  of the given  size  which is a semi orthogonal matrix as described in 1 Cannot construct a vector i.e  length(size  1  is forbidden For  length(size  2  a  prod(size[1:(end  1  by  size[end  orthogonal matrix is computed before reshaping it to the original dimensions Examples References 1 Saxe McClelland Ganguli Exact solutions to the nonlinear dynamics of learning in deep linear neural networks ICLR 2014 https://arxiv.org/abs/1312.6120"},{"doctype":"documentation","id":"references/Flux.glorot_normal","title":"glorot_normal","text":"Return an  Array{Float32  of the given  size  containing random numbers drawn from a normal distribution with standard deviation  gain  sqrt(2  fan_in  fan_out  using  nfan   Flux.nfan This method is described in 1 and also known as Xavier initialization Examples References 1 Glorot Xavier and Yoshua Bengio Understanding the difficulty of training deep feedforward neural networks  Proceedings of the thirteenth international conference on artificial intelligence and statistics  2010"},{"doctype":"documentation","id":"references/DataAugmentation.allequal","title":"allequal","text":""},{"doctype":"documentation","id":"references/Flux.epseltype","title":"epseltype","text":""},{"doctype":"documentation","id":"references/MLUtils.MappedData","title":"MappedData","text":""},{"doctype":"documentation","id":"references/FastVision.Models.ResBlock","title":"ResBlock","text":""},{"doctype":"documentation","id":"references/DataAugmentation.Maybe","title":"Maybe","text":"With probability  p  apply transformation  tfm "},{"doctype":"documentation","id":"references/FastVision.IMAGENET_STDS","title":"IMAGENET_STDS","text":""},{"doctype":"documentation","id":"references/FastAI.encodingscolumn","title":"encodingscolumn","text":""},{"doctype":"documentation","id":"references/Flux.onehotbatch","title":"onehotbatch","text":"Returns a  OneHotMatrix  where  k th column of the matrix is  onehot(xs[k labels   onehot This is a sparse matrix which stores just a  Vector{UInt32  containing the indices of the nonzero elements If one of the inputs in  xs  is not found in  labels  that column is  onehot(default labels  if  default  is given else an error If  xs  has more dimensions  M  ndims(xs  1  then the result is an  AbstractArray{Bool M+1  which is one-hot along the first dimension i.e  result k  onehot(xs[k labels  Note that  xs  can be any iterable such as a string And that using a tuple for  labels  will often speed up construction certainly for less than 32 classes Examples"},{"doctype":"documentation","id":"references/Flux.OneHotVector","title":"OneHotVector","text":"These are constructed by  onehot  and  onehotbatch  Parameter  I  is the type of the underlying storage and  T  its eltype"},{"doctype":"documentation","id":"references/DataAugmentation.Identity","title":"Identity","text":"The identity transformation"},{"doctype":"documentation","id":"references/Flux.hasaffine","title":"hasaffine","text":"Return  true  if a normalisation layer has trainable shift and scale parameters  false  otherwise See  BatchNorm   InstanceNorm   GroupNorm  and  LayerNorm "},{"doctype":"documentation","id":"references/FastAI.blockbackbone","title":"blockbackbone","text":"Create a default backbone that takes in block  inblock "},{"doctype":"documentation","id":"references/FastAI.ShowBackend","title":"ShowBackend","text":"Abstract type for backends that allow showing blocks of data in an interpretable way Extending For a  ShowBackend   Backend  you should implement the following methods createhandle Backend  creates a context that blocks of data can be shown to showblock handle Backend block::B obs  shows a block of type  B  This needs to be implemented for every block type you want to be able to show showblocks handle Backend blocks obss  shows a collection of blocks"},{"doctype":"documentation","id":"references/Flux.Losses.binary_focal_loss","title":"binary_focal_loss","text":"Return the  binary_focal_loss  The input ŷ is expected to be normalized i.e  softmax  output For  γ  0  the loss is mathematically equivalent to  Losses.binarycrossentropy  Example See also  Losses.focal_loss  for multi-class setting"},{"doctype":"documentation","id":"references/Flux.paramtype","title":"paramtype","text":""},{"doctype":"document","id":"documents/docs/notebooks/registries.ipynb","title":"Feature registries in FastAI.jl","text":"ImageShow _show reg show IOContext stdout displaysize reg _show _show load _show recipe _show data blocks load recipe data _show _show task load blocks size learner task data downloaded description ismissing _show blocks Any _show blocks Any _show Feature registries in FastAI.jl Datasets We can get more information on a specific dataset And load it triggering a lazy download Dataset recipes Of course to load datasets into a format that we can work with FastAI.jl has so-called dataset recipes We can likewise look at an entry And load it giving us a ready-to-use data container and blocks Learning tasks Finally learning tasks can also be listed Finding features Aside from listing a big table with features we can also find entries that are relevant to us For example  find all the datasets that have already been downloaded Find all dataset recipes with classification targets Find all learning tasks with image inputs Outlook This work will support other future efforts Domain libraries  make it easy for third-party libraries to contribute features datasets recipes models tasks encodings and easy for users to discover these features No-code interfaces  having a consistent way to search for features and relating them to relevant  Block s makes it possible to build no-code dropdown-based interfaces to choose an appropriate dataset find a learning task or build a model for a task and finally train a model"},{"doctype":"documentation","id":"references/Flux.LSTM","title":"LSTM","text":"Long Short Term Memory  recurrent layer Behaves like an RNN but generally exhibits a longer memory span over sequences The arguments  in  and  out  describe the size of the feature vectors passed as input and as output That is it accepts a vector of length  in  or a batch of vectors represented as a  in x B  matrix and outputs a vector of length  out  or a batch of vectors of size  out x B  This constructor is syntactic sugar for  Recur(LSTMCell(a  and so LSTMs are stateful Note that the state shape can change depending on the inputs and so it is good to  reset  the model between inference calls if the batch size changes See the examples below See  this article  for a good overview of the internals Examples Batch size changes Failing to call  reset  when the input batch size changes can lead to unexpected behavior See the example in  RNN "},{"doctype":"documentation","id":"references/FastAI.showprediction","title":"showprediction","text":"Show a prediction  pred  If a  sample  is also given show it next to the prediction ŷ"},{"doctype":"documentation","id":"references/MLUtils.Datasets.make_sin","title":"make_sin","text":"Generates  n  noisy equally spaces samples of a sinus from  start  to  stop  by adding  noise  f_rand(length(x  to the result of  fun(x "},{"doctype":"documentation","id":"references/MLUtils.Datasets","title":"Datasets","text":""},{"doctype":"documentation","id":"references/FastVision.ImageClassificationMulti","title":"ImageClassificationMulti","text":"Learning task for multi-label image classification Images are resized to  size  and classified into multiple of  classes  Use  ImageClassificationSingle  for the single-class setting Keyword arguments computestats  false  Whether to compute image statistics on dataset  data  or use default ImageNet stats aug_projections   DataAugmentation.Identity  augmentation to apply during  ProjectiveTransforms  resizing and cropping aug_image   DataAugmentation.Identity  pixel-level augmentation to apply during  ImagePreprocessing C  RGB{N0f8  Color type images are converted to before further processing Use  Gray{N0f8  for grayscale images"},{"doctype":"document","id":"documents/docs/discovery.md","title":"Discovery","text":"blocks ENV data blocks load findfirst id blocks image mask sample data size sample eltype sample inputblock targetblock blocks targetblock inputblock targetblock image mask blocks blocks task blocks size Discovery As you may have seen in  the introduction  FastAI.jl makes it possible to train models in just 5 lines of code However if you have a task in mind you need to know what datasets you can train on and if there are convenience learning task constructors For example the introduction loads the  imagenette2-160  dataset and uses  ImageClassificationSingle  to construct a learning task Now what if instead of classifying an image into one class we want to classify every single pixel into a class semantic segmentation Now we need a dataset with pixel-level annotations and a learning task that can process those segmentation masks For finding both we can make use of  Block s A  Block  represents a kind of data for example images labels or keypoints For supervised learning tasks we have an input block and a target block If we wanted to classify whether 2D images contain a cat or a dog we could use the blocks  Image{2 Label([\"cat dog  while for semantic segmentation we'll have an input  Image  block and a target  Mask  block Finding a dataset To find a dataset with compatible samples we can pass the types of these blocks as a filter to  datasets  which will show us only dataset recipes for loading those blocks We can see that the  camvid_tiny  dataset can be loaded so that each sample is a pair of an image and a segmentation mask Let's use a data recipe to load a  data container  and concrete blocks As with every data container we can load a sample using  getobs  which gives us a tuple of an image and a segmentation mask Loading the dataset recipe also returned  blocks  which are the concrete  Block  instances for the dataset We passed in  types  of blocks  Image Mask  and get back  instances  since the specifics of some blocks depend on the dataset For example the returned target block carries the labels for every class that a pixel can belong to With these  blocks  we can also validate a sample of data using  checkblock  which is useful as a sanity check when using custom data containers Summary In short if you have a learning task in mind and want to load a dataset for that task then define the types of input and target block e.g  blocktypes  Image Label  use  filter datarecipes  blocks=blocktypes  to find compatbile dataset recipes and run  load datarecipes id  to load a data container and the concrete blocks Exercises Find and load a dataset for multi-label image classification Hint the block for multi-category outputs is called  LabelMulti  List all datasets with  Image  as input block and any target block Hint the supertype of all types is  Any  Finding a learning task Armed with a dataset we can go to the next step creating a learning task Since we already have blocks defined this amounts to defining the encodings that are applied to the data before it is used in training Here FastAI.jl already defines some convenient constructors for learning tasks and you can find them with  learningtasks  Here we can pass in either block types as above or the block instances Looks like we can use the  ImageSegmentation  function to create a learning task Every function returned can be called with  blocks  and optionally some keyword arguments for customization And that's the basic workflow for getting started with a supervised task Exercises Find all learning task functions with images as inputs"},{"doctype":"documentation","id":"references/FluxTraining.runtests","title":"runtests","text":"Equivalent to  ReTest.retest(FluxTraining pattern kwargs  This function is defined automatically in any module containing a  testset  possibly nested within submodules"},{"doctype":"documentation","id":"references/DataAugmentation.getbounds","title":"getbounds","text":"Return the spatial bounds of  item  For a 2D-image  Image{2  the bounds are the 4 corners of the bounding rectangle In general for an N-dimensional item the bounds are a vector of the N^2 corners of the N-dimensional hypercube bounding the data"},{"doctype":"documentation","id":"references/Flux.patience","title":"patience","text":"Return a function that internally counts by one when  predicate  true  otherwise the count is reset to zero If the count is greater than or equal to  wait  the function returns  true  otherwise it returns  false  Examples"},{"doctype":"documentation","id":"references/Flux.zeros","title":"zeros","text":""},{"doctype":"documentation","id":"references/FastAI.IndexGrouper","title":"IndexGrouper","text":""},{"doctype":"documentation","id":"references/FastAI.taskdataset","title":"taskdataset","text":"Transform data container  data  of samples into a data container of  x y pairs Maps  encodesample(task context sample  over the observations in  data "},{"doctype":"documentation","id":"references/DataAugmentation.project!","title":"project!","text":"Project  item  using projection  P  and crop to  indices  if given Store result in  bufitem  Inplace version of  project  Default implementation falls back to  project "},{"doctype":"documentation","id":"references/FastAI.Registries.runtests","title":"runtests","text":"Equivalent to  ReTest.retest(FastAI.Registries pattern kwargs  This function is defined automatically in any module containing a  testset  possibly nested within submodules"},{"doctype":"documentation","id":"references/Flux.expand","title":"expand","text":""},{"doctype":"documentation","id":"references/Flux.Losses.kldivergence","title":"kldivergence","text":"Return the  Kullback-Leibler divergence  between the given probability distributions The KL divergence is a measure of how much one probability distribution is different from the other It is always non-negative and zero only when both the distributions are equal Example"},{"doctype":"documentation","id":"references/Flux.AdaptiveMeanPool","title":"AdaptiveMeanPool","text":"Adaptive mean pooling layer Calculates the necessary window size such that its output has  size(y)[1:N  out  Expects as input an array with  ndims(x  N+2  i.e channel and batch dimensions after the  N  feature dimensions where  N  length(out  See also  MaxPool   AdaptiveMaxPool  Examples"},{"doctype":"documentation","id":"references/FastAI.ParamGrouper","title":"ParamGrouper","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.WeightDecay","title":"WeightDecay","text":"opt Decay weights by  λ  Typically composed  with other optimizers as the first transformation to the gradient making it equivalent to adding  L_2  regularization with coefficient   λ  to the loss Examples"},{"doctype":"documentation","id":"references/FluxTraining.ES","title":"ES","text":""},{"doctype":"documentation","id":"references/FluxTraining.runafter","title":"runafter","text":""},{"doctype":"documentation","id":"references/FluxTraining.MetricsPrinter","title":"MetricsPrinter","text":"Callback that prints metrics after every epoch Relies on the metrics computed by  Metrics  so will error if no  Metrics  callback is used This callback is added by default to every  Learner  unless you pass in  usedefaultcallbacks  false "},{"doctype":"documentation","id":"references/MLUtils.RingBuffer","title":"RingBuffer","text":"A  Channel like data structure that rotates through  size  buffers You can either pass in a vector of  buffers  or a single  buffer  that is copied  size  times put s work by mutating one of the buffers The result can then be  take n Invalidation Only one result is valid at a time On the next  take  the previous result will be reused as a buffer and be mutated by a  put"},{"doctype":"documentation","id":"references/FastVision.Models.PixelShuffleICNR","title":"PixelShuffleICNR","text":""},{"doctype":"documentation","id":"references/FluxTraining.ConditionalCallback","title":"ConditionalCallback","text":"Wrapper callback that only forwards events to the wrapped callback if  CallbackCondition   condition  is met See  throttle "},{"doctype":"documentation","id":"references/Flux.Optimise.RMSProp","title":"RMSProp","text":"opt opt Optimizer using the  RMSProp  algorithm Often a good choice for recurrent networks Parameters other than learning rate generally don't need tuning Parameters Learning rate  η  Amount by which gradients are discounted before updating the weights Momentum  ρ  Controls the acceleration of gradient descent in the prominent direction in effect damping oscillations Examples"},{"doctype":"documentation","id":"references/Flux.istraining","title":"istraining","text":""},{"doctype":"documentation","id":"references/DataAugmentation.mask_extrapolation","title":"mask_extrapolation","text":""},{"doctype":"documentation","id":"references/DataAugmentation.transformbounds","title":"transformbounds","text":"Apply  CoordinateTransformations.Transformation  to  bounds "},{"doctype":"documentation","id":"references/Flux.Scale","title":"Scale","text":"Create an element-wise layer whose forward pass is given by This uses    instead of matrix multiplication    of  Dense  The learnable scale  bias are initialised  init(size  and  zeros32(size  with  init=ones32  by default You may specify the function  init  turn off trainable bias with  bias=false  or provide the array(s explicitly Used by  LayerNorm  with  affine=true  Examples"},{"doctype":"documentation","id":"references/FastAI.PropagateNever","title":"PropagateNever","text":"Never propagate a wrapper type See  propagate  for more information"},{"doctype":"documentation","id":"references/FastAI.Datasets.grandparentname","title":"grandparentname","text":""},{"doctype":"documentation","id":"references/DataAugmentation.ScaleRatio","title":"ScaleRatio","text":""},{"doctype":"documentation","id":"references/FastAI.finetune!","title":"finetune!","text":"Behaves like the fastai implementation  fastai.Learner.fine_tune  Keyword arguments freezeepochs  1  Number of epochs to train with the backbone completely frozen grouper  FastAI.defaultgrouper(learner.model   ParamGrouper  which assigns groups  1  backbone or  2  head for every parameter in  learner.model  The default expects  learner.model  to be a  Chain(backbone head  backbone_factor  0.1  Factor by which updates to backbone model are discounted during the second phase of training Any additional keyword arguments are passed to  fitonecycle "},{"doctype":"documentation","id":"references/FastAI.Context","title":"Context","text":"Represents a context in which a data transformation is made This allows using dispatching for varying behavior for example to apply augmentations only during training or use non-destructive cropping during inference See  Training   Validation  and  Inference "},{"doctype":"documentation","id":"references/DataAugmentation.ComposedProjectiveTransform","title":"ComposedProjectiveTransform","text":"Wrap multiple projective  tfms  and apply them efficiently The projections are fused into a single projection and only points inside the final crop are evaluated"},{"doctype":"documentation","id":"references/FastVision.plotimage!","title":"plotimage!","text":""},{"doctype":"documentation","id":"references/DataAugmentation.showbounds!","title":"showbounds!","text":""},{"doctype":"documentation","id":"references/FluxTraining.EarlyStopping","title":"EarlyStopping","text":"model lossfn callbacks Disjunction InvalidValue TimeLimit callback Disjunction InvalidValue TimeLimit model lossfn callbacks callback Stop training early when  criteria  are met See  EarlyStopping.jl  for available stopping criteria Passing an integer  n  uses the simple patience criterion stop if the validation loss hasn't increased for  n  epochs You can control which phases are taken to measure the out-of-sample loss and the training loss with keyword arguments  trainphase  default  AbstractTrainingPhase  and  testphase  default  AbstractValidationPhase  Examples"},{"doctype":"documentation","id":"references/DataAugmentation.setwrapped","title":"setwrapped","text":""},{"doctype":"documentation","id":"references/FastMakie.gridlayout","title":"gridlayout","text":""},{"doctype":"documentation","id":"references/FluxTraining.Write","title":"Write","text":""},{"doctype":"documentation","id":"references/FastVision.Models.XResNet","title":"XResNet","text":"Create an XResNet model backbone following the  implementation in   fastai  c_in::Int  3  The number of input channels e.g  1  for grayscale images and  3  for RGB images ndim::Int  2  The number of dimensions for the convolutional and pooling layers e.g  2  for 2D input images and  3  for 3D volumes"},{"doctype":"documentation","id":"references/FluxTraining.callbackgraph","title":"callbackgraph","text":"Creates a directed acyclic graph from a list of  callbacks  Ordering is given through  runafter  and  resolveconflict  If a write conflict cannot be resolved i.e  resolveconflict  is not implemented throws an error"},{"doctype":"documentation","id":"references/DataAugmentation.PinOrigin","title":"PinOrigin","text":"Projective transformation that translates the data so that the upper left bounding corner is at the origin  0 0  or the multidimensional equivalent Projective transformations on images return  OffsetArray s but not on keypoints Hardware like GPUs do not support OffsetArrays so they will be unwrapped and no longer match up with the keypoints Pinning the data to the origin makes sure that the resulting  OffsetArray  has the same indices as a regular array starting at one"},{"doctype":"documentation","id":"references/FluxTraining.TensorBoardBackend","title":"TensorBoardBackend","text":"TensorBoard backend for logging callbacks Takes the same arguments as  TensorBoardLogger.TBLogger "},{"doctype":"documentation","id":"references/FluxTraining.garbagecollect","title":"garbagecollect","text":""},{"doctype":"documentation","id":"references/DataAugmentation.Categorify","title":"Categorify","text":"cols col1 col2 col3 row zip cols item row cols catdict Dict col1 tfm catdict col1 tfm item Label encodes the values of a row present in  TabularItem  for the columns specified in  cols  using  dict  which contains the column names as dictionary keys and the unique values of column present as dictionary values if there are any  missing  values in the values to be transformed they are replaced by 1 Example"},{"doctype":"documentation","id":"references/FastVision.ImageTableMultiLabel","title":"ImageTableMultiLabel","text":""},{"doctype":"documentation","id":"references/MLUtils.DataLoader","title":"DataLoader","text":"An object that iterates over mini-batches of  data  each mini-batch containing  batchsize  observations except possibly the last one Takes as input a single data array a tuple or a named tuple of arrays or in general any  data  object that implements the  numobs  and  getobs  methods The last dimension in each array is the observation dimension i.e the one divided into mini-batches The original data is preserved in the  data  field of the DataLoader Arguments data  The data to be iterated over The data type has to be supported by  numobs  and  getobs  batchsize  If less than 0 iterates over individual observations Otherwise each iteration except possibly the last yields a mini-batch containing  batchsize  observations Default  1  buffer  If  buffer=true  and supported by the type of  data  a buffer will be allocated and reused for memory efficiency You can also pass a preallocated object to  buffer  Default  false  collate  Batching behavior If  nothing  default a batch is  getobs(data indices  If  false  each batch is  getobs(data i for i in indices  When  true  applies  batch  to the vector of observations in a batch recursively collating arrays in the last dimensions See  batch  for more information and examples parallel  Whether to use load data in parallel using worker threads Greatly speeds up data loading by factor of available threads Requires starting Julia with multiple threads Check  Threads.nthreads  to see the number of available threads  Passing  parallel  true  breaks ordering guarantees  Default  false  partial  This argument is used only when  batchsize  0  If  partial=false  and the number of observations is not divisible by the batchsize then the last mini-batch is dropped Default  true  rng  A random number generator Default  Random.GLOBAL_RNG  shuffle  Whether to shuffle the observations before iterating Unlike wrapping the data container with  shuffleobs(data   shuffle=true  ensures that the observations are shuffled anew every time you start iterating over  eachobs  Default  false  Examples"},{"doctype":"documentation","id":"references/DataAugmentation.scaleprojection","title":"scaleprojection","text":""},{"doctype":"documentation","id":"references/FastTabular.TabularModel","title":"TabularModel","text":"Create a tabular model which operates on a tuple of categorical values label or one-hot encoded and continuous values The categorical backbones  catbackbone  and continuous backbone  contbackbone  operate on each element of the input tuple The output from these backbones is then passed through a series of linear-batch norm-dropout layers before a  finalclassifier  block Keyword arguments outsize  The output size of the final classifier block For single classification tasks this would be the number of classes and for regression tasks this would be the number of target continuous variables layersizes  A vector of sizes for each hidden layer in the sequence of linear layers dropout_rates  Dropout probabilities for the linear-batch norm-dropout layers This could either be a single number which would be used for for all the layers or a collection of numbers which are cycled through for each layer batchnorm  Set to  false  to skip each batch norm in the linear-batch norm-dropout sequence activation  The activation function to use in the classifier layers linear_first  Controls if the linear layer comes before or after batch norm and dropout Create a tabular model which operates on a tuple of categorical values label or one-hot encoded and continuous values The default categorical backbone  catbackbone  is a  Flux.Parallel  set of  Flux.Embedding  layers corresponding to each categorical variable The default continuous backbone  contbackbone  is a single  Flux.BatchNorm  The output from these backbones is concatenated then passed through a series of linear-batch norm-dropout layers before a  finalclassifier  block Arguments n_cont  The number of continuous columns outsize  The output size of the model layersizes  A vector of sizes for each hidden layer in the sequence of linear layers Keyword arguments cardinalities  A collection of sizes number of classes for each categorical column size_overrides  An optional argument which corresponds to a collection containing embedding sizes to override the value returned by the rule of thumb for a particular index corresponding to  cardinalities  or  nothing "},{"doctype":"documentation","id":"references/Flux.Optimise.StopException","title":"StopException","text":""},{"doctype":"documentation","id":"references/MLUtils.unsqueeze","title":"unsqueeze","text":"Return  x  reshaped into an array one dimensionality higher than  x  where  dims  indicates in which dimension  x  is extended See also  flatten   stack  Examples Returns a function which acting on an array inserts a dimension of size 1 at  dims  Examples"},{"doctype":"documentation","id":"references/FastVision.imagedatasetstats","title":"imagedatasetstats","text":"Given a data container of images  data  compute the color channel-wise means and standard deviations across all observations Images are converted to color type  C  e.g  RGB{N0f8   Gray{N0f8  before statistics are calculated If  progress  true  show a progress bar"},{"doctype":"documentation","id":"references/DataAugmentation.showbounds","title":"showbounds","text":""},{"doctype":"documentation","id":"references/DataAugmentation.Keypoints","title":"Keypoints","text":"StaticArrays points SVector y x y x zip item points item N dimensional keypoints represented as  SVector{N T  Spatial bounds are given by the polygon  bounds::Vector{SVector{N T  or  sz::NTuple{N Int  Examples"},{"doctype":"documentation","id":"references/FastAI.showoutputs","title":"showoutputs","text":"Show model outputs to  backend  If a vector of encoded samples  encsamples  is also given show them next to the outputs Use  showoutputbatch  to show collated batches of outputs Run a trained model in  learner  on  n  samples and visualize the outputs"},{"doctype":"documentation","id":"references/Flux.GRUv3Cell","title":"GRUv3Cell","text":""},{"doctype":"document","id":"documents/docs/notebooks/serialization.ipynb","title":"Saving and loading models for inference","text":"Metalhead dir joinpath load data dir filterfn loadfn classes unique data task classes backbone Metalhead ResNet50 layers end learner task data backbone backbone callbacks learner task learner model force task model model model x y samples data i i rand data images sample sample samples labels sample sample samples preds task model images device context acc sum labels preds length preds CairoMakie task collect zip images preds Saving and loading models for inference In the end we train models because we want to use them for inference that is using them to generate predictions on new targets The general formula for doing this in FastAI.jl is to first train a  model  for a  task  for example using  fitonecycle  or  finetune  and then save the model and the learning task configuration to a file using  savetaskmodel  In another session you can then use  loadtaskmodel  to load both Since the learning task contains all preprocessing logic we can then use  predict  and  predictbatch  to generate predictions for new inputs Let's fine-tune an image classification model see  here  for more info and go through that process Now we can save the model using  savetaskmodel  In another session we can now use  loadtaskmodel  to load both model and learning task from the file Since the model weights are transferred to the CPU before being saved we need to move them to the GPU manually if we want to use that for inference Finally let's select 9 random images from the dataset and see if the model classifies them correctly"},{"doctype":"documentation","id":"references/FastAI.checktask_core","title":"checktask_core","text":"Check if  task  conforms to the  core interface   sample  and  model  are used for testing If you have implemented the testing interface and don't supply these as arguments  mocksample(task  and  mockmodel(task  will be used"},{"doctype":"documentation","id":"references/FastVision.ImageSegmentationFolders","title":"ImageSegmentationFolders","text":"Dataset recipe for loading 2D image segmentation datasets from a common format where images and masks are stored as images in two different subfolders  root>/<imagefolder  and  root>/<maskfolder  The class labels should be in a newline-delimited file  root>/<labelfile "},{"doctype":"documentation","id":"references/FastAI.BlockTask","title":"BlockTask","text":"Create an  AbstractBlockTask  directly passing in a named tuple  blocks  and  encodings  See  SupervisedTask  for supervised training tasks"},{"doctype":"documentation","id":"references/Flux.Losses.add_blanks","title":"add_blanks","text":"add_blanks(z Adds blanks to the start and end of  z  and between items in  z"},{"doctype":"documentation","id":"references/FluxTraining.Events.LossBegin","title":"LossBegin","text":"Event  called between calculating  y_pred  and calculating loss"},{"doctype":"documentation","id":"references/FluxTraining.Recorder","title":"Recorder","text":"Maintains a  History  It's stored in  learner.cbstate.history "},{"doctype":"documentation","id":"references/FastVision.Models.pixelshufflehead","title":"pixelshufflehead","text":""},{"doctype":"documentation","id":"references/FluxTraining.LogMetrics","title":"LogMetrics","text":"logcb model lossfn callbacks logcb Callback that logs step and epoch metrics to one or more  LoggerBackend s See also  LoggerBackend   Loggables.Loggable   log_to   TensorBoardBackend Example"},{"doctype":"documentation","id":"references/Flux.Losses.xlogx","title":"xlogx","text":"Return  x  log(x  for  x ≥ 0  handling  x  0  by taking the limit from above to get zero"},{"doctype":"documentation","id":"references/MLUtils.group_indices","title":"group_indices","text":"Computes the indices of elements in the vector  x  for each distinct value contained This information is useful for resampling strategies such as stratified sampling See also  group_counts  Examples"},{"doctype":"documentation","id":"references/Flux.loadparams!","title":"loadparams!","text":""},{"doctype":"documentation","id":"references/MLUtils.eachbatch","title":"eachbatch","text":""},{"doctype":"documentation","id":"references/FastVision.blockitemtype","title":"blockitemtype","text":"Return a constructor for a  DataAugmentation.Item  that can be projected Return  nothing  by default indicating that  block  cannot be turned into a projectable item for bounds with dimensionality  N  For example we have but"},{"doctype":"documentation","id":"references/Flux.PairwiseFusion","title":"PairwiseFusion","text":"y1 layer1 x1 y2 layer2 connection x2 y1 y3 layer3 connection x3 y2 y layers x i length layers y i connection x layers i y i Arguments connection  A function taking 2 inputs and combining them into a single output layers  The layers whose outputs are combined Inputs This layer behaves differently based on input type If input  x  is a tuple of length N or the input is  xs  with N  x s matching the number of  layers  then each layer receives a new input  x[i  combined with the previous output  y[i-1  using  connection  Thus  y1 y2 y3  PairwiseFusion(connection layer1 layer2 layer3)((x1 x2 x3  may be drawn as  or written as With just one input each layer receives the same  x  combined with the previous output Thus  y  PairwiseFusion(connection layers...)(x  obeys Returns A tuple of length N with the output of each fusion  y1   y2    yN  in the example above"},{"doctype":"documentation","id":"references/Flux.create_bias","title":"create_bias","text":"Return a bias parameter for a layer based on the value given to the constructor's keyword  bias=bias  bias  true  creates a trainable array of the given size of the same type as  weights  initialised to zero bias  false  returns  false  which is understood by AD to be non-differentiable bias::AbstractArray  uses the array provided provided it has the correct size It does not at present correct the  eltype  to match that of  weights "},{"doctype":"documentation","id":"references/Flux.Losses.ctc_alpha","title":"ctc_alpha","text":""},{"doctype":"documentation","id":"references/DataAugmentation.MapElem","title":"MapElem","text":"Applies  f  to every element in an  AbstractArrayItem "},{"doctype":"documentation","id":"references/Flux.NilNumber.Nil","title":"Nil","text":"Nil  is a singleton type with a single instance  nil  Unlike  Nothing  and  Missing  it is a number  Nil  Real  Number "},{"doctype":"documentation","id":"references/Flux.Bilinear","title":"Bilinear","text":"Creates a layer which is fully connected between two inputs and the output and otherwise similar to  Dense  Its output given vectors  x    y  is another vector  z  with for all  i ∈ 1:out  If  x  and  y  are matrices then each column of the output  z  B(x y  is of this form with  B  the Bilinear layer If the second input  y  is not given it is taken to be equal to  x  i.e  B(x  B(x x The two inputs may also be provided as a tuple  B((x y  B(x y  which is accepted as the input to a  Chain  If the two input sizes are the same  in1  in2  then you may write  Bilinear(in  out σ  The initialisation works as for  Dense  layer with  W  init(out in1 in2  By default the bias vector is  zeros(Float32 out  option  bias=false  will switch off trainable bias Either of these may be provided explicitly Examples"},{"doctype":"documentation","id":"references/FastVision.Models.pixelshuffle","title":"pixelshuffle","text":""},{"doctype":"documentation","id":"references/FluxTraining.CallbackRunner","title":"CallbackRunner","text":""},{"doctype":"documentation","id":"references/FastAI.Registries.registerrecipes","title":"registerrecipes","text":""},{"doctype":"documentation","id":"references/FluxTraining.Loss","title":"Loss","text":""},{"doctype":"documentation","id":"references/FastVision.RE_IMAGEFILE","title":"RE_IMAGEFILE","text":""},{"doctype":"documentation","id":"references/MLUtils.unbatch","title":"unbatch","text":"Reverse of the  batch  operation unstacking the last dimension of the array  x  See also  unstack  Examples"},{"doctype":"documentation","id":"references/FluxTraining.runepoch","title":"runepoch","text":"Run  epochfn  inside the context of an epoch Calls  epochfn(handle  where  handle(e  can be called to dispatch events Takes care of dispatching  EpochBegin  and  EpochEnd  events as well as handling  CancelEpochException s"},{"doctype":"documentation","id":"references/FastAI.Encoding","title":"Encoding","text":"Transformation of  Block s Can encode some  Block s  encode  and optionally decode them  decode  Interface encode(::E Context block::Block data  encodes  block  of  data  The default is to do nothing This should be overloaded for an encoding  E  concrete  Block  types and possibly a context decode(::E Context block::Block data  decodes  block  of  data  This should correspond as closely as possible to the inverse of  encode(::E   The default is to do nothing as not all encodings can be reversed This should be overloaded for an encoding  E  concrete  Block  types and possibly a context encodedblock(::E block::Block  block  returns the block that is obtained by encoding  block  with encoding  E  This needs to be constant for an instance of  E  so it cannot depend on the sample or on randomness The default is to return  nothing  meaning the same block is returned and not changed Encodings that return the same block but change the data e.g  ProjectiveTransforms  should return  block  decodedblock(::E block::Block  block  returns the block that is obtained by decoding  block  with encoding  E  This needs to be constant for an instance of  E  so it cannot depend on the sample or on randomness The default is to return  nothing  meaning the same block is returned and not changed encode!(buf E Context block::Block data  encodes  data  inplace decode!(buf E Context block::Block data  decodes  data  inplace"},{"doctype":"documentation","id":"references/DataAugmentation.FlipX","title":"FlipX","text":""},{"doctype":"documentation","id":"references/FastAI.predict","title":"predict","text":"Predict a  target  from  input  using  model  Optionally apply function  device  to  x  before passing to  model  and use  context  instead of the default context  Inference "},{"doctype":"documentation","id":"references/FastAI.lrfindtextplot","title":"lrfindtextplot","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.SkipException","title":"SkipException","text":""},{"doctype":"document","id":"documents/CONTRIBUTING.md","title":"Contributor guide for FastAI.jl","text":"Contributor guide for FastAI.jl First off thank you for considering contributing to FastAI.jl We welcome contributions and are happy to work with you FastAI.jl is part of the FluxML GitHub organization and follows the same guidelines laid out in  Flux.jl's CONTRIBUTING.md  That guide also includes a lot of tips for first-time contributors to open source Example contributions The list below just gives a few examples of welcome contributions but of course you can always open an issue to discuss other kinds of contributions Bug reports If you encounter an error and think it is due to a bug in FastAI.jl open an issue with a bug report as explained here  How to file a bug report Features There are many kinds of features that you can contribute to FastAI.jl like datasets models recipes and tasks In most cases it makes sense to open an issue first to discuss the scope and implementation of the feature Examples We're always happy about more usage examples for the documentation A good starting point for this can be an existing tutorial in another language like Python that you're recreating using FastAI.jl and the Julia ecosystem To get started with code or documentation contributions please see  DEVELOPING.md  for instructions on setting up a local dev environment and runnning the tests as well as the documentation"},{"doctype":"documentation","id":"references/Flux.ones32","title":"ones32","text":"Return an  Array{Float32  of the given  size "},{"doctype":"documentation","id":"references/Flux.params!","title":"params!","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.OAdam","title":"OAdam","text":"opt opt OAdam  Optimistic Adam is a variant of Adam adding an optimistic term suitable for adversarial training Parameters Learning rate  η  Amount by which gradients are discounted before updating the weights Decay of momentums  β::Tuple  Exponential decay for the first β1 and the second β2 momentum estimate Examples"},{"doctype":"documentation","id":"references/DataAugmentation.Project","title":"Project","text":""},{"doctype":"documentation","id":"references/Flux.Losses.crossentropy","title":"crossentropy","text":"Return the cross entropy between the given probability distributions calculated as Cross entropy is typically used as a loss in multi-class classification in which case the labels  y  are given in a one-hot format  dims  specifies the dimension or the dimensions containing the class probabilities The prediction  ŷ  is supposed to sum to one across  dims  as would be the case with the output of a  softmax  operation For numerical stability it is recommended to use  logitcrossentropy  rather than  softmax  followed by  crossentropy   Use  label_smoothing  to smooth the true labels as preprocessing before computing the loss See also  logitcrossentropy   binarycrossentropy   logitbinarycrossentropy  Example"},{"doctype":"documentation","id":"references/MLUtils.oversample","title":"oversample","text":"X rand Y X_bal Y_bal X Y size X_bal length Y_bal sum Y_bal sum Y_bal data DataFrame i data i data DataFrame nrow data Generate a re-balanced version of  data  by repeatedly sampling existing observations in such a way that every class will have at least  fraction  times the number observations of the largest class in  classes  This way all classes will have a minimum number of observations in the resulting data set relative to what largest class has in the given original  data  As an example by default i.e with  fraction  1  the resulting dataset will be near perfectly balanced On the other hand with  fraction  0.5  every class in the resulting data with have at least 50 as many observations as the largest class The  classes  input is an array with the same length as  numobs(data  The convenience parameter  shuffle  determines if the resulting data will be shuffled after its creation if it is not shuffled then all the repeated samples will be together at the end sorted by class Defaults to  true  For this function to work the type of  data  must implement  numobs  and  getobs  For example the following code allows  oversample  to work on a  DataFrame  Note that if  data  is a tuple and  classes  is not given then it will be assumed that the last element of the tuple contains the classes See  ObsView  for more information on data subsets See also  undersample "},{"doctype":"documentation","id":"references/MLUtils.rand_like","title":"rand_like","text":"Create an array with the given element type and size based upon the given source array  x  All element of the new array will be set to a random value The last two arguments are both optional defaulting to the given array's eltype and size The dimensions may be specified as an integer or as a tuple argument The default random number generator is used unless a custom one is passed in explicitly as the first argument See also  Base.rand  and  randn_like  Examples"},{"doctype":"documentation","id":"references/FastAI.OneHotTensor","title":"OneHotTensor","text":"A block representing a one-hot encoded N-dimensional array categorical variable For example a single categorical label is a  OneHotTensor{0 T  aliased to  OneHotTensor{T  Use the  OneHot  encoding to one-hot encode  Label s or  LabelMulti s"},{"doctype":"documentation","id":"references/FluxTraining.loadmodel","title":"loadmodel","text":"Loads a model that was saved to  path  using  FluxTraining savemodel "},{"doctype":"documentation","id":"references/Flux.Losses.mse","title":"mse","text":"Return the loss corresponding to mean square error See also  mae   msle   crossentropy  Example"},{"doctype":"documentation","id":"references/FastAI.TaskDataset","title":"TaskDataset","text":"Transform data container  data  of samples into a data container of encoded samples Maps  encodesample(task context sample  over the observations in  data  Also handles in-place  MLUtils.getobs  through  encodesample "},{"doctype":"documentation","id":"references/FastAI.discrlr_optimizer","title":"discrlr_optimizer","text":"Create an optimizer that discounts updates parameters which  ParamGrouper  puts into group  1  by  factor "},{"doctype":"documentation","id":"references/Flux.AlphaDropout","title":"AlphaDropout","text":"A dropout layer Used in  Self-Normalizing Neural Networks  The AlphaDropout layer ensures that mean and variance of activations remain the same as before Does nothing to the input once  testmode  is true Examples"},{"doctype":"documentation","id":"references/DataAugmentation.CenterResizeCrop","title":"CenterResizeCrop","text":""},{"doctype":"documentation","id":"references/FastAI.accuracy_thresh","title":"accuracy_thresh","text":""},{"doctype":"documentation","id":"references/FluxTraining.step!","title":"step!","text":"Run one step of training for  learner  on batch Behavior is customized through  phase  Extending This is a required method for custom  Phase s to implement To implement  step  it is recommended you make use of  runstep  to get begin and end events as well as proper handling of  CancelStepException s See the implementations of  TrainingPhase  and  ValidationPhase  for reference"},{"doctype":"documentation","id":"references/Flux.normalise","title":"normalise","text":"Normalise  x  to mean 0 and standard deviation 1 across the dimension(s given by  dims  Per default  dims  is the last dimension  ϵ  is a small additive factor added to the denominator for numerical stability"},{"doctype":"documentation","id":"references/FastVision.Image","title":"Image","text":"rand RGB rand rand Gray N0f8 rand rand RGB N0f8 Block  for an N-dimensional image  obs  is valid for  Image{N  if it is an N-dimensional array with color or number element type Examples Creating a block Example valid images The color channels if any are not counted as a dimension and represented through color types like  RGB{N0f8  You can create a random observation using  mockblock  To visualize a 2D-image observation use  showblock  This is supported for both the  ShowText  and the  ShowMakie  backend"},{"doctype":"documentation","id":"references/FastVision.Models.conv_final","title":"conv_final","text":""},{"doctype":"documentation","id":"references/Flux.applypairwisefusion","title":"applypairwisefusion","text":""},{"doctype":"documentation","id":"references/FastVision.augs_projection","title":"augs_projection","text":"Helper to create a set of projective transformations for image mask and keypoint data Similar to fastai's  aug_transforms  Keyword arguments flipx  true  Whether to perform a horizontal flip with probability  1/2  See  FlipX  flipy  false  Whether to perform a vertical flip with probability  1/2  See  FlipY  max_zoom  1.5  Maximum factor by which to zoom Set to  1  to disable See  Zoom  max_rotate  10  Maximum absolute degree by which to rotate Set to  0  to disable See  Rotate  max_warp  0.05  Intensity of corner warp Set to  0  to disable See  WarpAffine "},{"doctype":"documentation","id":"references/Flux.GlobalMeanPool","title":"GlobalMeanPool","text":"Global mean pooling layer Transforms w,h,c,b)-shaped input into 1,1,c,b)-shaped output by performing mean pooling on the complete w,h)-shaped feature maps"},{"doctype":"documentation","id":"references/FastVision.maskfromimage","title":"maskfromimage","text":""},{"doctype":"documentation","id":"references/FluxTraining.CheckDataIteratorValid","title":"CheckDataIteratorValid","text":""},{"doctype":"documentation","id":"references/Flux.OneHotArray","title":"OneHotArray","text":"These are constructed by  onehot  and  onehotbatch  Parameter  I  is the type of the underlying storage and  T  its eltype"},{"doctype":"documentation","id":"references/Flux.Optimise.runall","title":"runall","text":""},{"doctype":"documentation","id":"references/DataAugmentation.testprojective","title":"testprojective","text":"Test invariants of a  ProjectiveTransform  getprojection  is defined and given a constant  randstate  parameter always returns the same result It preserves the item type i.e  apply(tfm I  I  Applying it to multiple items with the same bounds results in the same bounds for all items"},{"doctype":"documentation","id":"references/DataAugmentation.ToEltype","title":"ToEltype","text":"tfm Float32 item rand Int tfm item Converts any  AbstractArrayItem  to an  AbstractArrayItem{N T  Supports  apply  Examples"},{"doctype":"document","id":"documents/CHANGELOG.md","title":"Changelog","text":"Changelog All notable changes to this project will be documented in this file The format is based on  Keep a Changelog  and this project adheres to  Semantic Versioning  v0.5 unreleased Changed BREAKING Now uses  MLUtils.jl  to create and load datasets and data containers Replaces dependencies MLDataPattern.jl LearnBase.jl and DataLoaders.jl Data containers must now implement the  Base.getindex  MLUtils.getobs  and  Base.length  MLUtils.numobs  interfaces Previously exported  MLDataPattern.datasubset  has been replaced by  MLUtils.ObsView Documentation has been updated appropriately BREAKING  FastAI.Vision  now lives in a separate package  FastVision  that holds all computer vision-related functionality BREAKING  FastAI.Tabular  now lives in a separate package  FastTabular  that holds all tabular data-related functionality Removed BREAKING  FastAI.Models  submodule  Models  submodule of domain libraries e.g  FastVision.Models  should now be used v0.4.3 2022/05/14 Added Feature registries let you find datasets data recipes and learning tasks for your projects It is now easier to search for functionality related to kinds of data and load it See the updated  discovery tutorial  added first support for text datasets adding the  Paragraph  block and  FastAI.Textual  submodule https://github.com/FluxML/FastAI.jl/pull/207 Removed the old APIs for registries have been removed and functionality for accessing them  finddatasets   loaddataset  has been deprecated See the updated docs for how to find functionality using the new feature registries v0.4.2 2022/04/30 Added Compatibility with FluxTraining.jl v0.3 https://github.com/FluxML/FastAI.jl/pull/223 v0.4.1 Added New documentation frontend based on Pollen.jl https://fluxml.ai/FastAI.jl/dev/i Now supports Flux.jl v0.13 https://github.com/FluxML/FastAI.jl/pull/202 Changed Now has ImageIO.jl as a dependency to ensure that fast jpg loading using JpegTurbo.jl is used v0.4.0 2022-03-19 Added Made block-based learning method more modular  SupervisedMethod  now supplants  BlockMethod    PR getencodings  and  getblocks  should now be used to get block information and encodings from a method See the new tutorial training a Variational Autoencoder See also the docstrings for  AbstractBlockTask  and  SupervisedTask Changed BREAKING all learning method names have been renamed to task i.e  method    task  and  Method    Task  Specifically these exported symbols are affected BlockMethod    BlockTask  describemethod    describetask  methodmodel    taskmodel  methoddataset    taskdataset  methoddataloaders    taskdataloaders  methodlossfn    tasklossfn  findlearningmethods    findlearningtasks  methodlearner    tasklearner  savemethodmodel    savetaskmodel  loadmethodmodel    loadtaskmodel BlockMethod  now deprecated in favor of  SupervisedMethod INTERNAL domain-specific functionality has moved to submodules  FastAI.Vision  computer vision and  FastAI.Tabular  tabular data Exports of  FastAI  are not affected INTERNAL test suite now runs on InlineTest.jl Removed v0.3.0 2021/12/11 Added A new API for visualizing data See  this issue  for motivation This includes High-level functions for visualizing data related to a learning method  showsample    showsamples   showencodedsample   showencodedsamples   showbatch   showprediction   showpredictions   showoutput   showoutputs   showoutputbatch Support for multiple backends including a new text-based show backend that you can use to visualize data in a non-graphical environment This is also the default unless  Makie  is imported Functions for showing blocks directly  showblock   showblocks Interfaces for extension  ShowBackend   showblock   showblocks Removed The old visualization API incl all its  plot  methods  plotbatch   plotsample   plotsamples   plotpredictions 0.2.0 2021/09/21 Added High-level API FasterAI dataset recipes learning method helpers Find datasets and learning methods based on  Block s  finddatasets   findlearningmethods loaddataset  for quickly loading data containers from configured recipes Data container recipes  DatasetRecipe   loadrecipe  Documentation setions for FasterAI interfaces Discovery Blocks and encodings New interfaces blockbackbone  creates a default backbone for an input block Support for tabular data along with recipes and learning methods Tabular classification tutorial TabularPreprocessing   TableRow   TableDataset   TabularClassificiationSingle   TabularRegression Changed Documentation sections to reference FasterAI interfaces README Introduction Data containers Combined how-tos on training  into a single page Breaking changes to  methodlearner  now accepts  callbacks  as kwarg validdata  no longer keyword model  and  backbone  now kwargs  isbackbone  removed if neither  backbone  or  model  are given uses  blockbackbone  for default backbone see updated docstring for details"},{"doctype":"document","id":"documents/docs/notebooks/siamese.ipynb","title":"Siamese image similarity","text":"Pkg Pkg add CairoMakie CairoMakie activate! type path joinpath load files path filterfn files image files Images transform_image image sz image_resized imresize convert RGB N0f8 image sz sz a permuteddimsview channelview image_resized transform_image files summary label_func path match path label_func files labels map label_func files length unique labels data files file file label_func file data image label data Random idxs shuffle length files cut round Int length idxs trainidxs valididxs idxs cut idxs cut end trainfiles validfiles files trainidxs files valididxs summary trainfiles validfiles SiamesePairs labels same other valid SiamesePairs labels valid ulabels unique labels same Dict label i i l enumerate labels l label label ulabels other Dict label i i l enumerate labels l label label ulabels SiamesePairs labels same other valid si SiamesePairs idx Int rng si valid MersenneTwister idx Random GLOBAL_RNG rand rng idx rand rng si same si labels idx idx rand rng si other si labels idx si SiamesePairs length si labels siamesedata files valid transformfn identity labels map label_func files si SiamesePairs labels valid valid si obs i j same obs image1 transformfn files i image2 transformfn files j image1 image2 same traindata siamesedata trainfiles transformfn transform_image validdata siamesedata validfiles transformfn transform_image valid summary traindata traindl traindata validdl validdata task buffered sharestate buffered task traindata siamesedata trainfiles valid validdata siamesedata validfiles valid traindl valdl traindata validdata task traindl sample task sample traindata validdl sample task sample validdata SiameseModel E H encoder E head H SiameseModel m SiameseModel xs1 xs2 m head cat m encoder xs1 m encoder xs2 dims Metalhead encoder encoder Metalhead ResNet50 pretrain layers end h w ch b encoder head ch model SiameseModel encoder head xs ys first traindl ŷs model xs lossfn optimizer callbacks learner model traindl valdl optimizer lossfn callbacks plot learner encoder Metalhead ResNet50 pretrain layers end h w ch b encoder head ch model SiameseModel encoder head learner model traindl valdl lossfn callbacks learner Siamese image similarity This tutorial is adapted from  this tutorial  in fast.ai's documentation It tries to stay as close to the original as possible but diverges where the APIs differ In this tutorial we will see how to deal with a new type of task using the middle layer of the fastai library The example we will use is a Siamese network that takes two images and determine if they are of the same class or not In particular we will see how to quickly get DataLoaders from a standard PyTorch Datasets how to adapt this in a Transform to get some of the show features of fastai how to add some new behavior to show_batch/show_results for a custom task how to write a custom DataBlock how to create your own model from a pretrained model how to pass along a custom splitter to Learner to take advantage of transfer learning UPDATE Setup Since we'll be implementing some image operations manually we'll add the  Images  package Preparing the data To make our data ready for training a model we need to create data iterators for training and validation for example using  DataLoader  Usually the first step is to create a  data container  that is then wrapped inside a  DataLoader  Unlike in fast.ai FastAI.jl separates the loading part from the encoding part In this case loading means getting pairs of images and encoding includes preprocessing and augmenting them We'll first create a data container that just loads pairs of images and later show how to apply transforms on top of that Using the low-level API First we'll use  datasets  to download and untar the dataset and then find all image files We can open the first image and have a look at it Note that array indices start at 1 in Julia Let's wrap all the standard preprocessing resize and conversion to tensor and reordering of the channels in one helper function Note some differences to Python here images by default are 2D-arrays of pixels so we need to use  channelview  to get a 3D array with the color dimension expanded pixel values are assumed to be between 0 and 1 so we do not need to divide them by 255 For flexibility we also separate the loading  loadfile  from the transformations applied to the image We can see the label of our image is in the filename before the last  and some number We can then use a regex expression to create a label function Now let's gather all unique labels We could now use  mapobs  to create a data container from our list of files It applies a function like loading an image lazily and we can get single observations using  getobs  and the number of observtions with  numobs  It is the same as a  torch.utils.data.Dataset  For example the following example creates a data container with tuples of an image and a category that could be used for image classification To create our Siamese datasets however we will need to create tuples of images for inputs and the target will be  true  if the images are of the same class  false  otherwise First we'll shuffle the files and split them into a training and a validation set Let's create a custom data container that returns pairs of indices and a Boolean indicating whether the label is the same Half of the pairs will have the same label and half will not Additionally during training the other image will be chosen randomly while for the validation the pairs will always be the same While you can get far with basic data containers like  loadfolderdata  and transformations like  mapobs  and  filterobs  sometimes it's simplest to create a custom data container You just need to implement  getobs  and  numobs  for your type similar to how you would implement  getindex  and  len  for a PyTorch  Dataset  We can combine this data container that gives us pairs of indices with the files and map the loading and preprocessing functions over it to get a data container that is ready to be passed to a  DataLoader  We can see that each observation consists of two images and a Boolean To use the above data containers in training we can simply pass them to a  DataLoader  Next let's look at how we can extend this example to make use FastAI.jl's data augmentation visualize our data and more Using the Data Block API This is where FastAI.jl's API diverges a bit from fast.ai's Note how above we made sure to separate the data container creation and loading from disk from the preprocessing that is applied to every observation In FastAI.jl the preprocessing or encoding is implemented through  a learning task  Learning tasks contain any configuration and beside data processing have extensible functions for visualizations and model building One advantage of this separation between loading and encoding is that the data container can easily be swapped out as long as it has observations suitable for the learning task in this case a tuple of two images and a Boolean It also makes it easy to  export  models and all the necessary configuration The easiest way to create learning tasks is using the data block API which should suit the very most of all use cases It is also possible to directly  implement the lower-level  LearningTask  interface  The best way to understand it is to use it so let's build a learning task for Siamese image similarity We specify the kinds of input and target data as  blocks  Here we have two 2D images as input  Image{2 Image{2  and a binary label  Label([true false  as output We also pass in a tuple of encodings that describe how the data is transformed before being fed to a model Here  ProjectiveTransforms  resizes the images to the same size  ImagePreprocessing  converts the images to the right format and  OneHot  one-hot encodes the labels We can get a better understanding of the representations the data goes through using  describetask  We can reuse all the code above for creating the data container we just omit the preprocessing function  taskdataloaders  constructs training and validation data loaders from data containers by mapping the task encoding over the data containers The above is equivalent to lazily mapping the encoding over the data containers and creating a  DataLoader  from them By separating the data container preparation from the task-specific data preprocessing and augmentation we were able to completely reuse the data container creation and quickly add FastAI.jl's data augmentation With the data iterators ready for training we still need a model to train For the Siamese similarity setting a common model architecture is a encoder/feature extractor that is applied to both images and a head that transforms the concatenated image features resulting in a similar/not similar categorical output Flux.jl makes it easy to create models in a similar fashion to PyTorch modules The forward pass is implemented by overloading the call operator and the backward pass is automatically generated We'll use a XResNet model for the encoder and the same head that is used for classification models Using  Flux.outputsize  we can check how many channels the encoder outputs without having to evaluate the model We need to double this feature count since the head gets the concatenated features from two images Let's test the model works on a batch of training data We'll use categorical crossentropy for logits as a loss function and ADAM as optimizer Now we can create a  Learner  and start training following the usual process"},{"doctype":"documentation","id":"references/Flux.sparse_init","title":"sparse_init","text":"Return a  Matrix{Float32  of size  rows cols  where each column contains a fixed fraction of zero elements given by  sparsity  Non-zero elements are normally distributed with a mean of zero and standard deviation  std  This method is described in 1 Examples References 1 Martens J Deep learning via Hessian-free optimization  Proceedings of the 27th International Conference on International Conference on Machine Learning  2010"},{"doctype":"documentation","id":"references/FastVision.colorchannels","title":"colorchannels","text":""},{"doctype":"documentation","id":"references/FluxTraining.ConflictResolution","title":"ConflictResolution","text":"A conflict resolution strategy for resolving write/write conflicts of two callbacks See  resolveconflict "},{"doctype":"documentation","id":"references/FluxTraining.replacecallback!","title":"replacecallback!","text":"Replace existing callback of type  C  on learner with  callback  Return the replaced callback If  learner  doesn't have a callback of type  C  add  callback  and return  nothing "},{"doctype":"documentation","id":"references/FastAI.showblockinterpretable","title":"showblockinterpretable","text":"encodings block x block encodings block x encodings block x Decode  block  successively by applying  encodings  until a block is gotten that can be shown by  backend  Useful to visualize encoded data that is not directly interpretable for example an  Image{2  representing an encoded  Image  Examples"},{"doctype":"documentation","id":"references/DataAugmentation.showitems","title":"showitems","text":"Visualize  items "},{"doctype":"documentation","id":"references/FastAI.showblocks!","title":"showblocks!","text":"data blocks loaddataset samples data i i range blocks samples Show a vector of observations  obss  of the same  block  type Examples Extending This is used for showing batches of observations unlike the  Tuple  variant of  showblock  which assumes an observation consists of multiple blocks Usually a  ShowBackend  will show an observation in one row with  showblock  and  showblocks  will show multiple rows"},{"doctype":"documentation","id":"references/DataAugmentation.copyitemdata!","title":"copyitemdata!","text":""},{"doctype":"documentation","id":"references/Flux.Losses","title":"Losses","text":""},{"doctype":"documentation","id":"references/MLUtils.groupobs","title":"groupobs","text":"data datas > data length datas Split data container data  data  into different data containers grouping observations by  f(obs "},{"doctype":"document","id":"documents/docs/howto/logtensorboard.md","title":"How to log to TensorBoard","text":"dir mktempdir backend dir metricscb backend hparamscb backend callbacks metricscb hparamscb data _ task _ learner task data callbacks callbacks learner How to log to TensorBoard TensorBoard is a format and viewer for logs of model training and can be used to inspect and compare the results of training runs We can log step and epoch metrics hyperparameters and even visualizations to TensorBoard using FluxTraining.jl's logging callbacks Generating logs To use logging callbacks we need to pass a log backend here  TensorBoardBackend  This design allows flexibly supporting other logging backends like Weights and Biases or neptune.ai in the future Then we create callbacks for logging metrics and hyperparameters to that backend Like any other callbacks these can then be passed to a  Learner  along with other callbacks and we can start training By using the  Metrics  callback we can log metrics other than the loss Inspecting logs To inspect the logs you will have to install the  tensorboard  command-line using  pip  you can access the command-line in the Julia REPL by pressing    After this one-time installation you can run it by pointing it to the log directory created above This should give you an URL that you can open in a browser which should look like this Note that you can also open TensorBoard and it will update as the training progresses"},{"doctype":"documentation","id":"references/Flux.gpu","title":"gpu","text":"Moves  m  to the current GPU device if available It is a no-op otherwise See the  CUDA.jl docs  to help identify the current device This works for functions and any struct marked with  functor "},{"doctype":"documentation","id":"references/FluxTraining.log_to","title":"log_to","text":"Log  loggable  to  backend  with  group  to index  i  loggable  is any  Loggables.Loggable group  can be a  String  or a tuple of  String s implying some grouping which can be used by a supporting backend i  is a step counter and unique for every group"},{"doctype":"documentation","id":"references/Flux.Recur","title":"Recur","text":"Recur  takes a recurrent cell and makes it stateful managing the hidden state in the background  cell  should be a model of the form For example here's a recurrent network that keeps a running total of its inputs Examples Folding over a 3d Array of dimensions  features batch time  is also supported"},{"doctype":"documentation","id":"references/Flux.dropout_mask","title":"dropout_mask","text":""},{"doctype":"documentation","id":"references/FluxTraining.getindexperm","title":"getindexperm","text":""},{"doctype":"documentation","id":"references/FluxTraining.Callback","title":"Callback","text":"Supertype of all callbacks Callbacks add custom functionality to the training loop by hooking into different  Events.Event s Any  Callback  can be used by passing it to  Learner  See  subtypes(FluxTraining.Callback  for implementations Extending See  Custom callbacks  for a less succinct tutorial format Create a  struct MyCallback  that subtypes  FluxTraining.Callback  Add event handlers by implementing methods for  on event phase callback learner  Methods should always dispatch on your callback and may dispatch on specific  Phases.Phase s and  Events.Event s For example to implement an event handler that runs at the end of every step during training  on(::StepEnd AbstractTrainingPhase MyCallback learner  Define what state the callback accesses and/or modifies by implementing  stateaccess MyCallback  While  learner  is always passed as an argument to  on  event handlers by default a callback can not read or write to its fields See  stateaccess  for more detail If a callback needs to write some state that other callbacks should be able to access it can store it in  learner.cbstate  if you add a permission in  stateaccess  If the callback needs some one-time initialization you can implement  init  which will be run at least once before any step is run"},{"doctype":"documentation","id":"references/FastAI.showpredictions","title":"showpredictions","text":"Show predictions  pred  If  samples  are also given show them next to the prediction"},{"doctype":"documentation","id":"references/FastAI.Datasets.DatasetLoader","title":"DatasetLoader","text":"A  DatasetLoader  defines how a dataset can made available and loaded See  DataDepLoader  as an example A  DatasetLoader  has to implement the following functions loaddata makeavailable isavailable"},{"doctype":"documentation","id":"references/FastAI.decay_optim","title":"decay_optim","text":"Add  WeightDecay  with value  wd  to optimizer  optim "},{"doctype":"document","id":"documents/docs/fastai_api_comparison.md","title":"fastai API comparison","text":"task task learner task data learner learner learner res learner plot res fastai API comparison FastAI.jl is in many ways similar to the original Python  fastai  but also has its differences This reference goes through all the sections in the  fastai A Layered API for Deep Learning  paper and comments what the interfaces for the same functionality in FastAI.jl are and where they differ or functionality is still missing Applications FastAI.jl's own data block API makes it possible to derive every part of a high-level interface with a unified API across tasks Instead it suffices to create a learning task and based on the blocks and encodings specified the proper model builder loss function and visualizations are implemented see below For a high-level API a complete  Learner  can be constructed using  tasklearner  without much boilerplate There are some helper functions for  creating these learning tasks for example  ImageClassificationSingle  and  ImageSegmentation  FastAI.jl additionally has a unified API for registering and discovering functionality across applications also based on the data block abstraction   datasets  and  datarecipes  let you quickly load common datasets matching some data modality and  learningtasks  lets you find learning task helpers for common tasks See  the discovery tutorial  for more info Vision Computer vision is well-supported in FastAI.jl with different tasks and optimized data pipelines for N-dimensional images masks and keypoints See the tutorial section for many examples Tabular FastAI.jl also has support for tabular data Deployment Through FastAI.jl's  LearningTask  interface the data processing logic is decoupled from the dataset creation and training and can be easily serialized and loaded to make predictions See the tutorial on  saving and loading models  There is no integration yet for text and collaborative filtering applications High-level API High-level API foundations FastAI.jl also has a data block API but it differs from fastai's in a number of ways In the Julia package it only handles the data encoding and decoding part and doesn't concern itself with creating datasets For dataset loading see the  data container API  As mentioned above the high-level application-specific logic is also derived from the data block API To use it you need to specify a tuple of input and target blocks as well as a tuple of encodings that are applied to the data The encodings  are invertible data-specific data processing steps which correspond to  fastai.Transform s As in fastai dispatch is used to transform applicable data and pass other data through unchanged Unlike in fastai there are no default steps associated with a block allowing greater flexibility We can create a  BlockTask  similar to  fastai.DataBlock  and get information about the representations the data goes through From this short definition many things can be derived data encoding model output decoding how to create a model from a backbone the loss function to use how to visualize samples and predictions Together with a  data container   data  we can quickly create a  Learner  using  tasklearner  which like in fastai handles the training for us There are no application-specific  Learner  constructors like  cnn_learner  or  unet_learner  in FastAI.jl High-level training protocols like the  one-cycle learning rate schedule   fine-tuning  and the  learning rate finder  are then available to us Incrementally adapting PyTorch code Since it is a Julia package FastAI.jl is not written on top of PyTorch but a Julia library for deep learning  Flux.jl  In any case the point of this section is to note that the abstractions in fastai are decoupled and existing projects can easily be reused This is also the case for FastAI.jl as it is built on top of several decoupled libraries Many of these were built specifically for FastAI.jl but they are unaware of each other and useful in their own right Flux.jl  provides models optimizers and loss functions fulfilling a similar role to PyTorch MLUtils.jl  gives you tools for building and transforming data containers Also it takes care of efficient parallelized iteration of data containers DataAugmentation.jl  takes care of the lower levels of high-performance composable data augmentations FluxTraining.jl  contributes a highly extensible training loop with 2-way callbacks If that seems like a lot don't worry If you've installed FastAI.jl the functionality of most of these packages is reexported and you don't have to install any of them explicitly Consistency across domains While computer vision is the only domain with mature support for now the abstractions underlying FastAI.jl are carefully crafted to ensure that learning tasks for different domains can be created using the same set of interfaces This shows in that there's no need for application-specific functionality above the data block API Mid-level APIs Learner The  Learner  is very similar to fastai's It takes a model any parameterized differentiable function like a neural network or even  a trebuchet simulator training and validation data iterators these can be  DataLoader s which paralellize data loading but any iterator over batches can be used optimizer loss function Two-way callbacks The training loop also supports two-way callbacks See the  FluxTraining.jl docs  for a list of all available callbacks While supporting all the functionality of fastai's callbacks and training loop it also provides  an extensible training loop API  that makes it straightforward to integrate custom training steps with the available callbacks As a result different training steps for problems other than standard supervised training can make use of existing callbacks  without the need to handle control flow through callbacks Additionally callbacks have an additional level of safety by being required to declare what state they access and modify With a little more effort up-front this guarantees correct ordering of callback execution through  a dependency graph  In the future this will also make it possible to automatically run callbacks in parallel and asynchronously to reduce overhead by long-running callbacks like costly metric calculations and logging over the network Encodings and blocks In the paper this subsection is in the low-level section named Transforms and Pipelines but I'm putting it here since it is the core of FastAI.jl's data block API FastAI.jl provides  Encoding s and  Block s which correspond to fastai's  Transform s and  Block s Encodings implement an  encode  and optionally  decode  function that describes how data corresponding to some blocks is transformed and how that transformation can be inverted There is also support for stateful encodings like  ProjectiveTransforms  which need to use the same random state to augment every data point Additionally encodings describe what kind of block data is returned from encoding allowing inspection of the whole data pipeline The  Block s are used to dispatch in the  encode  function to implement block-specific transformations If no  encode  task is implemented for a pair of encoding and block the default is to pass the data through unchanged like in fastai The  Block s also allow implementing task-specific functionality blocklossfn  takes a prediction and encoded target block to determine a good loss function to use For example for image classification we want to compare two one-hot encoded labels and hence define  blocklossfn(::OneHotTensor{0 OneHotTensor{0  logitcrossentropy  blockmodel  constructs a model from a backbone that maps an input block to an output block For example for image segmentation we have  ImageTensor{N  as the input block and  OneHotTensor{N  one-hot encoded N-dimensional masks as output so  blockmodel  turns the backbone into a U-Net showblock  defines how to visualize a block of data Generic optimizer FastAI.jl uses the optimizers from Flux.jl which provides a similarly  composable API for optimzers  Generalized metric API Metrics are handled by the  Metrics  callback which takes in reducing metric functions or  FluxTraining.AbstractMetric s which have a similar API to fastai's fastai.data.external FastAI.jl makes all the same datasets available in  fastai.data.external  available See  datasets  for a list of all datasets that can be downloaded funcs_kwargs and DataLoader fastai.data.core In FastAI.jl you are not restricted to a specific type of data iterator and can pass any iterator over batches to  Learner  In cases where performance is important  DataLoader  can speed up data iteration by loading and batching samples in parallel on background threads All transformations of data happen through the data container interface which requires a type to implement  Base.getindex  MLUtils.getobs  and  Base.length  MLUtils.numobs  similar to PyTorch's  torch.utils.data.Dataset  Data containers are then transformed into other data containers Some examples mapobs f data  lazily maps a function  f  of over  data  such that  getobs(mapobs(f data idx  f(getobs(data idx  For example  mapobs(loadfile files  turns a vector of image files into a data container of images DataLoader(data batchsize  is a wrapper around  BatchView  which turns a data container of samples into one of collated batches and  eachobsparallel  which creates a parallel buffered iterator over the observations here batches in the resulting container groupobs f data  splits a container into groups using a grouping function  f  For example  groupobs(grandparentname files  creates training splits for files where the grandparent folder indicates the split MLUtils.ObsView data idxs  lazily takes a subset of the observations in  data  For more information see the  data container tutorial  and the  MLUtils.jl docs  At a higher level there are also convenience functions like  loadfolderdata  to create data containers Layers and architectures Flux.jl already does a better job at functionally creating model architectures than PyTorch so FastAI.jl makes use of its layers For example  Flux.SkipConnection   corresponds to fastai's  MergeLayer  The  FastAI.Models  submodule currently provides some high-level architectures like  xresnet18  and a U-Net builder  UNetDynamic  that can create U-Nets from  any  convolutional feature extractor The  optional dependency   Metalhead.jl  also provides common pretrained vision models Low-level APIs Due to the nature of the Julia language and its design around multiple dispatch packages tend to compose really well so it was not necessary to reimplement or provide a unified API for low-level operations We'll comment on the libraries that we were able to use PyTorch foundations Unlike Python Julia has native support for N-dimensional regular arrays As such there is a standard interface for arrays and libraries don't need to implement their own Consider that every deep learning framework in Python implements their own CPU and GPU arrays which is part of the reason they are  frameworks  not  libraries  with the latter being vastly preferable Julia's standard libraries implements the standard CPU  Array  type GPU arrays are implemented through  CUDA.jl   CuArray  type with unified support for GPU vendors other than nvidia in the works As a result Flux.jl the deep learning library of choice for FastAI.jl does not need to reimplement their own CPU and GPU array versions This kind of composability in general largely benefits what can be accomplished in Julia Some other libraries which are used under the hood for image processing the  Images.jl  ecosystem of packages is used for reading and processing tabular data  DataFrames.jl  and  Tables.jl  for plotting  Makie.jl  Type dispatch Multiple dispatch already is a core feature of the Julia language hence the extensible interfaces in FastAI.jl are built around it and are natural fit for the language Object-oriented semantic tensors As mentioned above Julia has great support for arrays with extra functionality available to packages that provide wrapper arrays like  NamedDims.jl  which should generally  just work  with every part of the library Hence there is no need for an addtional API that unifies separate packages which in turn makes FastAI.jl more composable with other packages In encodings the array types are used for dispatch only where an especially performant implementation is possible and the block information is used for dispatching the semantics of the encoding GPU-accelerated augmentation FastAI.jl does not support GPU-accelerated augmentation yet Please open an issue if you run into a situation where data processing  becomes the bottleneck  and we'll prioritize this The affine transformations implemented in DataAugmentation.jl and used in FastAI.jl are properly composed to ensure high quality results They are also optimized for speed and memory usage with complete support for inplace transformations Convenience functionality Much of the convenience provided by fastai is not required in Julia delegates  Due to the absence of deep class hierarchies keyword arguments are seldom passed around the only instance where this happens in FastAI.jl is  tasklearner  patch  since Julia is built around multiple dispatch not classes you just implement the task for a type no patching needed L  due to first-class array support such a wrapper list container isn't needed nbdev There is no  nbdev equivalent in Julia at the moment That said this documentation is generated by a document creation package  Pollen.jl  that could be extended to support such a workflow It already has support for different source and output formats like Jupyter notebooks code execution and is built for interactive work with incremental rebuilds Hopefully this page has given you some context for how FastAI.jl relates to fastai and how to map concepts between the two You are encouraged to go through the tutorials to see the design decisions made in practice"},{"doctype":"documentation","id":"references/FastAI.TestWrapper","title":"TestWrapper","text":""},{"doctype":"documentation","id":"references/Flux.ones","title":"ones","text":""},{"doctype":"documentation","id":"references/Flux.Losses.huber_loss","title":"huber_loss","text":"Return the mean of the  Huber loss  given the prediction  ŷ  and true values  y "},{"doctype":"documentation","id":"references/Flux.LSTMCell","title":"LSTMCell","text":""},{"doctype":"documentation","id":"references/FluxTraining.protect","title":"protect","text":""},{"doctype":"documentation","id":"references/FastVision.loadmask","title":"loadmask","text":"Load a segmentation mask from an image file Returns an efficiently stored array of type  eltype(classes "},{"doctype":"documentation","id":"references/Flux.onehot","title":"onehot","text":"Return a  OneHotVector  which is roughly a sparse representation of  x  labels  Instead of storing say  Vector{Bool  it stores the index of the first occurrence of  x  in  labels  If  x  is not found in labels then it either returns  onehot(default labels  or gives an error if no default is given See also  onehotbatch  to apply this to many  x s and  onecold  to reverse either of these as well as to generalise  argmax  Examples"},{"doctype":"documentation","id":"references/Flux.Losses.label_smoothing","title":"label_smoothing","text":"Returns smoothed labels meaning the confidence on label values are relaxed When  y  is given as one-hot vector or batch of one-hot its calculated as when  y  is given as a number or batch of numbers for binary classification its calculated as in which case the labels are squeezed towards  0.5  α is a number in interval 0 1 called the smoothing factor Higher the value of α larger the smoothing of  y  dims  denotes the one-hot dimension unless  dims=0  which denotes the application of label smoothing to binary distributions encoded in a single number Example"},{"doctype":"documentation","id":"references/Flux.Dropout","title":"Dropout","text":"Dropout layer While training for each input this layer either sets that input to  0  with probability  p  or scales it by  1  1  p  To apply dropout along certain dimension(s specify the  dims  keyword e.g  Dropout(p dims  3  will randomly zero out entire channels on WHCN input also called 2D dropout This is used as a regularisation i.e it reduces overfitting during training In the forward pass this layer applies the  Flux.dropout  function See that for more details Specify  rng  to use a custom RNG instead of the default Custom RNGs are only supported on the CPU Does nothing to the input once  Flux.testmode  is  true  Examples"},{"doctype":"documentation","id":"references/MLUtils.BatchView","title":"BatchView","text":"X Y A X typeof A AbstractVector eltype A SubArray Float64 length A size A x X typeof x SubArray Float64 x x y X Y partial typeof x SubArray Float64 typeof y SubArray String x y x y rand collate partial size x size y x y X Y typeof x SubArray Float64 typeof y SubArray String Create a view of the given  data  that represents it as a vector of batches Each batch will contain an equal amount of observations in them The batch-size can be specified using the  parameter  batchsize  In the case that the size of the dataset is not dividable by the specified  batchsize  the remaining observations will be ignored if  partial=false  If   partial=true  instead the last batch-size can be slightly smaller Note that any data access is delayed until  getindex  is called If used as an iterator the object will iterate over the dataset once effectively denoting an epoch For  BatchView  to work on some data structure the type of the given variable  data  must implement the data container interface See  ObsView  for more info Arguments data   The object describing the dataset Can be of any type as long as it implements  getobs  and  numobs  see Details for more information batchsize   The batch-size of each batch It is the number of observations that each batch must contain except possibly for the last one partial   If  partial=false  and the number of observations is not divisible by the batch-size then the last mini-batch is dropped collate  Batching behavior If  nothing  default a batch is  getobs(data indices  If  false  each batch is  getobs(data i for i in indices  When  true  applies  batch  to the vector of observations in a batch recursively collating arrays in the last dimensions See  batch  for more information and examples Examples"},{"doctype":"documentation","id":"references/FastTabular.TableDatasetRecipe","title":"TableDatasetRecipe","text":"Recipe for loading a  TableDataset   tablefile  is the path of a file that can be read as a table  catcols  and  contcols  indicate the categorical and continuous columns of the table If they are not given they are detected automatically"},{"doctype":"documentation","id":"references/Flux.MeanPool","title":"MeanPool","text":"Mean pooling layer averaging all pixels in a block of size  window  Expects as input an array with  ndims(x  N+2  i.e channel and batch dimensions after the  N  feature dimensions where  N  length(window  By default the window size is also the stride in each dimension The keyword  pad  accepts the same options as for the  Conv  layer including  SamePad  See also  Conv   MaxPool   AdaptiveMeanPool  Examples"},{"doctype":"documentation","id":"references/DataAugmentation.AdjustBrightness","title":"AdjustBrightness","text":"TestImages item testimage tfm titems tfm item _ titems ncol npad Adjust the brightness of an image by a factor chosen uniformly from  f ∈ 1-δ 1+δ  by multiplying each color channel by  f  You can also pass any  Distributions.Sampleable  from which the factor is selected Example"},{"doctype":"documentation","id":"references/FastAI.getgroup","title":"getgroup","text":""},{"doctype":"documentation","id":"references/DataAugmentation.ItemWrapper","title":"ItemWrapper","text":""},{"doctype":"documentation","id":"references/FastVision.ImageSegmentation","title":"ImageSegmentation","text":"Learning task for image segmentation Images are resized to  size  and a class is predicted for every pixel Keyword arguments computestats  false  Whether to compute image statistics on dataset  data  or use default ImageNet stats aug_projections   DataAugmentation.Identity  augmentation to apply during  ProjectiveTransforms  resizing and cropping aug_image   DataAugmentation.Identity  pixel-level augmentation to apply during  ImagePreprocessing C  RGB{N0f8  Color type images are converted to before further processing Use  Gray{N0f8  for grayscale images"},{"doctype":"documentation","id":"references/Flux.Losses.tversky_loss","title":"tversky_loss","text":"Return the  Tversky loss  Used with imbalanced data to give more weight to false negatives Larger β weigh recall more than precision by placing more emphasis on false negatives Calculated as 1  sum(|y  ŷ  1  sum(y  ŷ  β*(1  y  ŷ  1  β y   1  ŷ  1"},{"doctype":"documentation","id":"references/Flux.convfilter","title":"convfilter","text":"Constructs a standard convolutional weight matrix with given  filter  and channels from  in  to  out  Accepts the keyword  init  default  glorot_uniform  to control the sampling distribution This is internally used by the  Conv  layer"},{"doctype":"documentation","id":"references/Flux.conv_reshape_bias","title":"conv_reshape_bias","text":""},{"doctype":"documentation","id":"references/FluxTraining.FrequencyThrottle","title":"FrequencyThrottle","text":""},{"doctype":"document","id":"documents/docs/glossary.md","title":"Glossary","text":"Glossary Terms commonly used in  FastAI.jl  Type abbreviations In many docstrings generic types are abbreviated with the following symbols Many of these refer to a learning task the context should make clear which task is meant DC{T  A  data container  of type T meaning a type that implements the data container interface  getindex  getobs  and  length  numobs  where  getobs  DC{T Int  Int  that is each observation is of type  T  I  Type of the unprocessed input in the context of a task T  Type of the target variable X  Type of the processed input This is fed into a  model  though it may be batched beforehand  Xs  represents a batch of processed inputs Y  Type of the model output  Ys  represents a batch of model outputs model  M  A learnable mapping  M  X  Y  or  M  Xs  Ys  It predicts an encoded target from an encoded input The learnable part of a learning task Some examples of these in use LearningTask  is a concrete approach to learning to predict  T  from  I  by using the encoded representations  X  and  Y  encodeinput  task context I  X  encodes an input so that a prediction can be made by a model A task dataset is a  DC{(I T  i.e a data container where each observation is a 2-tuple of an input and a target Definitions Data container A data structure that is used to load a number of data observations separately and lazily It defines how many observations it holds with  numobs  and how to load a single observation with  getobs  Learning task An instance of  DLPipelines.LearningTask  A concrete approach to solving a learning task Encapsulates the logic and configuration for processing data to train a model and make predictions See the DLPipelines.jl documentation for more information Task data container  dataset DC{(I T  A data container containing pairs of inputs and targets Used in  taskdataset   taskdataloaders  and  evaluate "},{"doctype":"documentation","id":"references/FluxTraining.SanityCheck","title":"SanityCheck","text":"Callback that runs sanity  Check s when the  Learner  is initialized If  usedefault  is  true  it will run all checks in FluxTraining.CHECKS in addition to the ones you pass in"},{"doctype":"documentation","id":"references/FastAI.checkblock","title":"checkblock","text":"rand RGB rand RGB Check whether  obs  is compatible with  block  returning a  Bool  Examples Extending An implementation of  checkblock  should be as specific as possible The default method returns  false  so you only need to implement methods for valid types and return  true "},{"doctype":"documentation","id":"references/DataAugmentation.FromCenter","title":"FromCenter","text":""},{"doctype":"documentation","id":"references/DataAugmentation.Bounds","title":"Bounds","text":""},{"doctype":"documentation","id":"references/FastAI.encodestate","title":"encodestate","text":""},{"doctype":"documentation","id":"references/FluxTraining.ToGPU","title":"ToGPU","text":"Callback that moves model and batch data to the GPU during training Convenience for  ToDevice Flux.gpu "},{"doctype":"documentation","id":"references/MLUtils.Loader","title":"Loader","text":"Create a threaded iterator that iterates over  f(arg for arg in args  using threads that prefill a channel of length  channelsize  Note results may not be returned in the correct order depending on  executor "},{"doctype":"documentation","id":"references/FastAI.defaultgrouper","title":"defaultgrouper","text":""},{"doctype":"documentation","id":"references/FluxTraining.on","title":"on","text":"Handle  event  with  Callback   callback  By default this event handler does nothing for a callback To see events which an  AbstractCallback  handles use Extending You can add event handlers to  Callback s by implementing a method for  on  See also  Callback  and  custom callbacks  A method of  on  should  always  dispatch on the callback type i.e  on(event phase cb::MyCallback learner  It may also dispatch on specific  Event s and  Phase  It should not dispatch on a specific type for  learner "},{"doctype":"documentation","id":"references/DataAugmentation.showitem!","title":"showitem!","text":"Visualize  item  Should return an image"},{"doctype":"documentation","id":"references/DataAugmentation.centered","title":"centered","text":"Transform  P  so that is applied around the center of  bounds  instead of the origin"},{"doctype":"documentation","id":"references/MLUtils.batchseq","title":"batchseq","text":"Take a list of  N  sequences and turn them into a single sequence where each item is a batch of  N  Short sequences will be padded by  pad  Examples"},{"doctype":"documentation","id":"references/FastAI.Datasets.initdatadeps","title":"initdatadeps","text":""},{"doctype":"documentation","id":"references/FastAI.showsample","title":"showsample","text":"data blocks loaddataset task data sample data task sample task sample Show an unprocessed  sample  for  LearningTask   task  to  backend ShowBackend  Examples"},{"doctype":"documentation","id":"references/DataAugmentation.Crop","title":"Crop","text":""},{"doctype":"documentation","id":"references/FastVision.Models.catchannels","title":"catchannels","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.RAdam","title":"RAdam","text":"opt opt Rectified Adam  optimizer Parameters Learning rate  η  Amount by which gradients are discounted before updating the weights Decay of momentums  β::Tuple  Exponential decay for the first β1 and the second β2 momentum estimate Examples"},{"doctype":"documentation","id":"references/FastVision.Models.UNetDynamic","title":"UNetDynamic","text":"Metalhead backbone Metalhead ResNet50 pretrain layers end unet backbone k_out unet unet backbone fdownscalk_out unet Create a U-Net model from convolutional  backbone  architecture After every downsampling layer i.e pooling or strided convolution a skip connection and an upsampling block are inserted resulting in a convolutional network with the same spatial output dimensions as its input Outputs an array with  k_out  channels Keyword arguments fdownscale  0  Number of upsampling steps to leave out By default there will be one upsampling step for every downsampling step in  backbone  Hence if the input spatial size is  h w  the output size will be  h/2^fdownscale w/2^fdownscale  i.e to get outputs at half the resolution set  fdownscale  1  kwargs  Other keyword arguments are passed through to  upsample  Examples"},{"doctype":"documentation","id":"references/FastVision.mockarray","title":"mockarray","text":""},{"doctype":"documentation","id":"references/Flux.Losses.binarycrossentropy","title":"binarycrossentropy","text":"Return the binary cross-entropy loss computed as Where typically the prediction  ŷ  is given by the output of a  sigmoid  activation The  ϵ  term is included to avoid infinity Using  logitbinarycrossentropy  is recomended over  binarycrossentropy  for numerical stability Use  label_smoothing  to smooth the  y  value as preprocessing before computing the loss See also  crossentropy   logitcrossentropy  Examples"},{"doctype":"documentation","id":"references/DataAugmentation.denormalize","title":"denormalize","text":""},{"doctype":"documentation","id":"references/Flux.NilNumber.nil","title":"nil","text":"Nil  is a singleton type with a single instance  nil  Unlike  Nothing  and  Missing  it is a number  Nil  Real  Number "},{"doctype":"documentation","id":"references/FluxTraining.testbatch","title":"testbatch","text":""},{"doctype":"documentation","id":"references/MLUtils.fill_like","title":"fill_like","text":"Create an array with the given element type and size based upon the given source array  x  All element of the new array will be set to  val  The third and fourth arguments are both optional defaulting to the given array's eltype and size The dimensions may be specified as an integer or as a tuple argument See also  zeros_like  and  ones_like  Examples"},{"doctype":"documentation","id":"references/Flux.multigate","title":"multigate","text":""},{"doctype":"documentation","id":"references/DataAugmentation.OneOf","title":"OneOf","text":"Apply one of  tfms  selected randomly with probability  ps  each or uniformly chosen if no  ps  is given"},{"doctype":"documentation","id":"references/FastAI.Datasets.matches","title":"matches","text":""},{"doctype":"documentation","id":"references/FastAI.mockmodel","title":"mockmodel","text":"Generate a  model  compatible with  task  for testing Create a fake model that maps batches of block  xblock  to batches of block  ŷblock  Useful for testing"},{"doctype":"documentation","id":"references/Flux.NilNumber","title":"NilNumber","text":""},{"doctype":"documentation","id":"references/DataAugmentation.TabularItem","title":"TabularItem","text":""},{"doctype":"documentation","id":"references/FastAI.decodestate","title":"decodestate","text":""},{"doctype":"documentation","id":"references/FastTabular.TableRow","title":"TableRow","text":"Block  for table rows with M categorical and N continuous columns  data  is valid if it satisfies the  AbstractRow  interface in Tables.jl values present in indices for categorical and continuous columns are consistent and  data  is indexable by the elements of  catcols  and  contcols "},{"doctype":"documentation","id":"references/FastAI.setschedules!","title":"setschedules!","text":"learner learner learner Set  schedules  on  learner s  Scheduler  callback so that training resumes from there If  learner  does not have a  Scheduler  callback yet adds it"},{"doctype":"document","id":"documents/README.md","title":"FastAI.jl","text":"data blocks load task blocks learner task data callbacks learner task learner Pkg Pkg add FastAI.jl FastAI.jl is a Julia library for training state-of-the art deep learning models From loading datasets and creating data preprocessing pipelines to training FastAI.jl takes the boilerplate out of deep learning projects It equips you with reusable components for every part of your project while remaining customizable at every layer FastAI.jl comes with support for common computer vision and tabular data learning tasks with more to come FastAI.jl's high-level workflows combine functionality from many packages in the ecosystem most notably  Flux.jl   FluxTraining.jl   DataAugmentation.jl  and  MLUtils.jl  See our  documentation  to find out more Example As an example here is how to train an image classification model Setup To get started install FastAI.jl using the Julia package manager or try it out with this  Google Colab template  Getting started To dive in you may be interested in an  overview of the high-level API  seeing some  example learning tasks  finding out  how you can search for and find datasets and other functionality  or our contributor guide Get in touch You can get in touch here on GitHub or on the JuliaLang Zulip in the  ml-contributors  channel  Acknowledgements FastAI.jl takes inspiration from the fantastic  fastai  library for Python Jeremy Howard and the fastai team kindly approved this project and its use of the fastai name This project also builds on many packages in the Julia ecosystem"},{"doctype":"documentation","id":"references/FastVision.IMAGENET_MEANS","title":"IMAGENET_MEANS","text":""},{"doctype":"documentation","id":"references/Flux.Upsample","title":"Upsample","text":"An upsampling layer One of two keywords must be given If  scale  is a number this applies to all but the last two dimensions channel and batch of the input It may also be a tuple to control dimensions individually Alternatively keyword  size  accepts a tuple to directly specify the leading dimensions of the output Currently supported upsampling  mode s and corresponding NNlib's methods are nearest    NNlib.upsample_nearest bilinear    NNlib.upsample_bilinear trilinear    NNlib.upsample_trilinear Examples"},{"doctype":"documentation","id":"references/FastTabular","title":"FastTabular","text":""},{"doctype":"document","id":"documents/docs/background/datapipelines.md","title":"Performant data pipelines","text":"DataLoaders batchviewcollated data blocks load task blocks size taskdata data task batchdata batchviewcollated taskdata NBATCHES i enumerate batchdata i i batchdata i NBATCHES i enumerate batchdata i NBATCHES data blocks load task blocks learner task data NBATCHES first learner data training learner model model i NBATCHES learner i zip learner data training NBATCHES BenchmarkTools N data data N i N data i obss data i i N i N task obss i data_orig load data_orig buffered data_320px load data_320px buffered data_160px load data_160px buffered Performant data pipelines Bottlenecks in data pipelines and how to measure and fix them When training large deep learning models on a GPU we clearly want wait as short as possible for the training to complete The hardware bottleneck is usually the GPU power you have available to you This means that data pipelines need to be fast enough to keep the GPU at 100 utilization that is keep it from starving Reducing the time the GPU has to wait for the next batch of data directly lowers the training time until the GPU is fully utilized There are other ways to reduce training time like using hyperparameter schedules and different optimizers for faster convergence but we'll only talk about improving GPU utilization here Reasons for low GPU utilization The main cause of low GPU utilization is that the next batch of data is not available after a training step and the GPU has to wait This means that in order to get full GPU utilization loading a batch must not take longer than a training step and the data must be loaded in the background so that it is ready the moment the GPU needs it These issues can be addressed by using worker threads to load multiple batches in parallel keeping the primary thread free and reducing the time it takes to load a single batch FastAI.jl by default uses  DataLoader  from the  DataLoaders.jl  package which addresses points 1 and 2 For those familiar with PyTorch it closely resembles  torch.utils.data.DataLoader  It also efficiently collates the data by reusing a buffer where supported We can measure the large performance difference by comparing a naive sequential data iterator with  eachobsparallel  the data iterator that  DataLoader  uses Running each timer twice to forego compilation time the sequential iterator takes 20 seconds while the parallel iterator using 11 background threads only takes 2.5 seconds This certainly isn't a proper benchmark but it shows the performance can be improved by an order of magnitude with no effort Beside increasing the amount of compute available with worker threads as above the data loading performance can also be improved by reducing the time it takes to load a single batch Since a batch is made up of some number of observations this usually boils down to reducing the loading time of a single observation If you're using the  LearningTask  API this can be further broken down into the loading and encoding part Measuring performance So how do you know if your GPU is underutilized If it isn't then improving data pipeline performance won't help you at all One way to check this is to start training and run   watch n 0.1 nvidia-smi  in a terminal which displays and refreshs GPU stats every 1/10th of a second If  GPU-Util  stays between 90 and 99 you're good If that's not the case you might see it frantically jumping up and down We can get a better estimate of how much training time can be sped up by running the following experiment Load one batch and run  n  optimization steps on this batch The time this takes corresponds to the training time when the GPU does not have to wait for data to be available Next take your data iterator and time iterating over the first  n  batches  without  an optimization step The speed of the complete training loop data loading and optimization will be around the maximum of either measurement Roughly speaking if 1 takes 100 seconds and 2 takes 200 seconds you know that you can speed up training by about a factor of 2 if you reduce data loading time by half after which the GPU will become the bottleneck Again make sure to run each measurement twice so you don't include the compilation time To find performance bottlenecks in the loading of each observation you'll want to compare the time it takes to load an observation of the task data container and the time it takes to encode that observation This will give you a pretty good idea of where the performance bottleneck is Note that the encoding performance is often dependent of the task configuration If we used  ImageClassification  with input size  64 64  it would be much faster Improving performance So you've identified the data pipeline as a performance bottleneck What now Before anything else make sure you're doing the following Use  DataLoaders.DataLoader  as a data iterator If you're using  taskdataloaders  or  tasklearner  this is already the case Start Julia with multiple threads by specifying the  t n  t auto  flag when starting Julia If it is successful  Threads.nthreads  should be larger than  1  If the data loading is still slowing down training you'll probably have to speed up the loading of each observation As mentioned above this can be broken down into observation loading and encoding The exact strategy will depend on your use case but here are some examples Reduce loading time of image datasets by presizing For many computer vision tasks you will resize and crop images to a specific size during training for GPU performance reasons If the images themselves are large loading them from disk itself can take some time If your dataset consists of 1920x1080 resolution images but you're resizing them to 256x256 during training you're wasting a lot of time loading the large images  Presizing  means saving resized versions of each image to disk once and then loading these smaller versions during training We can see the performance difference using ImageNette since it comes in 3 sizes original 360px and 180px Reducing allocations with inplace operations When implementing the  LearningTask  interface you have the option to implement  encode!(buf task context sample  an inplace version of  encode  that reuses a buffer to avoid allocations Reducing allocations often speeds up the encoding step and can also reduce the frequency of garbage collector pauses during training which can reduce GPU utilization Using efficient data augmentation Many kinds of augmentation can be composed efficiently A prime example of this are image transformations like resizing scaling and cropping which are powered by  DataAugmentation.jl  See  its documentation  to find out how to implement efficient composable data transformations"},{"doctype":"documentation","id":"references/DataAugmentation.AdjustContrast","title":"AdjustContrast","text":"TestImages item testimage tfm titems tfm item _ titems ncol npad Adjust the contrast of an image by a factor chosen uniformly from  f ∈ 1-δ 1+δ  Pixels  c  are transformed  c  μ*(1-f  where  μ  is the mean color of the image You can also pass any  Distributions.Sampleable  from which the factor is selected Example"},{"doctype":"document","id":"documents/docs/notebooks/presizing.ipynb","title":"Presizing vision datasets for performance","text":"p_160 load p_320 load p_orig load imagedatacontainer dir dir data_160 imagedatacontainer p_160 data_320 imagedatacontainer p_320 data_orig imagedatacontainer p_orig MosaicViews mosaicview data_160 data_320 data_orig nrow idxs rand data_160 name data zip data_160 data_320 data_orig println name i idxs data i Images presizeimage image sz ratio maximum sz size image newsz round Int size image ratio σ ratio ratio k KernelFactors gaussian σ imresize imfilter image k NA newsz SZ image data_orig presizeimage image SZ FilePathsBase DSTDIR Path mktempdir presizeimagedir srcdir dstdir sz pathdata srcdir i pathdata mkpath parent pathdata i Threads i pathdata srcp pathdata i p relpath srcp srcdir dstp joinpath dstdir p img srcp img_presized presizeimage img sz save string dstp img_presized presizeimagedir p_orig DSTDIR SZ data loadtaskdata DSTDIR ImageClassification Presizing vision datasets for performance In this tutorial we'll look at how we can improve the performance of computer vision data pipelines by presizing Presizing is a dataset preprocessing step executed before any training in which all images are loaded and saved at a smaller size This can improve performance if image loading is a bottleneck Presizing is useful when the original image sizes in a dataset are much larger than the size we use during training if the images are downsized every time they are loaded anyway a lot of work can be avoided by doing this once before training For example if we train an image classification model on resized image crops of size  160 160  then we can presize every image so that the shorter length is at least 160 Let's first look at the performance difference at this makes Conveniently the ImageNette dataset comes in 3 sizes original 320px and 160px Let's download them and create data containers that load the images and do nothing else Now every observation is simply a single image but the sizes vary Let's see how long it takes to load a random subset of 100 images Quite a difference The 320px version loads about 4 times slower than the 160px version after all there are 4 times as many pixels Note that image loading is only a part of the data pipeline Optimizing it with presizing only makes sense if it becomes a bottleneck See  Performant data pipelines  for a more general discussion Implementing presizing Next we'll look at how to do presizing ourselves For an image classification dataset this entails copying the folder structure but replacing every image with a downscaled version Let's say as above we want to train a model on images of size 160 160 Since we still want to use random crops during training we don't want to do the cropping yet Instead we downscale the image while preserving the aspect ratio so that the smallest side is still at least 160 pixels long The following function does just that for a single image Now we need to run this over every image in a folder To speed things up we run this in parallel using  Threads.@threads  We can now load the created dataset as a regular image classification dataset Remarks Keypoint and segmentation data  Presizing can of course be useful with other image datasets like those for segmantic segmentation and with keypoint data You have to be more careful when presizing those though since the target variable is affected by the resizing if an image is downsized then any segmentation masks and keypoints on it also need to be downsized Progressive resizing  Presizing can also be used in conjunction with progressive resizing a technique pioneered by Jeremy Howard where the training starts with small image sizes for speed and uses larger image sizes later for better performance This can improve convergence speed quite a bit"},{"doctype":"documentation","id":"references/FastAI.OneHot","title":"OneHot","text":"Encoding  that turns categorical labels into one-hot encoded arrays of type  T  Encodes"},{"doctype":"documentation","id":"references/FluxTraining.defaultcallbacks","title":"defaultcallbacks","text":""},{"doctype":"documentation","id":"references/FluxTraining.NoConflict","title":"NoConflict","text":"Return from  resolveconflict  to indicate that while the callbacks modify the same state they can be used together without any problems"},{"doctype":"documentation","id":"references/MLUtils.batchindex","title":"batchindex","text":""},{"doctype":"documentation","id":"references/FastAI.decodeypred!","title":"decodeypred!","text":""},{"doctype":"documentation","id":"references/FluxTraining.Learner","title":"Learner","text":"Holds and coordinates all state of the training  model  is trained by optimizing  lossfn  with  optimizer  on  data  Arguments Positional arguments model  A Flux.jl model or a  NamedTuple  of models lossfn  Loss function with signature  lossfn(model(x y  Number  Keyword arguments optional data    Data iterators A 2-tuple will be treated as  trainingdataiter validdataiter  You can also pass in an empty tuple    and use the  epoch  method with a  dataiter  as third argument A data iterator is an iterable over batches For regular supervised training each batch should be a tuple  xs ys  optimizer  ADAM  The optimizer used to update the  model s weights callbacks    A list of callbacks that should be used If  usedefaultcallbacks  true  this will be extended by the default callbacks usedefaultcallbacks  true  Whether to add some basic callbacks Included are  Metrics   Recorder   ProgressPrinter   StopOnNaNLoss  and  MetricsPrinter  cbrunner  LinearRunner  Callback runner to use Fields Use this as a reference when implementing callbacks model   optimizer  and  lossfn  are stored as passed in data  is a  PropDict  of data iterators usually  training  and  validation  params  An instance of  model s parameters of type  Flux.Params  If  model  is a  NamedTuple  then  params  is a  NamedTuple  as well step PropDict  State of the last step Contents depend on the last run  Phase  cbstate PropDict  Special state container that callbacks can save state to for other callbacks Its keys depend on what callbacks are being used See the  custom callbacks guide  for more info"},{"doctype":"documentation","id":"references/FastVision.Models.xresnet50","title":"xresnet50","text":""},{"doctype":"document","id":"documents/docs/data_containers.md","title":"Data containers","text":"data _ load findfirst datasetid image class obs data class image data dir load files dir p files loadimageclass p p p image class loadimageclass p class image data loadimageclass files traindata valdata data at datagroups files p p trainfiles validfiles datagroups datagroups entry data blocks load entry Data containers This tutorial explains what data containers are how they are used in FastAI.jl and how to create your own You are encouraged to follow along in a REPL or a Jupyter notebook and explore the code You will find small exercises at the end of some sections to deepen your understanding Introduction In the  quickstart  section you have already come in contact with data containers The following code was used to load a data container for image classification A data container is any type that holds observations of data and allows us to load them with  getobs  and query the number of observations with  numobs  In this case each observation is a tuple of an image and the corresponding class after all we want to use it for image classification load datasets id  makes it easy to a load a data container that is compatible with some block types but to get a better feel for what it does let's look under the hood by creating the same data container using some mid-level APIs Creating data containers from files Before we recreate the data container we'll download the dataset and get the path where the files are saved to Now we'll start with  loadfolderdata  which creates a data container here a  Vector  of files given a path We'll use the path of the downloaded dataset files  is a data container where each observation is a path to a file We'll confirm that using  getobs  Next we need to load an image and the corresponding class from the path If you have a look at the folder structure of  dir  you can see that the parent folder of each file gives the name of class So we can use the following function to load the  image class  pair from a path Finally we use  mapobs  to lazily transform each observation and have a data container ready to be used for training an image classifier Exercises Using  mapobs  and  loadfile  create a data container where every observation is only an image Change the above code to run on a different dataset from the list in  Datasets.DATASETS_IMAGECLASSIFICATION  Splitting a data container into subsets Until now we've only created a single data container containing all observations in a dataset In practice though you'll want to have at least a training and validation split The easiest way to get these is to randomly split your data container into two parts Here we split  data  into 80 training and 20 validation data Note the use of  shuffleobs  to make sure each split has approximately the same class distribution This is great for experimenting but where possible you will want to use the official training/validation split for a dataset Consider the image classification dataset folder structure As you can see the grandparent folder of each image indicates which split it is a part of  groupobs  allows us to partition a data container using a function Let's use it to split  filedata  based on the name of the grandparent directory We can't reuse  data  for this since it no longer carries the file information Using this official split it will be easier to compare the performance of your results with those of others Dataset recipes We saw above how different image classification datasets can be loaded with the same logic as long as they are in a common format To encapsulate the logic for loading common dataset formats FastAI.jl has  DatasetRecipe s When we used  datarecipes  in the  discovery tutorial  it showed us such recipes that allow loading a dataset for a specific task For example  imagenette2-160  has an associated  ImageFolders  recipe which we can load by getting the entry and calling  load  on it These recipes also take care of loading the data block information for the dataset Read the  discovery tutorial  to find out more about that"},{"doctype":"documentation","id":"references/FastAI.Datasets.obsslice","title":"obsslice","text":""},{"doctype":"documentation","id":"references/Flux.OneHotLike","title":"OneHotLike","text":""},{"doctype":"documentation","id":"references/FastAI.PropagateAlways","title":"PropagateAlways","text":""},{"doctype":"documentation","id":"references/FluxTraining.runchecks","title":"runchecks","text":""},{"doctype":"documentation","id":"references/FluxTraining.TimeThrottle","title":"TimeThrottle","text":""},{"doctype":"documentation","id":"references/FluxTraining.Phases.TrainingPhase","title":"TrainingPhase","text":"A regular training phase for supervised learning It iterates over batches in  learner.data.training  and updates the model parameters using  learner.optim  after calculating the gradients Throws the following events in this order EpochBegin  when an epoch starts StepBegin  when a step starts LossBegin  after the forward pass but before loss calculation BackwardBegin  after loss calculation but before backward pass BackwardEnd  after the bacward pass but before the optimization step StepEnd  when a step ends and EpochEnd  when an epoch ends It writes the following step state to  learner.state  grouped by the event from which on it is available StepBegin  xs  and  ys  encoded input and target batch LossBegin  ŷs  model output BackwardBegin  loss  loss BackwardEnd  grads  calculated gradients"},{"doctype":"documentation","id":"references/FastAI.fillblock","title":"fillblock","text":"Replaces all  nothing s in outblocks with the corresponding block in  inblocks   outblocks  may be obtained by"},{"doctype":"documentation","id":"references/Flux","title":"Flux","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.InvDecay","title":"InvDecay","text":"opt Apply inverse time decay to an optimiser so that the effective step size at iteration  n  is  eta  1  γ  n  where  eta  is the initial step size The wrapped optimiser's step size is not modified See also the  Scheduling Optimisers  section of the docs for more general scheduling techniques Examples InvDecay  is typically composed  with other optimizers as the last transformation of the gradient"},{"doctype":"documentation","id":"references/FastTabular.runtests","title":"runtests","text":"Equivalent to  ReTest.retest(FastTabular pattern kwargs  This function is defined automatically in any module containing a  testset  possibly nested within submodules"},{"doctype":"documentation","id":"references/DataAugmentation.ScaleFixed","title":"ScaleFixed","text":"Projective transformation that scales sides to  sizes  disregarding aspect ratio See also  ScaleKeepAspect "},{"doctype":"documentation","id":"references/DataAugmentation.Zoom","title":"Zoom","text":"Zoom into an item by a factor chosen from the interval  scales  or  distribution "},{"doctype":"documentation","id":"references/Flux.AdaptiveMaxPool","title":"AdaptiveMaxPool","text":"Adaptive max pooling layer Calculates the necessary window size such that its output has  size(y)[1:N  out  Expects as input an array with  ndims(x  N+2  i.e channel and batch dimensions after the  N  feature dimensions where  N  length(out  See also  MaxPool   AdaptiveMeanPool  Examples"},{"doctype":"documentation","id":"references/Flux.Optimise.ExpDecay","title":"ExpDecay","text":"opt Discount the learning rate  η  by the factor  decay  every  decay_step  steps till a minimum of  clip  Parameters Learning rate  η  Amount by which gradients are discounted before updating the weights decay  Factor by which the learning rate is discounted decay_step  Schedule decay operations by setting the number of steps between two decay operations clip  Minimum value of learning rate start Step at which the decay starts See also the  Scheduling Optimisers  section of the docs for more general scheduling techniques Examples ExpDecay  is typically composed  with other optimizers as the last transformation of the gradient Note you may want to start with  η=1  in  ExpDecay  when combined with other optimizers  Adam  in this case that have their own learning rate"},{"doctype":"documentation","id":"references/DataAugmentation.getprojection","title":"getprojection","text":"Create a projection for an item with spatial bounds  bounds  The projection should be a  CoordinateTransformations.Transformation  See  CoordinateTransformations.jl"},{"doctype":"documentation","id":"references/DataAugmentation.adjustcontrast!","title":"adjustcontrast!","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.Descent","title":"Descent","text":"opt opt ps model gs gradient ps loss x y opt ps gs Classic gradient descent optimiser with learning rate  η  For each parameter  p  and its gradient  δp  this runs  p  η*δp Parameters Learning rate  η  Amount by which gradients are discounted before updating the weights Examples"},{"doctype":"documentation","id":"references/FastAI.blocklossfn","title":"blocklossfn","text":"Construct a loss function that compares a batch of model outputs  ŷs  and encoded targets  ys  and returns a scalar loss For example for  block  OneHotLabel(classes  i.e an encoded  Label(classes  we have  blocklossfn(block block  Flux.Losses.logitcrossentropy "},{"doctype":"documentation","id":"references/Flux.Optimise.Adam","title":"Adam","text":"opt opt Adam  optimiser Parameters Learning rate  η  Amount by which gradients are discounted before updating the weights Decay of momentums  β::Tuple  Exponential decay for the first β1 and the second β2 momentum estimate Examples"},{"doctype":"documentation","id":"references/Flux.conv_dims","title":"conv_dims","text":""},{"doctype":"documentation","id":"references/FluxTraining.Events.EpochEnd","title":"EpochEnd","text":"Event  called at the end of an epoch"},{"doctype":"documentation","id":"references/FastAI.getencodings","title":"getencodings","text":""},{"doctype":"documentation","id":"references/Flux.early_stopping","title":"early_stopping","text":"Return a function that internally counts by one when  distance(best_score f  min_dist  where  best_score  is the last seen best value of  f  If the count is greater than or equal to  delay  the function returns  true  otherwise it returns  false  The count is reset when  distance(best_score f  min_dist  Examples"},{"doctype":"documentation","id":"references/FluxTraining.Phases","title":"Phases","text":""},{"doctype":"documentation","id":"references/Flux.Losses.xlogy","title":"xlogy","text":"Return  x  log(y  for  y  0  and zero when  x  0 "},{"doctype":"documentation","id":"references/DataAugmentation.CroppedProjectiveTransform","title":"CroppedProjectiveTransform","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.DataDepLoader","title":"DataDepLoader","text":"A dataset loader that uses DataDeps.jl to load datasets The DataDep has to be registered before creating the loader and will error otherwise"},{"doctype":"documentation","id":"references/FastVision.KeypointTensor","title":"KeypointTensor","text":"Block for encoded  Keypoints N T M  returned by  KeypointPreprocessing "},{"doctype":"documentation","id":"references/DataAugmentation.apply!","title":"apply!","text":"Applies  tfm  to  item  mutating the preallocated  buffer  buffer  can be obtained with  buffer  makebuffer(tfm item Default to  apply(tfm item  non-mutating version"},{"doctype":"documentation","id":"references/FastAI.Datasets.parentname","title":"parentname","text":""},{"doctype":"documentation","id":"references/FastVision.isimagefile","title":"isimagefile","text":""},{"doctype":"documentation","id":"references/FastVision.Models.upsample_block_small","title":"upsample_block_small","text":"An upsampling block that increases the spatial dimensions of the input by 2 using pixel-shuffle upsampling"},{"doctype":"documentation","id":"references/DataAugmentation.itemfield","title":"itemfield","text":""},{"doctype":"documentation","id":"references/Flux.Optimise.EPS","title":"EPS","text":""},{"doctype":"documentation","id":"references/FluxTraining.SmoothLoss","title":"SmoothLoss","text":""},{"doctype":"documentation","id":"references/Flux.dropout","title":"dropout","text":"The dropout function If  active  is  true  for each input either sets that input to  0  with probability  p  or scales it by  1  1  p   dims  specifies the unbroadcasted dimensions e.g  dims=1  applies dropout along columns and  dims=2  along rows If  active  is  false  it just returns the input  x  Specify  rng  for custom RNGs instead of the default RNG Note that custom RNGs are only supported on the CPU Warning when using this function you have to manually manage the activation state Usually in fact dropout is used while training but is deactivated in the inference phase This can be automatically managed using the  Dropout  layer instead of the  dropout  function The  Dropout  layer is what you should use in most scenarios"},{"doctype":"documentation","id":"references/FluxTraining.Phases.Phase","title":"Phase","text":"Abstract supertype for all phases See  subtypes(FluxTraining.Phase  A  Phase  is used in dispatch for training loop functions  step  and  epoch  as well as in  Callback  handler methods  on "},{"doctype":"document","id":"documents/docs/notebooks/keypointregression.ipynb","title":"Keypoint regression","text":"CairoMakie CairoMakie activate! type DelimitedFiles readdlm Metalhead FilePathsBase path load files path files imagefiles path filterfn annotfiles path filterfn p occursin p imagefiles annotfiles readcalibrationfile p readdlm string p CAL readcalibrationfile joinpath path loadannotfile annotpath cal CAL ctr readdlm string annotpath cx ctr cal ctr cal cy ctr cal ctr cal SVector cy cx data imagefiles loadannotfile annotfiles ids map p parse Int p imagefiles obs image ks data image ks size image traindata data data ids validdata data data ids traindata validdata sz task sz buffered augmentations max_warp sz im k traindata x y task im k summary x y task y xs ys task traindata task xs ys inblock N outblock N backbone N outsz backbone ntuple _ N inblock nchannels outch outsz end head outch prod outblock sz N p backbone head backbone Metalhead ResNet layers end model task backbone traindl validdl traindata validdata task learner model task data traindl validdl optimizer callbacks learner task learner model task learner n context task learner n context Keypoint regression Single keypoint regression consists of localizing a keypoint in an image Here we'll be training on a head pose dataset where every image has a person in it and the head of the person is annotated Since keypoint datasets all have different formats we have to do a bit more manual work to get the task dataset loaded First we import everything we'll need Creating a task data container load datasets id  downloads the files but it's up to us to load them into a usable format In the end the task data container should contain tuples of an image and a keypoint each First we create a  loadfolderdata  from the directory where the dataset has been downloaded to Loading a  loadfolderdata  simply treats every file as a single observation However that is not what we want here for every observation we have one image and one annotation file that make up one observation and we want to ignore all other files like the README To achieve this we'll create two data containers containing all the image paths and annotation paths respectively by filtering the container with all paths Next we need to map functions over each observation that load the data from the files An image file can be loaded using the  loadfile  utility The keypoints have a custom format so we write a helper function to parse them from a text file The details of how the format is loaded aren't important Now we can use  mapobs  to lazily map the loading function over the container Note that beside loading the image and keypoint we also extract the subject ID from the path We'll use this in a bit for splitting the dataset appropriately and we don't have access to the path information anymore once we have a container of loaded data We can visualize an observation using  DataAugmentation.showitems  if we wrap the data in item types Before we can start using this data container for training we need to split it into a training and validation dataset Since there are 13 different persons with many images each randomly splitting the container does not make sense The validation dataset would then contain many images that are very similar to those seen in training and would hence say little about the generalization ability of a model We instead use the first 12 subjects as a training dataset and validate on the last The learning task Next we need to define a  learning task  that encodes and augments each image and keypoint in a form that we can train a model on We need to create a  LearningTask  struct for which we can define these transformations Here we make use of  ProjectiveTransforms  for resizing cropping and augmenting the image and keypoint and  ImagePreprocessing  to reshape and normalize the image Finally  KeypointPreprocessing  makes sure keypoints fall between 1 and 1 We can check that each image is resized to  224 224  and the keypoints are normalized Decoding the encoded targets should give back a point within the original image bounds That is looking good We can see that the keypoint is aligned with center of the head even after heavy augmentation Now it is finally time to train a model Training We'll use a modified ResNet as a model backbone and add a couple layers that regress the keypoint  taskmodel  knows how to do this by looking at the data blocks used and calling  blockmodel KeypointTensor{2 Float32}((1 KeypointTensor{2 Float32}((1 backbone  The implementation for reference looks like this Next we create a pair of training and validation data loaders They take care of batching and loading the data in parallel in the background With the addition of an optimizer and a loss function we can now create a  Learner  and start training Just like  taskmodel   tasklossfn  selects the appropriate loss function for a  BlockTask s blocks Here both the encoded target block and model output block are  block  KeypointTensor{2 Float32}((1  so  blocklossfn(block block  is called which returns Mean Squared Error as a suitable loss function We can save the model for later inference using  savetaskmodel  The loss is going down during training which is a good sign but visualizing the predictions against the ground truth will give us a better idea of how well the model performs We'll use  showoutputs  to compare batches of encoded targets and model outputs For this we can run the model on a batch from the validation dataset and see how it performs We can also see that the trained model generalizes well to the heavy augmentation employed during training The augmentation also explains why the training loss is so much higher than the validation loss"},{"doctype":"documentation","id":"references/FluxTraining","title":"FluxTraining","text":""},{"doctype":"documentation","id":"references/FastAI.showoutputbatch","title":"showoutputbatch","text":"Show collated batch of outputs to  backend  If a collated batch of encoded samples  batch  is also given show them next to the outputs See  showoutputs  if you have vectors of outputs and not collated batches"},{"doctype":"documentation","id":"references/FastAI.Datasets.unbatch","title":"unbatch","text":""},{"doctype":"documentation","id":"references/Flux.params","title":"params","text":"Given a model or specific layers from a model create a  Params  object pointing to its trainable parameters This can be used with the  gradient  function see  Taking Gradients  or as input to the  Flux.train   Flux.train function The behaviour of  params  on custom types can be customized using  Functor.@functor  or  Flux.trainable  Examples"},{"doctype":"documentation","id":"references/Flux.Optimise.AMSGrad","title":"AMSGrad","text":"opt opt The  AMSGrad  version of the Adam optimiser Parameters don't need tuning Parameters Learning rate  η  Amount by which gradients are discounted before updating the weights Decay of momentums  β::Tuple  Exponential decay for the first β1 and the second β2 momentum estimate Examples"},{"doctype":"documentation","id":"references/MLUtils.mapobs","title":"mapobs","text":"data data mdata data mdata data nameddata x sqrt y log data nameddata x sqrt y log nameddata x sqrt Lazily map  f  over the observations in a data container  data  Lazily map each function in tuple  fs  over the observations in data container  data  Returns a tuple of transformed data containers Map a  NamedTuple  of functions over  data  turning it into a data container of  NamedTuple s Field syntax can be used to select a column of the resulting data container"},{"doctype":"documentation","id":"references/FastVision.plotmask!","title":"plotmask!","text":""},{"doctype":"documentation","id":"references/Flux.Losses.MAX_THREADS","title":"MAX_THREADS","text":""},{"doctype":"documentation","id":"references/Flux.CrossCor","title":"CrossCor","text":"Standard cross correlation layer  filter  is a tuple of integers specifying the size of the convolutional kernel  in  and  out  specify the number of input and output channels Parameters are controlled by additional keywords with defaults  init=glorot_uniform  and  bias=true  See also  Conv  for more detailed description of keywords Examples Constructs a CrossCor layer with the given weight and bias Accepts the same keywords and has the same defaults as  CrossCor(k::NTuple{N,Integer ch::Pair{<:Integer,<:Integer σ    CrossCor Examples"},{"doctype":"documentation","id":"references/FastMakie.runtests","title":"runtests","text":"Equivalent to  ReTest.retest(FastMakie pattern kwargs  This function is defined automatically in any module containing a  testset  possibly nested within submodules"},{"doctype":"documentation","id":"references/FastAI.showsamples","title":"showsamples","text":"data blocks loaddataset task data samples data i i task samples task samples Show a vector of unprocessed  samples  for  LearningTask   task  to  backend ShowBackend  Examples"},{"doctype":"documentation","id":"references/FastAI.Inference","title":"Inference","text":""},{"doctype":"documentation","id":"references/DataAugmentation.showimage!","title":"showimage!","text":""},{"doctype":"documentation","id":"references/FastAI.LearningTask","title":"LearningTask","text":"Represents a concrete approach for solving a learning task A  LearningTask  defines how data is processed encoded and decoded before and after going through a model Extending It is recommended to use  AbstractBlockTask s like  BlockTask  and  SupervisedTask  to construct tasks but you may subtype  LearningTask  for lower-level control There is a core interface that will allow you to train models and perform inference for supervised tasks It consists of encodesample encodeinput decodeypred You can optionally implement additional interfaces to get support for higher-level features of the library Training interface  tasklossfn   taskmodel Testing interface  mocksample   mockinput   mocktarget   mockmodel Batching  shouldbatch"},{"doctype":"documentation","id":"references/FastVision.getsamplebounds","title":"getsamplebounds","text":""},{"doctype":"documentation","id":"references/FastTabular.tabular_embedding_backbone","title":"tabular_embedding_backbone","text":""},{"doctype":"documentation","id":"references/Flux.ofeltype","title":"ofeltype","text":""},{"doctype":"documentation","id":"references/FluxTraining.CancelEpochException","title":"CancelEpochException","text":"learner phase _ batches learner phase learner step loss throw Throw during fitting to cancel the currently running epoch This prematurely ends the current epoch without throwing an error Must be thrown inside the context of  runepoch  Examples"},{"doctype":"documentation","id":"references/FastVision.getimagepreprocessing","title":"getimagepreprocessing","text":""},{"doctype":"documentation","id":"references/Flux.nil_input","title":"nil_input","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.DATASETS","title":"DATASETS","text":""},{"doctype":"documentation","id":"references/MLUtils.getobs!","title":"getobs!","text":"Inplace version of  getobs(data idx  If this method is defined for the type of  data  then  buffer  should be used to store the result instead of allocating a dedicated object Implementing this function is optional In the case no such method is provided for the type of  data  then  buffer  will be  ignored  and the result of  getobs  returned This could be because the type of  data  may not lend itself to the concept of  copy  Thus supporting a custom  getobs  is optional and not required"},{"doctype":"documentation","id":"references/FastAI.makebatch","title":"makebatch","text":"Create a batch of encoded data by loading  idxs  from data container  data  Useful for inspection and as input to  showbatch  Samples are encoded in  context  which defaults to  Training "},{"doctype":"documentation","id":"references/FastAI.decode","title":"decode","text":""},{"doctype":"documentation","id":"references/FastAI.isshowable","title":"isshowable","text":""},{"doctype":"documentation","id":"references/FastAI.encodedblockfilled","title":"encodedblockfilled","text":""},{"doctype":"documentation","id":"references/FastAI.getbatch","title":"getbatch","text":"Get a batch of data from  learner  Take a batch of training data by default or validation data if  validation  true  If  n  take only the first  n  samples from the batch"},{"doctype":"documentation","id":"references/FastAI.AbstractBlock","title":"AbstractBlock","text":"Abstract supertype of all blocks You should not subtype form this but instead from  Block  or  WrapperBlock "},{"doctype":"documentation","id":"references/MLUtils.kfolds","title":"kfolds","text":"julia train_idx val_idx x_train x_val X k x_train y_train val X Y k x_train x_val X k Compute the train/validation assignments for  k  repartitions of  n  observations and return them in the form of two vectors The first vector contains the index-vectors for the training subsets and the second vector the index-vectors for the validation subsets respectively A general rule of thumb is to use either  k  5  or  k  10  The following code snippet generates the indices assignments for  k  5 Each observation is assigned to the validation subset once and only once Thus a union over all validation index-vectors reproduces the full range  1:n  Note that there is no random assignment of observations to subsets which means that adjacent observations are likely to be part of the same validation subset Repartition a  data  container  k  times using a  k  folds strategy and return the sequence of folds as a lazy iterator Only data subsets are created which means that no actual data is copied until  getobs  is invoked Conceptually a k-folds repartitioning strategy divides the given  data  into  k  roughly equal-sized parts Each part will serve as validation set once while the remaining parts are used for training This results in  k  different partitions of  data  In the case that the size of the dataset is not dividable by the specified  k  the remaining observations will be evenly distributed among the parts Multiple variables are supported e.g for labeled data By default the folds are created using static splits Use  shuffleobs  to randomly assign observations to the folds See  leavepout  for a related function"},{"doctype":"documentation","id":"references/Flux.Losses.log_plus_f","title":"log_plus_f","text":""},{"doctype":"documentation","id":"references/MLUtils.ObsView","title":"ObsView","text":"X Y size X v X v typeof v v X v X v typeof v SubArray subset X Y subset typeof subset Tuple x X typeof x SubArray Float64 x y X Y typeof x SubArray Float64 typeof y String x y X Y typeof x SubArray Float64 typeof y String x y X Y Used to represent a subset of some  data  of arbitrary type by storing which observation-indices the subset spans Furthermore subsequent subsettings are accumulated without needing to access actual data The main purpose for the existence of  ObsView  is to delay data access and movement until an actual batch of data or single observation is needed for some computation This is particularily useful when the data is not located in memory but on the hard drive or some remote location In such a scenario one wants to load the required data only when needed Any data access is delayed until  getindex  is called and even  getindex  returns the result of  obsview  which in general avoids data movement until  getobs  is called If used as an iterator the view will iterate over the dataset once effectively denoting an epoch Each iteration will return a lazy subset to the current observation Arguments data   The object describing the dataset Can be of any type as long as it implements  getobs  and  numobs  see Details for more information indices   Optional The index or indices of the observation(s in  data  that the subset should represent Can be of type  Int  or some subtype of  AbstractVector  Methods getindex   Returns the observation(s of the given index/indices No data is copied aside from the required indices numobs   Returns the total number observations in the subset getobs   Returns the underlying data that the  ObsView  represents at the given relative indices Note that these indices are in subset space and in general will not directly correspond to the same indices in the underlying data set Details For  ObsView  to work on some data structure the desired type  MyType  must implement the following interface getobs(data::MyType idx   Should return the observation(s indexed by  idx  In what form is up to the user Note that  idx  can be of type  Int  or  AbstractVector  numobs(data::MyType   Should return the total number of observations in  data The following methods can also be provided and are optional getobs(data::MyType   By default this function is the identity function If that is not the behaviour that you want for your type you need to provide this method as well obsview(data::MyType idx   If your custom type has its own kind of subset type you can return it here An example for such a case are  SubArray  for representing a subset of some  AbstractArray  getobs!(buffer data::MyType idx   Inplace version of  getobs(data idx  If this method is provided for  MyType  then  eachobs  can preallocate a buffer that is then reused every iteration Note  buffer  should be equivalent to the return value of  getobs(::MyType   since this is how  buffer  is preallocated by default Examples See also obsview    getobs   numobs   splitobs   shuffleobs   kfolds "},{"doctype":"documentation","id":"references/FastAI.loadtaskmodel","title":"loadtaskmodel","text":"Load a trained  model  along with a  task  from  path  that were saved using  savetaskmodel  JLD2.jl  is used for serialization"},{"doctype":"documentation","id":"references/FastTabular.TabularPreprocessing","title":"TabularPreprocessing","text":"Encodes a  TableRow  by applying the following preprocessing steps DataAugmentation.NormalizeRow  for normalizing a row of data for continuous columns DataAugmentation.FillMissing  for filling missing values DataAugmentation.Categorify  for label encoding categorical columns which can be later used for indexing into embedding matrices or a sequence of these transformations"},{"doctype":"documentation","id":"references/FastAI.decodeypred","title":"decodeypred","text":""},{"doctype":"documentation","id":"references/DataAugmentation.compose","title":"compose","text":"Compose tranformations Use    as an alias Defaults to creating a  Sequence  of transformations but smarter behavior can be implemented For example  MapElem(f  MapElem(g  MapElem(g ∘ f "},{"doctype":"documentation","id":"references/DataAugmentation.showkeypoint!","title":"showkeypoint!","text":""},{"doctype":"documentation","id":"references/Flux.FluxCUDAAdaptor","title":"FluxCUDAAdaptor","text":""},{"doctype":"documentation","id":"references/FastAI.blockname","title":"blockname","text":"A short name describing a block that can be used in visualizations and other diagrams"},{"doctype":"documentation","id":"references/FastVision.Models.UNetFinalBlock","title":"UNetFinalBlock","text":""},{"doctype":"documentation","id":"references/Flux.Losses.compute_alpha_kernel","title":"compute_alpha_kernel","text":""},{"doctype":"documentation","id":"references/MLUtils.frequencies","title":"frequencies","text":""},{"doctype":"documentation","id":"references/FastAI.tasklearner","title":"tasklearner","text":"data blocks loaddataset task blocks learner task data learner learner task traindata validdata learner task data callbacks TensorboardBackend Create a  Learner  to train a model for learning task  task  using  data  Keyword arguments callbacks     Callback s to use during training batchsize  16  Batch size used for the training data loader backbone  nothing  Backbone model to construct task-specific model from using  taskmodel task backbone  model  nothing  Complete model to use If given the  backbone  argument is ignored optimizer  Adam  Optimizer passed to  Learner  lossfn   tasklossfn task  Loss function passed to  Learner  Any other keyword arguments will be passed to  taskdataloaders  Examples Full example Custom training and validation split Using callbacks"},{"doctype":"documentation","id":"references/MLUtils.LoaderState","title":"LoaderState","text":""},{"doctype":"documentation","id":"references/DataAugmentation.getrandstate","title":"getrandstate","text":"Generates random state for stochastic transformations Calling  apply(tfm item  is equivalent to  apply(tfm item randstate  getrandstate(tfm  It defaults to  nothing  so you it only needs to be implemented for stochastic  Transform s"},{"doctype":"documentation","id":"references/Flux.Losses.logitbinarycrossentropy","title":"logitbinarycrossentropy","text":"Mathematically equivalent to  binarycrossentropy(σ(ŷ y  but is more numerically stable See also  crossentropy   logitcrossentropy  Examples"},{"doctype":"documentation","id":"references/FastVision.grabbounds","title":"grabbounds","text":"Looks through  blocks  for block data that carries  N dimensional bounds information needed for projective transformations"},{"doctype":"documentation","id":"references/Flux.loadmodel!","title":"loadmodel!","text":"julia dst tanh bias tanh julia dst weight julia src relu bias julia dst src julia dst weight julia iszero dst bias Copy all the parameters trainable and non-trainable from  src  into  dst  Recursively walks  dst  and  src  together using  Functors.children  and calling  copyto  on parameter arrays or throwing an error when there is a mismatch Non-array elements such as activation functions are not copied and need not match Zero bias vectors and  bias=false  are considered equivalent see extended help for more details Examples Extended help Throws an error when dst  and  src  do not share the same fields at any level the sizes of leaf nodes are mismatched between  dst  and  src copying non-array values to/from an array parameter except inactive parameters described below dst  is a tied parameter i.e refers to another parameter and loaded into multiple times with mismatched source values Inactive parameters can be encoded by using the boolean value  false  instead of an array If  dst  false  and  src  is an all-zero array no error will be raised and no values copied however attempting to copy a non-zero array to an inactive parameter will throw an error Likewise copying a  src  value of  false  to any  dst  array is valid but copying a  src  value of  true  will error"},{"doctype":"documentation","id":"references/FluxTraining.Events.EpochBegin","title":"EpochBegin","text":"Event  called at the beginning of an epoch"},{"doctype":"documentation","id":"references/FastAI.Registries.datasets","title":"datasets","text":"path load info id Show a registry of available datasets Pass in filters as keyword arguments to look at a subset See also  finding functionality   learningtasks  and  datarecipes  For more information on registries see  FeatureRegistries.jl  Examples Show all available learning tasks Download a dataset Get an explanation of fields in the dataset registry Show all datasets with  image  in their name"},{"doctype":"documentation","id":"references/Flux.crosscor","title":"crosscor","text":""},{"doctype":"documentation","id":"references/FastAI.decodeŷ","title":"decodeŷ","text":""},{"doctype":"documentation","id":"references/FluxTraining.onecycle","title":"onecycle","text":"Creates a one-cycle  Schedule  over  nsteps  steps from  start_val  over  max_val  to  end_val  Examples"},{"doctype":"documentation","id":"references/FastAI.DiscriminativeLRs","title":"DiscriminativeLRs","text":"model paramgroups model dlro paramgroups Dict o dlro Use different learning rates based on  paramgroups   factors  maps each group to a factor that the learning rate is multiplied by so for a parameter  x  the factor is  get(factors getgroup(paramgroups x 1  See  ParamGroups  Examples Combining with regular gradient descent but only training a part of the model"},{"doctype":"documentation","id":"references/FastAI.blockcolumn","title":"blockcolumn","text":""},{"doctype":"documentation","id":"references/Flux.identity_init","title":"identity_init","text":"Return an  Array{Float32  of the given  size  which yields an identity mapping when used as parameters in most Flux layers Use  gain  to scale the identity by a constant Often useful in the context of transfer learning i.e when one wants to add more capacity to a model but start from the same mapping Has the following behaviour 1D A  Vector  of  zeros  useful for an identity bias 2D An identity matrix useful for an identity matrix multiplication More than 2D A dense block array of center tap spatial filters useful for an identity convolution Some caveats Not all layers will be identity mapping when used with this init Exceptions include recurrent layers and normalization layers Layers must have  input_size  output_size  for identity mapping to be possible When this is not the case extra dimensions of the array are padded with zeros For convolutional layers in addition to the above the kernel sizes must also be odd and padding must be applied so that output feature maps have the same size as input feature maps e.g by using  SamePad  Use keyword  shift  integer or tuple to apply circular shift to the output equivalent to  Base.circshift(identity_init(size shift  For consistency with other initialisers it accepts  rng::AbstractRNG  as an optional first argument But this is ignored since the result is not random Examples"},{"doctype":"documentation","id":"references/Flux.eachlastdim","title":"eachlastdim","text":""},{"doctype":"documentation","id":"references/FastAI.savetaskmodel","title":"savetaskmodel","text":"Save a trained  model  along with a  task  to  path  for later inference Use  loadtaskmodel  for loading both back into a session If  path  already exists only write to it if  force  true  If  model  weights are on a GPU they will be moved to the CPU before saving so they can be loaded in a non-GPU environment JLD2.jl  is used for serialization"},{"doctype":"documentation","id":"references/FastAI.Continuous","title":"Continuous","text":"Block  for collections of numbers  obs  is a valid observation if it's length is  size  and contains  Number s"},{"doctype":"documentation","id":"references/FastVision.ImageFolders","title":"ImageFolders","text":"julia Tuple Recipe for loading a single-label image classification dataset stored in a hierarchical folder format If  split  true  split the data container on the name of the grandparent folder The label defaults to the name of the parent folder but a custom function can be passed as  labelfn "},{"doctype":"documentation","id":"references/FluxTraining.ProgressPrinter","title":"ProgressPrinter","text":"Prints a progress bar of the currently running epoch"},{"doctype":"documentation","id":"references/FastAI.testencoding","title":"testencoding","text":"Performs some tests that the encoding interface is set up properly for  encoding  and  block  Tests that obs  is a valid instance  block encode  returns a valid  encodedblock(encoding block decode  returns a valid  decodedblock(encoding encodedblock(encoding block  and that the block is identical to  block"},{"doctype":"documentation","id":"references/FluxTraining.handle","title":"handle","text":""},{"doctype":"documentation","id":"references/Flux.Losses.count_repeats","title":"count_repeats","text":""},{"doctype":"documentation","id":"references/DataAugmentation.OneHot","title":"OneHot","text":"item rand item One-hot encodes a  MaskMulti  with  n  classes and size  sz  into an array item of size  sz n  with element type  T  Supports  apply "},{"doctype":"documentation","id":"references/DataAugmentation.adjustbrightness!","title":"adjustbrightness!","text":""},{"doctype":"documentation","id":"references/DataAugmentation.Item","title":"Item","text":"Abstract supertype of concrete items Subtype if you want to create a new item If you want to wrap an existing item see  ItemWrapper "},{"doctype":"documentation","id":"references/FastAI.typify","title":"typify","text":""},{"doctype":"documentation","id":"references/DataAugmentation.reflectionmatrix","title":"reflectionmatrix","text":""},{"doctype":"documentation","id":"references/Flux.Maxout","title":"Maxout","text":"This contains a number of internal layers each of which receives the same input Its output is the elementwise maximum of the the internal layers outputs Instead of defining layers individually you can provide a zero-argument function which constructs them and the number to construct Maxout over linear dense layers satisfies the univeral approximation theorem See Goodfellow Warde-Farley Mirza Courville  Bengio Maxout Networks  https://arxiv.org/abs/1302.4389  See also  Parallel  to reduce with other operators Examples"},{"doctype":"documentation","id":"references/FluxTraining.errorwriteconflict","title":"errorwriteconflict","text":""},{"doctype":"documentation","id":"references/FastAI.Datasets.DATASETCONFIGS","title":"DATASETCONFIGS","text":""},{"doctype":"documentation","id":"references/FastAI.getblocks","title":"getblocks","text":""},{"doctype":"documentation","id":"references/Flux.f32","title":"f32","text":"Converts the  eltype  of model's parameters to  Float32  which is Flux's default Recurses into structs marked with  functor "},{"doctype":"documentation","id":"references/Flux.Optimise","title":"Optimise","text":""},{"doctype":"documentation","id":"references/DataAugmentation.ScaleKeepAspect","title":"ScaleKeepAspect","text":"TestImages image testimage tfm tfm image Scales the shortest side of  item  to  minlengths  keeping the original aspect ratio Examples"},{"doctype":"documentation","id":"references/FastVision.Bounded","title":"Bounded","text":"block wrapper block rand wrapper rand block block block A  WrapperBlock  for annotating spatial data blocks with size information for their spatial bounds As an example  Image{2  doesn't carry any size information since it supports variable-size images but sometimes it can be useful to have the exact size as information where it can be known Encoding using  ProjectiveTransforms  returns  Bounded s since it crops any input to the same size Examples Wrapping a  Bounded  into another  Bounded  with the same dimensionality will update the bounds"},{"doctype":"documentation","id":"references/DataAugmentation.adjustbrightness","title":"adjustbrightness","text":""},{"doctype":"documentation","id":"references/FastAI.LRFinderResult","title":"LRFinderResult","text":"Result of the learning rate finder  lrfind  Use  plot  to visualize"},{"doctype":"documentation","id":"references/FluxTraining.LoggerBackend","title":"LoggerBackend","text":"Backend for logging callbacks like To add support for logging  Loggables.Loggable   L  to backend  B  implement log_to backend::B loggable::L names i See also  LogMetrics   LogHyperParams   log_to"},{"doctype":"documentation","id":"references/Flux.glorot_uniform","title":"glorot_uniform","text":"Return an  Array{Float32  of the given  size  containing random numbers drawn from a uniform distribution on the interval  x x  where  x  gain  sqrt(6  fan_in  fan_out  This method is described in 1 and also known as Xavier initialization Examples References 1 Glorot Xavier and Yoshua Bengio Understanding the difficulty of training deep feedforward neural networks  Proceedings of the thirteenth international conference on artificial intelligence and statistics  2010"}]