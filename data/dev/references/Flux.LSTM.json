{"attributes":{"kind":"function","backlinks":[{"tag":"sourcefile","title":"Flux/src/Flux.jl","docid":"sourcefiles/Flux/src/Flux.jl"},{"tag":"sourcefile","title":"Flux/src/layers/recurrent.jl","docid":"sourcefiles/Flux/src/layers/recurrent.jl"}],"methods":[{"line":343,"file":"/home/runner/.julia/packages/Flux/KkC79/src/layers/recurrent.jl","method_id":"Flux.LSTM_1","symbol_id":"Flux.LSTM","filedoc":"sourcefiles/Flux/src/layers/recurrent.jl","signature":"LSTM(a...; ka...)"}],"name":"LSTM","title":"LSTM","symbol_id":"Flux.LSTM","public":true,"module_id":"Flux"},"tag":"documentation","children":[{"attributes":{},"tag":"md","children":[{"attributes":{"lang":""},"tag":"codeblock","children":["LSTM(in => out)\n"],"type":"node"},{"attributes":{},"tag":"p","children":[{"attributes":{"href":"https://www.researchgate.net/publication/13853244_Long_Short-term_Memory","title":""},"tag":"a","children":["Long Short Term Memory"],"type":"node"}," recurrent layer. Behaves like an RNN but generally exhibits a longer memory span over sequences."],"type":"node"},{"attributes":{},"tag":"p","children":["The arguments ",{"attributes":{},"tag":"code","children":["in"],"type":"node"}," and ",{"attributes":{},"tag":"code","children":["out"],"type":"node"}," describe the size of the feature vectors passed as input and as output. That is, it accepts a vector of length ",{"attributes":{},"tag":"code","children":["in"],"type":"node"}," or a batch of vectors represented as a ",{"attributes":{},"tag":"code","children":["in x B"],"type":"node"}," matrix and outputs a vector of length ",{"attributes":{},"tag":"code","children":["out"],"type":"node"}," or a batch of vectors of size ",{"attributes":{},"tag":"code","children":["out x B"],"type":"node"},"."],"type":"node"},{"attributes":{},"tag":"p","children":["This constructor is syntactic sugar for ",{"attributes":{},"tag":"code","children":["Recur(LSTMCell(a...))"],"type":"node"},", and so LSTMs are stateful. Note that the state shape can change depending on the inputs, and so it is good to ",{"attributes":{},"tag":"code","children":["reset!"],"type":"node"}," the model between inference calls if the batch size changes. See the examples below."],"type":"node"},{"attributes":{},"tag":"p","children":["See ",{"attributes":{"href":"https://colah.github.io/posts/2015-08-Understanding-LSTMs/","title":""},"tag":"a","children":["this article"],"type":"node"}," for a good overview of the internals."],"type":"node"},{"attributes":{},"tag":"h1","children":["Examples"],"type":"node"},{"attributes":{"lang":"jldoctest"},"tag":"codeblock","children":["julia> l = LSTM(3 => 5)\nRecur(\n  LSTMCell(3 => 5),                     # 190 parameters\n)         # Total: 5 trainable arrays, 190 parameters,\n          # plus 2 non-trainable, 10 parameters, summarysize 1.062 KiB.\n\njulia> l(rand(Float32, 3)) |> size\n(5,)\n\njulia> Flux.reset!(l);\n\njulia> l(rand(Float32, 3, 10)) |> size # batch size of 10\n(5, 10)\n"],"type":"node"},{"attributes":{"class":"warning"},"tag":"admonition","children":[{"attributes":{},"tag":"admonitiontitle","children":["Batch size changes"],"type":"node"},{"attributes":{},"tag":"admonitionbody","children":[{"attributes":{},"tag":"p","children":["Failing to call ",{"attributes":{},"tag":"code","children":["reset!"],"type":"node"}," when the input batch size changes can lead to unexpected behavior. See the example in ",{"attributes":{"reftype":"document","href":"@ref","title":"","document_id":"references/@ref"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["RNN"],"type":"node"}],"type":"node"},"."],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}