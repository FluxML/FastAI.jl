{"attributes":{"kind":"function","backlinks":[{"tag":"sourcefile","title":"Flux/src/losses/Losses.jl","docid":"sourcefiles/Flux/src/losses/Losses.jl"},{"tag":"sourcefile","title":"Flux/src/losses/functions.jl","docid":"sourcefiles/Flux/src/losses/functions.jl"}],"methods":[{"line":372,"file":"/home/runner/.julia/packages/Flux/KkC79/src/losses/functions.jl","method_id":"Flux.Losses.kldivergence_1","symbol_id":"Flux.Losses.kldivergence","filedoc":"sourcefiles/Flux/src/losses/functions.jl","signature":"kldivergence(ŷ, y; dims, agg, ϵ)"}],"name":"kldivergence","title":"kldivergence","symbol_id":"Flux.Losses.kldivergence","public":true,"module_id":"Flux.Losses"},"tag":"documentation","children":[{"attributes":{},"tag":"md","children":[{"attributes":{"lang":""},"tag":"codeblock","children":["kldivergence(ŷ, y; agg = mean, ϵ = eps(ŷ))\n"],"type":"node"},{"attributes":{},"tag":"p","children":["Return the ",{"attributes":{"href":"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence","title":""},"tag":"a","children":["Kullback-Leibler divergence"],"type":"node"}," between the given probability distributions."],"type":"node"},{"attributes":{},"tag":"p","children":["The KL divergence is a measure of how much one probability distribution is different from the other. It is always non-negative, and zero only when both the distributions are equal."],"type":"node"},{"attributes":{},"tag":"h1","children":["Example"],"type":"node"},{"attributes":{"lang":"jldoctest"},"tag":"codeblock","children":["julia> p1 = [1 0; 0 1]\n2×2 Matrix{Int64}:\n 1  0\n 0  1\n\njulia> p2 = fill(0.5, 2, 2)\n2×2 Matrix{Float64}:\n 0.5  0.5\n 0.5  0.5\n\njulia> Flux.kldivergence(p2, p1) ≈ log(2)\ntrue\n\njulia> Flux.kldivergence(p2, p1; agg = sum) ≈ 2log(2)\ntrue\n\njulia> Flux.kldivergence(p2, p2; ϵ = 0)  # about -2e-16 with the regulator\n0.0\n\njulia> Flux.kldivergence(p1, p2; ϵ = 0)  # about 17.3 with the regulator\nInf\n"],"type":"node"}],"type":"node"}],"type":"node"}