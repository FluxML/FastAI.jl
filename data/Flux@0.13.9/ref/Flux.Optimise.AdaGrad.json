{"attributes":{"kind":"struct","backlinks":[{"tag":"sourcefile","title":"Flux/optimise/optimisers.jl","docid":"Flux@0.13.9/src/optimise/optimisers.jl"},{"tag":"sourcefile","title":"Flux/deprecations.jl","docid":"Flux@0.13.9/src/deprecations.jl"},{"tag":"sourcefile","title":"Flux/Flux.jl","docid":"Flux@0.13.9/src/Flux.jl"},{"tag":"sourcefile","title":"Flux/optimise/Optimise.jl","docid":"Flux@0.13.9/src/optimise/Optimise.jl"}],"methods":[{"symbol_id":"Flux.Optimise.AdaGrad","module_id":"Flux.Optimise","file":"optimise/optimisers.jl","line":351,"signature":"(::Signature)"},{"symbol_id":"Flux.Optimise.AdaGrad","module_id":"Flux.Optimise","file":"optimise/optimisers.jl","line":347,"signature":"(::Signature)"},{"symbol_id":"Flux.Optimise.AdaGrad","module_id":"Flux.Optimise","file":"optimise/optimisers.jl","line":352,"signature":"(::Signature)"}],"package_id":"Flux@0.13.9","title":"AdaGrad","symbol_id":"Flux.Optimise.AdaGrad","exported":true,"module_id":"Flux.Optimise"},"tag":"documentation","children":[{"attributes":{"symbol":"Flux.Optimise.AdaGrad","line":328,"module":"Flux.Optimise","file":"optimise/optimisers.jl"},"tag":"docstring","children":[{"attributes":{},"tag":"md","children":[{"attributes":{"lang":""},"tag":"codeblock","children":["AdaGrad(η = 0.1, ϵ = 1.0e-8)\n"],"type":"node"},{"attributes":{},"tag":"p","children":[{"attributes":{"href":"http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf","title":""},"tag":"a","children":["AdaGrad"],"type":"node"}," optimizer. It has parameter specific learning rates based on how frequently it is updated. Parameters don't need tuning."],"type":"node"},{"attributes":{},"tag":"h1","children":["Parameters"],"type":"node"},{"attributes":{},"tag":"ul","children":[{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":["Learning rate (",{"attributes":{},"tag":"code","children":["η"],"type":"node"},"): Amount by which gradients are discounted before updating the weights."],"type":"node"}],"type":"node"}],"type":"node"},{"attributes":{},"tag":"h1","children":["Examples"],"type":"node"},{"attributes":{"lang":"julia"},"tag":"codeblock","children":[{"attributes":{},"tag":"julia","children":[{"attributes":{},"tag":"=","children":[{"attributes":{},"tag":"Identifier","children":["opt"],"type":"node"},{"attributes":{},"tag":"Whitespace","children":[" "],"type":"node"},{"attributes":{},"tag":"=","children":["="],"type":"node"},{"attributes":{},"tag":"Whitespace","children":[" "],"type":"node"},{"attributes":{},"tag":"call","children":[{"attributes":{"reftype":"symbol","document_id":"Flux@0.13.9/ref/Flux.Optimise.AdaGrad"},"tag":"reference","children":["AdaGrad"],"type":"node"},{"attributes":{},"tag":"(","children":["("],"type":"node"},{"attributes":{},"tag":")","children":[")"],"type":"node"}],"type":"node"}],"type":"node"},{"attributes":{},"tag":"NewlineWs","children":["\n"],"type":"node"},{"attributes":{},"tag":"NewlineWs","children":["\n"],"type":"node"},{"attributes":{},"tag":"=","children":[{"attributes":{},"tag":"Identifier","children":["opt"],"type":"node"},{"attributes":{},"tag":"Whitespace","children":[" "],"type":"node"},{"attributes":{},"tag":"=","children":["="],"type":"node"},{"attributes":{},"tag":"Whitespace","children":[" "],"type":"node"},{"attributes":{},"tag":"call","children":[{"attributes":{"reftype":"symbol","document_id":"Flux@0.13.9/ref/Flux.Optimise.AdaGrad"},"tag":"reference","children":["AdaGrad"],"type":"node"},{"attributes":{},"tag":"(","children":["("],"type":"node"},{"attributes":{},"tag":"Float","children":["0.001"],"type":"node"},{"attributes":{},"tag":")","children":[")"],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}