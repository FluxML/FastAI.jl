{"attributes":{"kind":"function","backlinks":[{"tag":"sourcefile","title":"Flux/src/optimise/train.jl","docid":"sourcefiles/Flux/src/optimise/train.jl"},{"tag":"sourcefile","title":"Flux/src/optimise/Optimise.jl","docid":"sourcefiles/Flux/src/optimise/Optimise.jl"}],"methods":[{"line":113,"file":"/home/runner/.julia/packages/Flux/js6mP/src/optimise/train.jl","method_id":"Flux.Optimise.train!_1","symbol_id":"Flux.Optimise.train!","filedoc":"sourcefiles/Flux/src/optimise/train.jl","signature":"train!(loss, ps::Zygote.Params, data, opt::Flux.Optimise.AbstractOptimiser; cb)"}],"name":"train!","title":"train!","symbol_id":"Flux.Optimise.train!","public":true,"module_id":"Flux.Optimise"},"tag":"documentation","children":[{"attributes":{},"tag":"md","children":[{"attributes":{"lang":""},"tag":"codeblock","children":["train!(loss, pars::Params, data, opt::AbstractOptimiser; [cb])\n"],"type":"node"},{"attributes":{},"tag":"p","children":["Uses a ",{"attributes":{},"tag":"code","children":["loss"],"type":"node"}," function and training ",{"attributes":{},"tag":"code","children":["data"],"type":"node"}," to improve the model's parameters according to a particular optimisation rule ",{"attributes":{},"tag":"code","children":["opt"],"type":"node"},"."],"type":"node"},{"attributes":{},"tag":"p","children":["For each ",{"attributes":{},"tag":"code","children":["d in data"],"type":"node"},", first the gradient of the ",{"attributes":{},"tag":"code","children":["loss"],"type":"node"}," is computed like this:"],"type":"node"},{"attributes":{"lang":""},"tag":"codeblock","children":["    gradient(() -> loss(d...), pars)  # if d isa Tuple\n    gradient(() -> loss(d), pars)     # otherwise\n"],"type":"node"},{"attributes":{},"tag":"p","children":["Here ",{"attributes":{},"tag":"code","children":["pars"],"type":"node"}," is produced by calling ",{"attributes":{"reftype":"document","href":"@ref","title":"","document_id":"references/@ref"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["Flux.params"],"type":"node"}],"type":"node"}," on your model. (Or just on the layers you want to train, like ",{"attributes":{},"tag":"code","children":["train!(loss, params(model[1:end-2]), data, opt)"],"type":"node"},".) This is the \"implicit\" style of parameter handling."],"type":"node"},{"attributes":{},"tag":"p","children":["Then, this gradient is used by optimizer ",{"attributes":{},"tag":"code","children":["opt"],"type":"node"}," to update the paramters:"],"type":"node"},{"attributes":{"lang":""},"tag":"codeblock","children":["    update!(opt, pars, grads)\n"],"type":"node"},{"attributes":{},"tag":"p","children":["The optimiser should be from the ",{"attributes":{"reftype":"document","href":"@ref","title":"","document_id":"references/@ref"},"tag":"reference","children":["Flux.Optimise"],"type":"node"}," module. Different optimisers can be combined using ",{"attributes":{"reftype":"document","href":"@ref","title":"","document_id":"references/@ref"},"tag":"reference","children":["Flux.Optimise.Optimiser"],"type":"node"},"."],"type":"node"},{"attributes":{},"tag":"p","children":["This training loop iterates through ",{"attributes":{},"tag":"code","children":["data"],"type":"node"}," once. You can use ",{"attributes":{"reftype":"document","href":"@ref","title":"","document_id":"references/@ref"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["@epochs"],"type":"node"}],"type":"node"}," to do this several times, or use for instance ",{"attributes":{},"tag":"code","children":["Iterators.repeat"],"type":"node"}," to make a longer ",{"attributes":{},"tag":"code","children":["data"],"type":"node"}," iterator."],"type":"node"},{"attributes":{},"tag":"h2","children":["Callbacks"],"type":"node"},{"attributes":{},"tag":"p","children":[{"attributes":{"reftype":"document","href":"@ref","title":"","document_id":"references/@ref"},"tag":"reference","children":["Callbacks"],"type":"node"}," are given with the keyword argument ",{"attributes":{},"tag":"code","children":["cb"],"type":"node"},". For example, this will print \"training\" every 10 seconds (using ",{"attributes":{"reftype":"document","href":"@ref","title":"","document_id":"references/@ref"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["Flux.throttle"],"type":"node"}],"type":"node"},"):"],"type":"node"},{"attributes":{"lang":""},"tag":"codeblock","children":["    train!(loss, params, data, opt, cb = throttle(() -> println(\"training\"), 10))\n"],"type":"node"},{"attributes":{},"tag":"p","children":["The callback can call ",{"attributes":{"reftype":"document","href":"@ref","title":"","document_id":"references/@ref"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["Flux.stop"],"type":"node"}],"type":"node"}," to interrupt the training loop."],"type":"node"},{"attributes":{},"tag":"p","children":["Multiple callbacks can be passed to ",{"attributes":{},"tag":"code","children":["cb"],"type":"node"}," as array."],"type":"node"}],"type":"node"}],"type":"node"}