{"attributes":{"kind":"function","backlinks":[{"tag":"sourcefile","title":"FluxTraining/callbacks/sanitycheck.jl","docid":"FluxTraining@0.3.8/src/callbacks/sanitycheck.jl"},{"tag":"document","title":"How to visualize data","docid":"FastAI@pr-283/doc/docs/notebooks/how_to_visualize.ipynb"},{"tag":"sourcefile","title":"Flux/Flux.jl","docid":"Flux@0.13.17/src/Flux.jl"},{"tag":"sourcefile","title":"Flux/functor.jl","docid":"Flux@0.13.17/src/functor.jl"},{"tag":"documentation","title":"Metric","docid":"FluxTraining@0.3.8/ref/FluxTraining.Metric"},{"tag":"document","title":"Performant data pipelines","docid":"FastAI@pr-283/doc/docs/background/datapipelines.md"},{"tag":"sourcefile","title":"FluxTraining/callbacks/callbacks.jl","docid":"FluxTraining@0.3.8/src/callbacks/callbacks.jl"},{"tag":"document","title":"Siamese image similarity","docid":"FastAI@pr-283/doc/docs/notebooks/siamese.ipynb"},{"tag":"sourcefile","title":"FastAI/FastAI.jl","docid":"FastAI@pr-283/src/FastAI.jl"},{"tag":"document","title":"Variational autoencoders","docid":"FastAI@pr-283/doc/docs/notebooks/vae.ipynb"},{"tag":"documentation","title":"Metrics","docid":"FluxTraining@0.3.8/ref/FluxTraining.Metrics"},{"tag":"documentation","title":"cpu","docid":"Flux@0.13.17/ref/Flux.cpu"},{"tag":"document","title":"Saving and loading models for inference","docid":"FastAI@pr-283/doc/docs/notebooks/serialization.ipynb"}],"methods":[{"symbol_id":"Flux.gpu","module_id":"Flux","file":"functor.jl","line":460,"signature":"(::Signature)"},{"symbol_id":"Flux.gpu","module_id":"Flux","file":"functor.jl","line":289,"signature":"(::Signature)"},{"symbol_id":"Flux.gpu","module_id":"Flux","file":"functor.jl","line":384,"signature":"(::Signature)"},{"symbol_id":"Flux.gpu","module_id":"Flux","file":"functor.jl","line":403,"signature":"(::Signature)"},{"symbol_id":"Flux.gpu","module_id":"Flux","file":"functor.jl","line":274,"signature":"(::Signature)"}],"package_id":"Flux@0.13.17","title":"gpu","symbol_id":"Flux.gpu","exported":true,"module_id":"Flux"},"tag":"documentation","children":[{"attributes":{"symbol":"Flux.gpu","line":243,"module":"Flux","file":"functor.jl"},"tag":"docstring","children":[{"attributes":{},"tag":"md","children":[{"attributes":{"lang":""},"tag":"codeblock","children":["gpu(m)\n"],"type":"node"},{"attributes":{},"tag":"p","children":["Copies ",{"attributes":{},"tag":"code","children":["m"],"type":"node"}," to the current GPU device (using current GPU backend), if one is available. If no GPU is available, it does nothing (but prints a warning the first time)."],"type":"node"},{"attributes":{},"tag":"p","children":["On arrays, this calls CUDA's ",{"attributes":{},"tag":"code","children":["cu"],"type":"node"},", which also changes arrays with Float64 elements to Float32 while copying them to the device (same for AMDGPU). To act on arrays within a struct, the struct type must be marked with ",{"attributes":{"href":"@ref","title":""},"tag":"a","children":[{"attributes":{},"tag":"code","children":["@functor"],"type":"node"}],"type":"node"},"."],"type":"node"},{"attributes":{},"tag":"p","children":["Use ",{"attributes":{"reftype":"symbol","href":"@ref","title":"","document_id":"Flux@0.13.17/ref/Flux.cpu"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["cpu"],"type":"node"}],"type":"node"}," to copy back to ordinary ",{"attributes":{},"tag":"code","children":["Array"],"type":"node"},"s. See also ",{"attributes":{"reftype":"symbol","href":"@ref","title":"","document_id":"Flux@0.13.17/ref/Flux.f32"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["f32"],"type":"node"}],"type":"node"}," and ",{"attributes":{"reftype":"symbol","href":"@ref","title":"","document_id":"Flux@0.13.17/ref/Flux.f16"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["f16"],"type":"node"}],"type":"node"}," to change element type only."],"type":"node"},{"attributes":{},"tag":"p","children":["See the ",{"attributes":{"href":"https://juliagpu.github.io/CUDA.jl/stable/usage/multigpu/","title":""},"tag":"a","children":["CUDA.jl docs"],"type":"node"}," to help identify the current device."],"type":"node"},{"attributes":{},"tag":"h1","children":["Example"],"type":"node"},{"attributes":{"lang":"julia-repl"},"tag":"codeblock","children":["julia> m = Dense(rand(2, 3))  # constructed with Float64 weight matrix\nDense(3 => 2)       # 8 parameters\n\njulia> typeof(m.weight)\nMatrix{Float64} (alias for Array{Float64, 2})\n\njulia> m_gpu = gpu(m)  # can equivalently be written m_gpu = m |> gpu\nDense(3 => 2)       # 8 parameters\n\njulia> typeof(m_gpu.weight)\nCUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}\n"],"type":"node"}],"type":"node"}],"type":"node"},{"attributes":{"symbol":"Flux.gpu","line":417,"module":"Flux","file":"functor.jl"},"tag":"docstring","children":[{"attributes":{},"tag":"md","children":[{"attributes":{"lang":""},"tag":"codeblock","children":["gpu(data::DataLoader)\n"],"type":"node"},{"attributes":{},"tag":"p","children":["Transforms a given ",{"attributes":{},"tag":"code","children":["DataLoader"],"type":"node"}," to apply ",{"attributes":{},"tag":"code","children":["gpu"],"type":"node"}," to each batch of data, when iterated over. (If no GPU is available, this does nothing.)"],"type":"node"},{"attributes":{},"tag":"h1","children":["Example"],"type":"node"},{"attributes":{"lang":"julia-repl"},"tag":"codeblock","children":["julia> dl = Flux.DataLoader((x = ones(2,10), y='a':'j'), batchsize=3)\n4-element DataLoader(::NamedTuple{(:x, :y), Tuple{Matrix{Float64}, StepRange{Char, Int64}}}, batchsize=3)\n  with first element:\n  (; x = 2×3 Matrix{Float64}, y = 3-element StepRange{Char, Int64})\n\njulia> first(dl)\n(x = [1.0 1.0 1.0; 1.0 1.0 1.0], y = 'a':1:'c')\n\njulia> c_dl = gpu(dl)\n4-element DataLoader(::MLUtils.MappedData{:auto, typeof(gpu), NamedTuple{(:x, :y), Tuple{Matrix{Float64}, StepRange{Char, Int64}}}}, batchsize=3)\n  with first element:\n  (; x = 2×3 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, y = 3-element StepRange{Char, Int64})\n\njulia> first(c_dl).x\n2×3 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n"],"type":"node"},{"attributes":{},"tag":"p","children":["For large datasets, this is preferred over moving all the data to the GPU before creating the ",{"attributes":{},"tag":"code","children":["DataLoader"],"type":"node"},", like this:"],"type":"node"},{"attributes":{"lang":"julia-repl"},"tag":"codeblock","children":["julia> Flux.DataLoader((x = ones(2,10), y=2:11) |> gpu, batchsize=3)\n4-element DataLoader(::NamedTuple{(:x, :y), Tuple{CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, UnitRange{Int64}}}, batchsize=3)\n  with first element:\n  (; x = 2×3 CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, y = 3-element UnitRange{Int64})\n"],"type":"node"},{"attributes":{"class":"warning"},"tag":"admonition","children":[{"attributes":{},"tag":"admonitiontitle","children":["Warning"],"type":"node"},{"attributes":{},"tag":"admonitionbody","children":[{"attributes":{},"tag":"p","children":["This only works if ",{"attributes":{},"tag":"code","children":["gpu"],"type":"node"}," is applied directly to the ",{"attributes":{},"tag":"code","children":["DataLoader"],"type":"node"},". While ",{"attributes":{},"tag":"code","children":["gpu"],"type":"node"}," acts recursively on Flux models and many basic Julia structs, it will not work on (say) a tuple of ",{"attributes":{},"tag":"code","children":["DataLoader"],"type":"node"},"s."],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}