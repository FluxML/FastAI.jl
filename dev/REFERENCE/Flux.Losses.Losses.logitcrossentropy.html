<HTML><head><title>Flux.Losses.logitcrossentropy</title><link href=../template/ansi.css rel=stylesheet ></link><link href=../template/hugobook.css rel=stylesheet ></link><meta content=Type=text/html; charset=utf-8 http-equiv=Content-Type ></meta></head><body><input onclick=toggleMenu() id=menu-control class=hidden toggle type=checkbox ></input><input id=toc-control type=checkbox class=hidden toggle ></input><main class=container flex ><aside id=menu-container class=book-menu ><nav class=book-menu-content ><h2 id=title >FastAI.jl</h2><div id=sidebar ><div class=doctree ><body><ul><li><p><a href=../README.md.html title= >README</a></p></li><li><p><a href=../docs/setup.md.html title= >Setup</a></p></li><li><p><a href=../docs/quickstart.md.html title= >Quickstart</a></p></li><li><p>Tutorials</p><ul><li><p><a href=../docs/introduction.md.html title= >Introduction</a></p></li><li><p><a href=../docs/data_containers.md.html title= >Data containers</a></p></li><li><p><a href=../docs/learning_methods.md.html title= >Learning methods</a></p></li><li><p><a href=../notebooks/serialization.ipynb.html title= >Saving and loading models</a></p></li><li><p><a href=../tutorials/presizing.ipynb.html title= >Presizing vision datasets</a></p></li></ul></li><li><p>Learning tasks</p><ul><li><p><a href=../docs/methods/imageclassification.md.html title= >Image classification</a></p></li><li><p><a href=../notebooks/imagesegmentation.ipynb.html title= >Image segmentation</a></p></li><li><p><a href=../notebooks/keypointregression.ipynb.html title= >Keypoint regression</a></p></li></ul></li><li><p>How To</p><ul><li><p><a href=../notebooks/fitonecycle.ipynb.html title= >Train a model from scratch</a></p></li><li><p><a href=../notebooks/finetune.ipynb.html title= >Finetune a pretrained model</a></p></li><li><p><a href=../notebooks/lrfind.ipynb.html title= >Find a good learning rate</a></p></li><li><p><a href=../docs/howto/augmentvision.md.html title= >Augment vision data</a></p></li><li><p><a href=../notebooks/how_to_visualize.ipynb.html title= >Visualize data</a></p></li><li><p><a href=../docs/howto/logtensorboard.md.html title= >Log to TensorBoard</a></p></li></ul></li><li><p>Reference</p><ul><li><p><a href=../REFERENCE.html title= >Docstrings</a></p></li><li><p><a href=../docs/interfaces.md.html title= >Interfaces</a></p></li><li><p><a href=../docs/api.md.html title= >API</a></p></li><li><p><a href=../docs/glossary.md.html title= >Glossary</a></p></li></ul></li><li><p>Background</p><ul><li><p><a href=../docs/background/datapipelines.md.html title= >Performant data pipelines</a></p></li></ul></li></ul></body></div></div></nav></aside><div class=book-page ><header class=book-header ></header><article><article><h1 id=fluxlosseslogitcrossentropy ><code>Flux.Losses.logitcrossentropy</code></h1><div class=docs ><div typesig=Tuple{Any, Any} module=Flux.Losses linenumber=209 binding=Flux.Losses.logitcrossentropy path=/home/runner/.julia/packages/Flux/0c9kI/src/losses/functions.jl class=doc ><pre lang= ><code>logitcrossentropy(ŷ, y; dims = 1, agg = mean)
</code></pre><p>Return the cross entropy calculated by</p><pre lang= ><code>agg(-sum(y .* logsoftmax(ŷ; dims); dims))
</code></pre><p>This is mathematically equivalent to <code>crossentropy(softmax(ŷ), y)</code>,
but is more numerically stable than using functions <a href=./@ref.html title= ><code>crossentropy</code></a>
and <a href=./@ref.html title= ><code>softmax</code></a> separately.</p><p>See also: <a href=./@ref.html title= ><code>binarycrossentropy</code></a>, <a href=./@ref.html title= ><code>logitbinarycrossentropy</code></a>, <a href=./@ref.html title= ><code>label_smoothing</code></a>.</p><h1 id=example >Example</h1><pre lang=jldoctest ><code>julia&gt; y_label = Flux.onehotbatch(collect(&quot;abcabaa&quot;), &#39;a&#39;:&#39;c&#39;)
3×7 Flux.OneHotArray{3,2,Vector{UInt32}}:
 1  0  0  1  0  1  1
 0  1  0  0  1  0  0
 0  0  1  0  0  0  0

julia&gt; y_model = reshape(vcat(-9:0, 0:9, 7.5f0), 3, 7)
3×7 Matrix{Float32}:
 -9.0  -6.0  -3.0  0.0  2.0  5.0  8.0
 -8.0  -5.0  -2.0  0.0  3.0  6.0  9.0
 -7.0  -4.0  -1.0  1.0  4.0  7.0  7.5

julia&gt; Flux.logitcrossentropy(y_model, y_label)
1.5791205f0

julia&gt; Flux.crossentropy(softmax(y_model), y_label)
1.5791197f0
</code></pre></div><h2 id=methods >Methods</h2># 1 method for generic function <b>logitcrossentropy</b>:<ul><li> logitcrossentropy(ŷ, y; <i>dims, agg</i>) in Flux.Losses at <a href="file:///home/runner/.julia/packages/Flux/0c9kI/src/losses/functions.jl" target="_blank">/home/runner/.julia/packages/Flux/0c9kI/src/losses/functions.jl:243</a></li> </ul></div></article></article><footer class=book-footer ></footer></div><aside class=book-toc ><nav id=toc class=book-toc-content ><ul><li><a href=#fluxlosseslogitcrossentropy >Flux.Losses.logitcrossentropy</a><ul></ul></li></ul></nav></aside></main></body></HTML>