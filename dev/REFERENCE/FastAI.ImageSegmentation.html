<HTML><head><title>FastAI.ImageSegmentation</title><link href=../template/hugobook.css rel=stylesheet ></link><link href=../template/ansi.css rel=stylesheet ></link><meta content=Type=text/html; charset=utf-8 http-equiv=Content-Type ></meta></head><body><input onclick=toggleMenu() id=menu-control class=hidden toggle type=checkbox ></input><input id=toc-control type=checkbox class=hidden toggle ></input><main class=container flex ><aside id=menu-container class=book-menu ><nav class=book-menu-content ><h2 id=title >FastAI.jl</h2><div id=sidebar ><div class=doctree ><body><ul><li><p><a href=../README.md.html title= >README</a></p></li><li><p><a href=../docs/setup.md.html title= >Setup</a></p></li><li><p><a href=../docs/quickstart.md.html title= >Quickstart</a></p></li><li><p>Tutorials</p><ul><li><p><a href=../docs/introduction.md.html title= >Introduction</a></p></li><li><p><a href=../docs/data_containers.md.html title= >Data containers</a></p></li><li><p><a href=../docs/learning_methods.md.html title= >Learning methods</a></p></li><li><p><a href=../docs/notebooks/serialization.ipynb.html title= >Saving and loading models</a></p></li><li><p><a href=../docs/tutorials/presizing.ipynb.html title= >Presizing vision datasets</a></p></li></ul></li><li><p>Learning tasks</p><ul><li><p><a href=../docs/methods/imageclassification.md.html title= >Image classification</a></p></li><li><p><a href=../docs/notebooks/imagesegmentation.ipynb.html title= >Image segmentation</a></p></li><li><p><a href=../docs/notebooks/keypointregression.ipynb.html title= >Keypoint regression</a></p></li></ul></li><li><p>How To</p><ul><li><p><a href=../docs/notebooks/fitonecycle.ipynb.html title= >Train a model from scratch</a></p></li><li><p><a href=../docs/notebooks/finetune.ipynb.html title= >Finetune a pretrained model</a></p></li><li><p><a href=../docs/notebooks/lrfind.ipynb.html title= >Find a good learning rate</a></p></li><li><p><a href=../docs/howto/augmentvision.md.html title= >Augment vision data</a></p></li><li><p><a href=../docs/notebooks/how_to_visualize.ipynb.html title= >Visualize data</a></p></li><li><p><a href=../docs/howto/logtensorboard.md.html title= >Log to TensorBoard</a></p></li></ul></li><li><p>Reference</p><ul><li><p><a href=../REFERENCE.html title= >Docstrings</a></p></li><li><p><a href=../docs/interfaces.md.html title= >Interfaces</a></p></li><li><p><a href=../docs/api.md.html title= >API</a></p></li><li><p><a href=../docs/glossary.md.html title= >Glossary</a></p></li></ul></li><li><p>Background</p><ul><li><p><a href=../docs/background/datapipelines.md.html title= >Performant data pipelines</a></p></li></ul></li></ul></body></div></div></nav></aside><div class=book-page ><header class=book-header ></header><article><article><h1 id=fastaiimagesegmentation ><code>FastAI.ImageSegmentation</code></h1><div class=docs ><div typesig=Union{} module=FastAI linenumber=2 binding=FastAI.ImageSegmentation path=/home/runner/work/FastAI.jl/FastAI.jl/src/methods/imagesegmentation.jl class=doc fields=Dict{Symbol, Any}() ><pre lang= ><code>ImageSegmentation(classes[, sz; kwargs...]) &lt;: LearningMethod
</code></pre><p>A learning method for semantic image segmentation:
given an image and a set of classes, determine <em>for every pixel</em> which
class it falls into. For example, assign some pixels the class “road” and
others the class “background”.</p><p>Images are resized and cropped to <code>sz</code> (see <a href=FastAI.ProjectiveTransforms.html ><code>ProjectiveTransforms</code></a>)
and preprocessed using <a href=FastAI.ImagePreprocessing.html ><code>ImagePreprocessing</code></a>.
<code>classes</code> is a vector of the class labels.</p><h2 id=keyword-arguments >Keyword arguments</h2><ul><li><p><code>aug_projection::</code><a href=DataAugmentation.Transform.html ><code>Transform</code></a><code> = Identity()</code>: Projective
augmentation to apply during training. See
<a href=FastAI.ProjectiveTransforms.html ><code>ProjectiveTransforms</code></a> and <a href=FastAI.augs_projection.html ><code>augs_projection</code></a>.</p></li><li><p><code>aug_image::</code><a href=DataAugmentation.Transform.html ><code>Transform</code></a><code> = Identity()</code>: Other image
augmentation to apply to cropped image during training. See
<a href=FastAI.ImagePreprocessing.html ><code>ImagePreprocessing</code></a> and <a href=FastAI.augs_lighting.html ><code>augs_lighting</code></a>.</p></li><li><p><code>downscale::Int = 0</code>: Downscale the masks by a factor of <code>2^downscale</code>, i.e. for
a value of <code>0</code>, the images and masks will have the same size and for a value of
<code>1</code>, the target masks will have half the size of the input images.</p></li><li><p><code>buffered = true</code>: Whether to use inplace transformations when projecting and
preprocessing image. Reduces memory usage.</p></li><li><p><code>means = IMAGENET_MEANS</code> and <code>stds = IMAGENET_STDS</code>: Color channel means and
standard deviations to use for normalizing the image.</p></li></ul><h2 id=learning-method-reference >Learning method reference</h2><p>This learning method implements the following interfaces:</p><ul class=tight ><li><p>Core interface</p></li><li><p>Plotting interface</p></li><li><p>Training interface</p></li><li><p>Testing interface</p></li></ul><h3 id=types >Types</h3><p>Types of data throughout the DLPipelines.jl pipeline.</p><ul><li><p><strong><code>sample</code></strong>: <code>Tuple</code>/<code>NamedTuple</code> of</p><ul><li><p><strong><code>input</code></strong><code>::AbstractArray{2, T}</code>: A 2-dimensional array with dimensions (height, width)
and elements of a color or number type. <code>Matrix{RGB{Float32}}</code> is a 2D RGB image,
while <code>Array{Float32, 3}</code> would be a 3D grayscale image. If element type is a number
it should fall between <code>0</code> and <code>1</code>. It is recommended to use the <code>Gray</code> color type
to represent grayscale images.</p></li><li><p><strong><code>target</code></strong><code>::AbstractArray{2, &lt;:Integer}</code>: A mask with the integer at each pixel giving
a class index into <code>classes</code>.</p></li></ul></li><li><p><strong><code>x</code></strong><code>::AbstractArray{Float32, 3}</code>: a normalized array with dimensions
<code>(height, width, color channels)</code>. See <a href=FastAI.ImagePreprocessing.html ><code>ImagePreprocessing</code></a> for additional information.</p></li><li><p><strong><code>y</code></strong><code>::AbstractArray{Float32, 3}</code>: a one-hot encoded array with dimensions
<code>(height/2^downscale, width/2^downscale, length(classes))</code> with the class index of the
corresponding pixel set to <code>1.</code> and other values to <code>0.</code>.</p></li><li><p><strong><code>ŷ</code></strong><code>::AbstractVector{Float32}</code>: vector of predicted class scores.</p></li></ul><h3 id=model-sizes >Model sizes</h3><p>Array sizes that compatible models must conform to.</p><ul><li><p>Full model: <code>(sz..., 3, batch) -&gt; ((sz ./ 2^downscale)..., batch)</code></p></li><li><p>Backbone model: <code>(sz..., 3, batch) -&gt; ((sz ./ f)..., ch, batch)</code> where <code>f</code>
is a downscaling factor <code>f = 2^k</code>. <code>methodmodel</code> will build a U-Net model
from the backbone by inserting an upscaling layer and skip connections for
every downscaling layer in the original network.</p></li></ul><p>It is recommended <em>not</em> to use <a href=NNlib.softmax.html ><code>NNlib.softmax</code></a> as the final layer for custom models,
as for numerical stability, the loss function takes in the logits.</p></div></div></article></article><footer class=book-footer ></footer></div><aside class=book-toc ><nav id=toc class=book-toc-content ><ul><li><a href=#fastaiimagesegmentation >FastAI.ImageSegmentation</a><ul></ul></li></ul></nav></aside></main></body></HTML>