{"attributes":{"kind":"function","backlinks":[{"tag":"sourcefile","title":"Flux/src/losses/Losses.jl","docid":"sourcefiles/Flux/src/losses/Losses.jl"},{"tag":"sourcefile","title":"Flux/src/losses/functions.jl","docid":"sourcefiles/Flux/src/losses/functions.jl"}],"methods":[{"line":214,"file":"/home/runner/.julia/packages/Flux/18YZE/src/losses/functions.jl","method_id":"Flux.Losses.crossentropy_1","symbol_id":"Flux.Losses.crossentropy","signature":"crossentropy(ŷ, y; dims, agg, ϵ)"}],"name":"crossentropy","title":"crossentropy","symbol_id":"Flux.Losses.crossentropy","public":true,"module_id":"Flux.Losses"},"tag":"documentation","children":[{"attributes":{},"tag":"md","children":[{"attributes":{"lang":""},"tag":"codeblock","children":["crossentropy(ŷ, y; dims = 1, ϵ = eps(ŷ), agg = mean)\n"],"type":"node"},{"attributes":{},"tag":"p","children":["Return the cross entropy between the given probability distributions;",{"attributes":{},"tag":"br","children":[],"type":"node"},"calculated as"],"type":"node"},{"attributes":{"lang":""},"tag":"codeblock","children":["agg(-sum(y .* log.(ŷ .+ ϵ); dims))\n"],"type":"node"},{"attributes":{},"tag":"p","children":["Cross entropy is typically used as a loss in multi-class classification,",{"attributes":{},"tag":"br","children":[],"type":"node"},"in which case the labels ",{"attributes":{},"tag":"code","children":["y"],"type":"node"}," are given in a one-hot format.",{"attributes":{},"tag":"br","children":[],"type":"node"},{"attributes":{},"tag":"code","children":["dims"],"type":"node"}," specifies the dimension (or the dimensions) containing the class probabilities.",{"attributes":{},"tag":"br","children":[],"type":"node"},"The prediction ",{"attributes":{},"tag":"code","children":["ŷ"],"type":"node"}," is supposed to sum to one across ",{"attributes":{},"tag":"code","children":["dims"],"type":"node"},",",{"attributes":{},"tag":"br","children":[],"type":"node"},"as would be the case with the output of a ",{"attributes":{"reftype":"document","href":"@ref","title":"","document_id":"references/@ref"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["softmax"],"type":"node"}],"type":"node"}," operation."],"type":"node"},{"attributes":{},"tag":"p","children":["For numerical stability, it is recommended to use ",{"attributes":{"reftype":"document","href":"@ref","title":"","document_id":"references/@ref"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["logitcrossentropy"],"type":"node"}],"type":"node"},{"attributes":{},"tag":"br","children":[],"type":"node"},"rather than ",{"attributes":{},"tag":"code","children":["softmax"],"type":"node"}," followed by ",{"attributes":{},"tag":"code","children":["crossentropy"],"type":"node"}," ."],"type":"node"},{"attributes":{},"tag":"p","children":["Use ",{"attributes":{"reftype":"document","href":"@ref","title":"","document_id":"references/@ref"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["label_smoothing"],"type":"node"}],"type":"node"}," to smooth the true labels as preprocessing before",{"attributes":{},"tag":"br","children":[],"type":"node"},"computing the loss."],"type":"node"},{"attributes":{},"tag":"p","children":["See also: ",{"attributes":{"reftype":"document","href":"@ref","title":"","document_id":"references/@ref"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["logitcrossentropy"],"type":"node"}],"type":"node"},", ",{"attributes":{"reftype":"document","href":"@ref","title":"","document_id":"references/@ref"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["binarycrossentropy"],"type":"node"}],"type":"node"},", ",{"attributes":{"reftype":"document","href":"@ref","title":"","document_id":"references/@ref"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["logitbinarycrossentropy"],"type":"node"}],"type":"node"},"."],"type":"node"},{"attributes":{},"tag":"h1","children":["Example"],"type":"node"},{"attributes":{"lang":"jldoctest"},"tag":"codeblock","children":["julia> y_label = Flux.onehotbatch([0, 1, 2, 1, 0], 0:2)\n3×5 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n 1  ⋅  ⋅  ⋅  1\n ⋅  1  ⋅  1  ⋅\n ⋅  ⋅  1  ⋅  ⋅\n\njulia> y_model = softmax(reshape(-7:7, 3, 5) .* 1f0)\n3×5 Matrix{Float32}:\n 0.0900306  0.0900306  0.0900306  0.0900306  0.0900306\n 0.244728   0.244728   0.244728   0.244728   0.244728\n 0.665241   0.665241   0.665241   0.665241   0.665241\n\njulia> sum(y_model; dims=1)\n1×5 Matrix{Float32}:\n 1.0  1.0  1.0  1.0  1.0\n\njulia> Flux.crossentropy(y_model, y_label)\n1.6076053f0\n\njulia> 5 * ans ≈ Flux.crossentropy(y_model, y_label; agg=sum)\ntrue\n\njulia> y_smooth = Flux.label_smoothing(y_label, 0.15f0)\n3×5 Matrix{Float32}:\n 0.9   0.05  0.05  0.05  0.9\n 0.05  0.9   0.05  0.9   0.05\n 0.05  0.05  0.9   0.05  0.05\n\njulia> Flux.crossentropy(y_model, y_smooth)\n1.5776052f0\n"],"type":"node"}],"type":"node"}],"type":"node"}