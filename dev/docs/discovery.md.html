<HTML><head><title>Discovery</title><script src=../template/highlight.min.js ></script><script src=../template/julia.min.js ></script><script src=../template/loadhighlightjs.js ></script><link href=../template/ansi.css rel=stylesheet ></link><link href=../template/hugobook.css rel=stylesheet ></link><meta content=Type=text/html; charset=utf-8 http-equiv=Content-Type ></meta><meta name=viewport content=width=device-width, initial-scale=1 ></meta></head><body><input onclick=toggleMenu() id=menu-control class=hidden toggle type=checkbox ></input><input id=toc-control type=checkbox class=hidden toggle ></input><main class=container flex ><aside id=menu-container class=book-menu ><nav class=book-menu-content ><h2 id=title >FastAI.jl</h2><div id=sidebar ><div class=doctree ><body><ul><li><p><a href=../README.md.html title= >README</a></p></li><li><p><a href=setup.md.html title= >Setup</a></p></li><li><p><a href=../notebooks/quickstart.ipynb.html title= >Quickstart</a></p></li><li><p>Tutorials</p><ul><li><p>Beginner</p><ul><li><p><a href=introduction.md.html title= >Introduction</a></p></li><li><p><a href=discovery.md.html title= >Discovery</a></p></li></ul></li><li><p>Intermediate</p><ul><li><p><a href=../notebooks/imagesegmentation.ipynb.html title= >Image segmentation</a></p></li><li><p><a href=../notebooks/keypointregression.ipynb.html title= >Keypoint regression</a></p></li><li><p><a href=../notebooks/tabularclassification.ipynb.html title= >Tabular classification</a></p></li><li><p><a href=data_containers.md.html title= >Data containers</a></p></li><li><p><a href=../notebooks/serialization.ipynb.html title= >Saving and loading models</a></p></li></ul></li><li><p>Advanced</p><ul><li><p><a href=../notebooks/presizing.ipynb.html title= >Presizing vision datasets</a></p></li><li><p><a href=../notebooks/vae.ipynb.html title= >Unsupervised learning</a></p></li><li><p><a href=learning_methods.md.html title= >Custom Learning methods</a></p></li></ul></li></ul></li><li><p>How To</p><ul><li><p><a href=../notebooks/training.ipynb.html title= >Train your model</a></p></li><li><p><a href=howto/augmentvision.md.html title= >Augment vision data</a></p></li><li><p><a href=howto/logtensorboard.md.html title= >Log to TensorBoard</a></p></li></ul></li><li><p>Reference</p><ul><li><p><a href=../REFERENCE.html title= >Docstrings</a></p></li><li><p><a href=fastai_api_comparison.md.html title= >fastai API comparison</a></p></li><li><p><a href=interfaces.md.html title= >Extension APIs</a></p></li><li><p><a href=glossary.md.html title= >Glossary</a></p></li></ul></li><li><p>Background</p><ul><li><p><a href=background/blocksencodings.md.html title= >Blocks and encodings</a></p></li><li><p><a href=background/datapipelines.md.html title= >Performant data pipelines</a></p></li></ul></li></ul></body></div></div></nav></aside><div class=book-page ><header class=book-header ></header><article><h1 id=discovery >Discovery</h1><p>As you may have seen in <a href=./introduction.md.html title= >the introduction</a>, FastAI.jl makes it possible to train models in just 5 lines of code. However, if you have a task in mind, you need to know what datasets you can train on and if there are convenience learning method constructors. For example, the introduction loads the <code>&quot;imagenette2-160&quot;</code> dataset and uses <a href=../REFERENCE/FastAI.Vision.Vision.ImageClassificationSingle.html ><code>FastAI.Vision.ImageClassificationSingle</code></a> to construct a learning method. Now what if, instead of classifying an image into one class, we want to classify every single pixel into a class (semantic segmentation)? Now we need a dataset with pixel-level annotations and a learning method that can process those segmentation masks.</p><p>For finding both, we can make use of <code>Block</code>s. A <code>Block</code> represents a kind of data, for example images, labels or keypoints. For supervised learning tasks, we have an input block and a target block. If we wanted to classify whether 2D images contain a cat or a dog, we could use the blocks <code>(Image{2}(), Label([&quot;cat&quot;, &quot;dog&quot;]))</code>, while for semantic segmentation, we’ll have an input <code>Image</code> block and a target <a href=../REFERENCE/FastAI.Mask.html ><code>Mask</code></a> block.</p><h2 id=finding-a-dataset >Finding a dataset</h2><p>To find a dataset with compatible samples, we can pass the types of these blocks to <a href=../REFERENCE/FastAI.Datasets.Datasets.finddatasets.html ><code>finddatasets</code></a> which will return a list of dataset names and recipes to load them in a suitable way.</p><div class=cellcontainer cell=main lang=julia ><pre class=codecell cell=main lang=julia ><code>using FastAI
import FastAI: Image
finddatasets(blocks=(Image, Mask))
</code></pre><pre class=coderesult ><code>2-element Vector{Pair{String, FastAI.Datasets.DatasetRecipe}}:
      "camvid" => FastAI.Vision.ImageSegmentationFolders("images", "labels", "codes.txt")
 "camvid_tiny" => FastAI.Vision.ImageSegmentationFolders("images", "labels", "codes.txt")</code></pre></div><p>We can see that the <code>&quot;camvid_tiny&quot;</code> dataset can be loaded so that each sample is a pair of an image and a segmentation mask. Let’s use <a href=../REFERENCE/FastAI.Datasets.Datasets.loaddataset.html ><code>loaddataset</code></a> to load a <a href=./data_containers.md.html title= >data container</a> and concrete blocks.</p><div class=cellcontainer cell=main lang=julia ><pre class=codecell cell=main lang=julia ><code>data, blocks = loaddataset(&quot;camvid_tiny&quot;, (Image, Mask))
</code></pre><pre class=codeoutput ><code>
7-Zip (a) [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21
p7zip Version 16.02 (locale=C.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU E5-2673 v4 @ 2.30GHz (406F1),ASM,AES-NI)


Extracting archive: 
--
Path = 
Type = tar
Code Page = UTF-8

Everything is Ok

Folders: 3
Files: 201
Size:       2315086
Compressed: 107008
</code></pre><pre class=coderesult ><code>((mapobs(loadfile, ["/home/runner/.julia/datadeps/fastai-camvid_tiny/camvid_tiny/images/0001TP_0067…]), mapobs(#76, ["/home/runner/.julia/datadeps/fastai-camvid_tiny/camvid_tiny/labels/0001TP_0067…])), (Image{2}(), Mask{2, String}(["Animal", "Archway", "Bicyclist", "Bridge", "Building", "Car", "CartLuggagePram", "Child", "Column_Pole", "Fence"  …  "SUVPickupTruck", "TrafficCone", "TrafficLight", "Train", "Tree", "Truck_Bus", "Tunnel", "VegetationMisc", "Void", "Wall"])))</code></pre></div><p>As with every data container, we can load a sample using <code>getobs</code> which gives us a tuple of an image and a segmentation mask.</p><div class=cellcontainer cell=main lang=julia ><pre class=codecell cell=main lang=julia ><code>image, mask = sample = getobs(data, 1)
size.(sample), eltype.(sample)
</code></pre><pre class=coderesult ><code>(((96, 128), (96, 128)), (ColorTypes.RGB{FixedPointNumbers.N0f8}, String))</code></pre></div><p><code>loaddataset</code> also returned <code>blocks</code> which are the concrete <code>Block</code> instances for the dataset. We passed in <em>types</em> of blocks (<code>(Image, Mask)</code>) and get back <em>instances</em> since the specifics of some blocks depend on the dataset. For example, the returned target block carries the labels for every class that a pixel can belong to.</p><div class=cellcontainer cell=main lang=julia ><pre class=codecell cell=main lang=julia ><code>inputblock, targetblock = blocks
targetblock
</code></pre><pre class=coderesult ><code>Mask{2, String}(["Animal", "Archway", "Bicyclist", "Bridge", "Building", "Car", "CartLuggagePram", "Child", "Column_Pole", "Fence"  …  "SUVPickupTruck", "TrafficCone", "TrafficLight", "Train", "Tree", "Truck_Bus", "Tunnel", "VegetationMisc", "Void", "Wall"])</code></pre></div><p>With these <code>blocks</code>, we can also validate a sample of data using <a href=../REFERENCE/FastAI.checkblock.html ><code>checkblock</code></a> which is useful as a sanity check when using custom data containers.</p><div class=cellcontainer cell=main lang=julia ><pre class=codecell cell=main lang=julia ><code>checkblock((inputblock, targetblock), (image, mask))
</code></pre><pre class=coderesult ><code>true</code></pre></div><h3 id=summary >Summary</h3><p>In short, if you have a learning task in mind and want to load a dataset for that task, then</p><ul><li><p>define the types of input and target block, e.g. <code>blocktypes = (Image, Label)</code>,</p></li><li><p>use <a href=../REFERENCE/FastAI.Datasets.Datasets.finddatasets.html ><code>finddatasets</code></a><code>(blocks=blocktypes)</code> to find compatbile datasets; and</p></li><li><p>run <a href=../REFERENCE/FastAI.Datasets.Datasets.loaddataset.html ><code>loaddataset</code></a><code>(datasetname, blocktypes)</code> to load a data container and the concrete blocks</p></li></ul><h3 id=exercises >Exercises</h3><ul><li><p>Find and load a dataset for multi-label image classification. (Hint: the block for multi-category outputs is called <code>LabelMulti</code>).</p></li><li><p>List all datasets with <code>Image</code> as input block and any target block. (Hint: the supertype of all types is <code>Any</code>)</p></li></ul><h2 id=finding-a-learning-method >Finding a learning method</h2><p>Armed with a dataset, we can go to the next step: creating a learning method. Since we already have blocks defined, this amounts to defining the encodings that are applied to the data before it is used in training. Here, FastAI.jl already defines some convenient constructors for learning methods and you can find them with <a href=../REFERENCE/FastAI.findlearningmethods.html ><code>findlearningmethods</code></a>. Here we can pass in either block types as above or the block instances we got from <code>loaddataset</code>.</p><div class=cellcontainer cell=main lang=julia ><pre class=codecell cell=main lang=julia ><code>findlearningmethods(blocks)
</code></pre><pre class=coderesult ><code>1-element Vector{typeof(ImageSegmentation)}:
 ImageSegmentation (generic function with 3 methods)</code></pre></div><p>Looks like we can use the <a href=../REFERENCE/FastAI.Vision.Vision.ImageSegmentation.html ><code>FastAI.Vision.ImageSegmentation</code></a> function to create a learning method for our learning task. Every function returned can be called with <code>blocks</code> and, optionally, some keyword arguments for customization.</p><div class=cellcontainer cell=main lang=julia ><pre class=codecell cell=main lang=julia ><code>method = ImageSegmentation(blocks; size = (64, 64))
</code></pre><pre class=coderesult ><code>SupervisedMethod(Image{2} -> Mask{2, String})</code></pre></div><p>And that’s the basic workflow for getting started with a supervised task.</p><h3 id=exercises >Exercises</h3><ul><li><p>Find all learning method functions with images as inputs.</p></li></ul></article><footer class=book-footer ></footer></div><aside class=book-toc ><nav id=toc class=book-toc-content ><ul><li><a href=#discovery >Discovery</a><ul><li><a href=#finding-a-dataset >Finding a dataset</a><ul><li><a href=#summary >Summary</a><ul></ul></li><li><a href=#exercises >Exercises</a><ul></ul></li></ul></li><li><a href=#finding-a-learning-method >Finding a learning method</a><ul><li><a href=#exercises >Exercises</a><ul></ul></li></ul></li></ul></li></ul></nav></aside></main></body></HTML>