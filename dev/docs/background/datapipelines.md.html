<HTML><head><title>Performant data pipelines</title><link href=../../template/hugobook.css rel=stylesheet ></link><meta content=Type=text/html; charset=utf-8 http-equiv=Content-Type ></meta></head><body><input onclick=toggleMenu() id=menu-control class=hidden toggle type=checkbox ></input><input id=toc-control type=checkbox class=hidden toggle ></input><main class=container flex ><aside id=menu-container class=book-menu ><nav class=book-menu-content ><h2 id=title >FastAI.jl</h2><div id=sidebar ><div class=doctree ><body><ul><li><p><a href=../../README.md.html title= >README</a></p></li><li><p><a href=../setup.md.html title= >Setup</a></p></li><li><p><a href=../quickstart.md.html title= >Quickstart</a></p></li><li><p>Tutorials</p><ul><li><p><a href=../introduction.md.html title= >Introduction</a></p></li><li><p><a href=../data_containers.md.html title= >Data containers</a></p></li><li><p><a href=../learning_methods.md.html title= >Learning methods</a></p></li></ul></li><li><p>Learning tasks</p><ul><li><p><a href=../methods/imageclassification.md.html title= >Image classification</a></p></li><li><p><a href=../methods/imagesegmentation.md.html title= >Image segmentation</a></p></li></ul></li><li><p>How To</p><ul><li><p><a href=../notebooks/fitonecycle.ipynb.html title= >Train a model from scratch</a></p></li><li><p><a href=../notebooks/finetune.ipynb.html title= >Finetune a pretrained model</a></p></li><li><p><a href=../notebooks/lrfind.ipynb.html title= >Find a good learning rate</a></p></li></ul></li><li><p>Reference</p><ul><li><p><a href=../../REFERENCE.html title= >Docstrings</a></p></li><li><p><a href=../interfaces.md.html title= >Interfaces</a></p></li><li><p><a href=../api.md.html title= >API</a></p></li><li><p><a href=../glossary.md.html title= >Glossary</a></p></li></ul></li><li><p>Background</p><ul><li><p><a href=datapipelines.md.html title= >Performant data pipelines</a></p></li></ul></li></ul></body></div></div></nav></aside><div class=book-page ><header class=book-header ></header><article><h1 id=performant-data-pipelines >Performant data pipelines</h1><p><em>Explainer on how data pipelines in FastAI.jl are made fast and how to make yours fast.</em></p><p>When training large deep learning models on a GPU we clearly want wait as short as possible for the training to complete. The hardware bottleneck is usually the GPU power you have available to you. This means that data pipelines need to be fast enough to keep the GPU at 100% utilization, that is, keep it from “starving”. Reducing the time the GPU has to wait for the next batch of data directly lowers the training time until the GPU is fully utilized. There are other ways to reduce training time like using hyperparameter schedules and different optimizers for faster convergence, but we’ll only talk about improving GPU utilization here.</p><h2 id=reasons-for-low-gpu-utilization >Reasons for low GPU utilization</h2><p>The main cause of low GPU utilization is that the next batch of data is not available after a training step and the GPU has to wait. This means that in order to get full GPU utilization,</p><ul><li><p>loading a batch must not take longer than a training step; and</p></li><li><p>the data must be loaded in the background, so that it is ready the moment the GPU needs it.</p></li></ul><p>These issues can be addressed by</p><ul><li><p>using worker threads to load multiple batches in parallel</p></li><li><p>keeping the primary thread free; and</p></li><li><p>reducing the time it takes to load a single batch</p></li></ul><p>FastAI.jl by default uses <code>DataLoader</code> from the <a href= title= >DataLoaders.jl</a> package which addresses points 1. and 2. For those familiar with PyTorch, it closely resembles <code>torch.utils.data.DataLoader</code>. It also efficiently collates the data by reusing a buffer where supported.</p><p>We can measure the large performance difference by comparing a naive sequential data iterator with <code>eachobsparallel</code>, the data iterator that <code>DataLoader</code> uses.</p><pre lang=julia ><code>using DataLoaders: batchviewcollated
using FastAI
using FastAI.Datasets

data = loadtaskdata(datasetpath(&quot;imagenette2-320&quot;), ImageClassificationTask)
method = ImageClassification(Datasets.getclassesclassification(&quot;imagenette2-320&quot;), (224, 224))

# maps data processing over `data`
methoddata = methoddataset(data, method, Training())

# creates a data container of collated batches
batchdata = batchviewcollated(methoddata, 16)

NBATCHES = 200

# sequential data iterator
@time for (i, batch) in enumerate(getobs(batchdata, i) for i in 1:nobs(batchdata))
    i != NBATCHES || break
end

# parallel data iterator
@time for (i, batch) in enumerate(eachobsparallel(batchdata))
    i != NBATCHES || break
end
</code></pre><p>Running each timer twice to forego compilation time, the sequential iterator takes 20 seconds while the parallel iterator using 11 background threads only takes 2.5 seconds. This certainly isn’t a proper benchmark, but it shows the performance can be improved by an order of magnitude with no effort.</p><p>Beside increasing the amount of compute available with worker threads as above, the data loading performance can also be improved by reducing the time it takes to load a single batch. Since a batch is made up of some number of observations, this usually boils down to reducing the loading time of a single observation. If you’re using the <code>LearningMethod</code> API, this can be further broken down into the loading and encoding part.</p><h2 id=measuring-performance >Measuring performance</h2><p>So how do you know if your GPU is underutilized? If it isn’t, then improving data pipeline performance won’t help you at all! One way to check this is to start training and run <code>&gt; watch -n 0.1 nvidia-smi</code> in a terminal which displays and refreshs GPU stats every 1/10th of a second. If <code>GPU-Util</code> stays between 90% and 99%, you’re good!</p><p>If that’s not the case, you might see it frantically jumping up and down. We can get a better estimate of how much training time can be sped up by running the following experiment:</p><ul><li><p>Load one batch and run <code>n</code> optimization steps on this batch. The time this takes corresponds to the training time when the GPU does not have to wait for data to be available.</p></li><li><p>Next take your data iterator and time iterating over the first <code>n</code> batches <em>without</em> an optimization step.</p></li></ul><p>The speed of the complete training loop (data loading and optimization) will be around the maximum of either measurement. Roughly speaking, if 1. takes 100 seconds and 2. takes 200 seconds, you know that you can speed up training by about a factor of 2 if you reduce data loading time by half, after which the GPU will become the bottleneck.</p><pre lang=julia ><code>using FastAI
using FastAI.Datasets
using FluxTraining: fitbatchphase!

data = loadtaskdata(datasetpath(&quot;imagenette2-320&quot;), ImageClassificationTask)
method = ImageClassification(Datasets.getclassesclassification(&quot;imagenette2-320&quot;), (224, 224))

learner = methodlearner(method, data, xresnet18())

NBATCHES = 100

# Measure GPU time
batch = gpu(first(learner.data.training))
learner.model = gpu(model)
@time for i in 1:NBATCHES
    fitbatchphase!(learner, batch, TrainingPhase())
end

# Measure data loading time
@time for (i, batch) in zip(learner.data.training, 1:NBATCHES)
end
</code></pre><p>Again, make sure to run each measurement twice so you don’t include the compilation time.</p><hr></hr><p>To find performance bottlenecks in the loading of each observation, you’ll want to compare the time it takes to load an observation of the task data container and the time it takes to encode that observation.</p><pre lang=julia ><code>using BenchmarkTools
using FastAI
using FastAI.Datasets

# Since loading times can vary per observation, we'll average the measurements over multiple observations
N = 10
data = datasubset(shuffleobs(loadtaskdata(datasetpath(&quot;imagenette2&quot;), ImageClassificationTask), 1:N))
method = ImageClassification(Datasets.getclassesclassification(&quot;imagenette2-320&quot;), (224, 224))

# Time it takes to load an `(image, class)` observation
@btime for i in 1:N
    getobs(data, i)
end


# Time it takes to encode an `(image, class)` observation into `(x, y)`
obss = [getobs(data, i) for i in 1:N]
@btime for i in 1:N
    encode(method, Training(), obss[i])
end
</code></pre><p>This will give you a pretty good idea of where the performance bottleneck is. Note that the encoding performance is often dependent of the method configuration. If we used <code>ImageClassification</code> with input size <code>(64, 64)</code> it would be much faster.</p><h2 id=improving-performance >Improving performance</h2><p>So, you’ve identified the data pipeline as a performance bottleneck. What now? Before anything else, make sure you’re doing the following:</p><ul><li><p>Use <code>DataLoaders.DataLoader</code> as a data iterator. If you’re using <a href=../../REFERENCE/DLPipelines.methoddataloaders.html ><code>DLPipelines.methoddataloaders</code></a> or <a href=../../REFERENCE/FastAI.methodlearner.html ><code>methodlearner</code></a>, this is already the case.</p></li><li><p>Start Julia with multiple threads by specifying the <code>-t n</code>/<code>-t auto</code> flag when starting Julia. If it is successful, <code>Threads.nthreads()</code> should be larger than <code>1</code>.</p></li></ul><p>If the data loading is still slowing down training, you’ll probably have to speed up the loading of each observation. As mentioned above, this can be broken down into observation loading and encoding. The exact strategy will depend on your use case, but here are some examples.</p><ul><li><p>Reduce loading time of image datasets by presizing</p><p>For many computer vision tasks, you will resize and crop images to a specific size during training for GPU performance reasons. If the images themselves are large, loading them from disk itself can take some time. If your dataset consists of 1920x1080 resolution images but you’re resizing them to 256x256 during training, you’re wasting a lot of time loading the large images. <em>Presizing</em> means saving resized versions of each image to disk once, and then loading these smaller versions during training. We can see the performance difference using ImageNette since it comes in 3 sizes: original, 360px and 180px.</p><pre lang=julia ><code>data_orig = loadtaskdata(datasetpath(&quot;imagenette2&quot;), ImageClassificationTask)
@time eachobsparallel(data_orig, buffered = false)

data_320px = loadtaskdata(datasetpath(&quot;imagenette2-320&quot;), ImageClassificationTask)
@time eachobsparallel(data_320px, buffered = false)

data_160px = loadtaskdata(datasetpath(&quot;imagenette2-160&quot;), ImageClassificationTask)
@time eachobsparallel(data_160px, buffered = false)
</code></pre></li><li><p>Reducing allocations with inplace operations</p><p>When implementing the <code>LearningMethod</code> interface, you have the option to implement <code>encode!(buf, method, context, sample)</code>, an inplace version of <code>encode</code> that reuses a buffer to avoid allocations. Reducing allocations often speeds up the encoding step and can also reduce the frequency of garbage collector pauses during training which can reduce GPU utilization.</p></li><li><p>Using efficient data augmentation</p><p>Many kinds of augmentation can be composed efficiently. A prime example of this are image transformations like resizing, scaling and cropping which are powered by <a href=https://github.com/lorenzoh/DataAugmentation.jl title= >DataAugmentation.jl</a>. See <a href=https://lorenzoh.github.io/DataAugmentation.jl/dev/docs/literate/intro.html title= >its documentation</a> to find out how to implement efficient, composable data transformations.</p></li></ul></article><footer class=book-footer ></footer></div><aside class=book-toc ><nav id=toc class=book-toc-content ><ul><li><a href=#performant-data-pipelines >Performant data pipelines</a><ul><li><a href=#reasons-for-low-gpu-utilization >Reasons for low GPU utilization</a><ul></ul></li><li><a href=#measuring-performance >Measuring performance</a><ul></ul></li><li><a href=#improving-performance >Improving performance</a><ul></ul></li></ul></li></ul></nav></aside></main></body></HTML>