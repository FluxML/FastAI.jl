{"attributes":{"kind":"struct","backlinks":[{"tag":"sourcefile","title":"Flux/deprecations.jl","docid":"Flux@0.13.9/src/deprecations.jl"},{"tag":"sourcefile","title":"Flux/outputsize.jl","docid":"Flux@0.13.9/src/outputsize.jl"},{"tag":"sourcefile","title":"Flux/layers/show.jl","docid":"Flux@0.13.9/src/layers/show.jl"},{"tag":"sourcefile","title":"Flux/Flux.jl","docid":"Flux@0.13.9/src/Flux.jl"},{"tag":"sourcefile","title":"Flux/layers/basic.jl","docid":"Flux@0.13.9/src/layers/basic.jl"}],"methods":[{"symbol_id":"Flux.Embedding","module_id":"Flux","file":"deprecations.jl","line":67,"signature":"(::Signature)"},{"symbol_id":"Flux.Embedding","module_id":"Flux","file":"layers/basic.jl","line":694,"signature":"(::Signature)"},{"symbol_id":"Flux.Embedding","module_id":"Flux","file":"layers/basic.jl","line":689,"signature":"(::Signature)"}],"package_id":"Flux@0.13.9","title":"Embedding","symbol_id":"Flux.Embedding","exported":true,"module_id":"Flux"},"tag":"documentation","children":[{"attributes":{"symbol":"Flux.Embedding","line":649,"module":"Flux","file":"layers/basic.jl"},"tag":"docstring","children":[{"attributes":{},"tag":"md","children":[{"attributes":{"lang":""},"tag":"codeblock","children":["Embedding(in => out; init=randn32)\n"],"type":"node"},{"attributes":{},"tag":"p","children":["A lookup table that stores embeddings of dimension ",{"attributes":{},"tag":"code","children":["out"],"type":"node"}," for a vocabulary of size ",{"attributes":{},"tag":"code","children":["in"],"type":"node"},", as a trainable matrix."],"type":"node"},{"attributes":{},"tag":"p","children":["This layer is often used to store word embeddings and retrieve them using indices. The input to the layer can be a vocabulary index in ",{"attributes":{},"tag":"code","children":["1:in"],"type":"node"},", an array of indices, or the corresponding [",{"attributes":{},"tag":"code","children":["onehot encoding"],"type":"node"},"](",{"attributes":{},"tag":"citation","children":[],"type":"node"}," OneHotArrays.onehotbatch)."],"type":"node"},{"attributes":{},"tag":"p","children":["For indices ",{"attributes":{},"tag":"code","children":["x"],"type":"node"},", the result is of size ",{"attributes":{},"tag":"code","children":["(out, size(x)...)"],"type":"node"},", allowing several batch dimensions. For one-hot ",{"attributes":{},"tag":"code","children":["ohx"],"type":"node"},", the result is of size ",{"attributes":{},"tag":"code","children":["(out, size(ohx)[2:end]...)"],"type":"node"},"."],"type":"node"},{"attributes":{},"tag":"h1","children":["Examples"],"type":"node"},{"attributes":{"lang":"jldoctest"},"tag":"codeblock","children":["julia> emb = Embedding(26 => 4, init=Flux.identity_init(gain=22))\nEmbedding(26 => 4)  # 104 parameters\n\njulia> emb(2)  # one column of e.weight (here not random!)\n4-element Vector{Float32}:\n  0.0\n 22.0\n  0.0\n  0.0\n\njulia> emb([3, 1, 20, 14, 4, 15, 7])  # vocabulary indices, in 1:26\n4Ã—7 Matrix{Float32}:\n  0.0  22.0  0.0  0.0   0.0  0.0  0.0\n  0.0   0.0  0.0  0.0   0.0  0.0  0.0\n 22.0   0.0  0.0  0.0   0.0  0.0  0.0\n  0.0   0.0  0.0  0.0  22.0  0.0  0.0\n\njulia> ans == emb(Flux.onehotbatch(\"cat&dog\", 'a':'z', 'n'))\ntrue\n\njulia> emb(rand(1:26, (10, 1, 12))) |> size  # three batch dimensions\n(4, 10, 1, 12)\n"],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}