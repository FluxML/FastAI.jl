{"attributes":{"kind":"function","backlinks":[{"tag":"sourcefile","title":"Flux/Flux.jl","docid":"Flux@0.13.9/src/Flux.jl"},{"tag":"sourcefile","title":"Flux/layers/recurrent.jl","docid":"Flux@0.13.9/src/layers/recurrent.jl"}],"methods":[{"symbol_id":"Flux.GRUv3","module_id":"Flux","file":"layers/recurrent.jl","line":498,"signature":"(::Signature)"}],"package_id":"Flux@0.13.9","title":"GRUv3","symbol_id":"Flux.GRUv3","exported":true,"module_id":"Flux"},"tag":"documentation","children":[{"attributes":{"symbol":"Flux.GRUv3","line":461,"module":"Flux","file":"layers/recurrent.jl"},"tag":"docstring","children":[{"attributes":{},"tag":"md","children":[{"attributes":{"lang":""},"tag":"codeblock","children":["GRUv3(in => out)\n"],"type":"node"},{"attributes":{},"tag":"p","children":[{"attributes":{"href":"https://arxiv.org/abs/1406.1078v3","title":""},"tag":"a","children":["Gated Recurrent Unit"],"type":"node"}," layer. Behaves like an RNN but generally exhibits a longer memory span over sequences. This implements the variant proposed in v3 of the referenced paper."],"type":"node"},{"attributes":{},"tag":"p","children":["The arguments ",{"attributes":{},"tag":"code","children":["in"],"type":"node"}," and ",{"attributes":{},"tag":"code","children":["out"],"type":"node"}," describe the size of the feature vectors passed as input and as output. That is, it accepts a vector of length ",{"attributes":{},"tag":"code","children":["in"],"type":"node"}," or a batch of vectors represented as a ",{"attributes":{},"tag":"code","children":["in x B"],"type":"node"}," matrix and outputs a vector of length ",{"attributes":{},"tag":"code","children":["out"],"type":"node"}," or a batch of vectors of size ",{"attributes":{},"tag":"code","children":["out x B"],"type":"node"},"."],"type":"node"},{"attributes":{},"tag":"p","children":["This constructor is syntactic sugar for ",{"attributes":{},"tag":"code","children":["Recur(GRUv3Cell(a...))"],"type":"node"},", and so GRUv3s are stateful. Note that the state shape can change depending on the inputs, and so it is good to ",{"attributes":{},"tag":"code","children":["reset!"],"type":"node"}," the model between inference calls if the batch size changes. See the examples below."],"type":"node"},{"attributes":{},"tag":"p","children":["See ",{"attributes":{"href":"https://colah.github.io/posts/2015-08-Understanding-LSTMs/","title":""},"tag":"a","children":["this article"],"type":"node"}," for a good overview of the internals."],"type":"node"},{"attributes":{},"tag":"h1","children":["Examples"],"type":"node"},{"attributes":{"lang":"jldoctest"},"tag":"codeblock","children":["julia> g = GRUv3(3 => 5)\nRecur(\n  GRUv3Cell(3 => 5),                    # 140 parameters\n)         # Total: 5 trainable arrays, 140 parameters,\n          # plus 1 non-trainable, 5 parameters, summarysize 848 bytes.\n\njulia> g(rand(Float32, 3)) |> size\n(5,)\n\njulia> Flux.reset!(g);\n\njulia> g(rand(Float32, 3, 10)) |> size # batch size of 10\n(5, 10)\n"],"type":"node"},{"attributes":{"class":"warning"},"tag":"admonition","children":[{"attributes":{},"tag":"admonitiontitle","children":["Batch size changes"],"type":"node"},{"attributes":{},"tag":"admonitionbody","children":[{"attributes":{},"tag":"p","children":["Failing to call ",{"attributes":{},"tag":"code","children":["reset!"],"type":"node"}," when the input batch size changes can lead to unexpected behavior. See the example in ",{"attributes":{"reftype":"symbol","href":"@ref","title":"","document_id":"Flux@0.13.9/ref/Flux.RNN"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["RNN"],"type":"node"}],"type":"node"},"."],"type":"node"}],"type":"node"}],"type":"node"},{"attributes":{},"tag":"h1","children":["Note:"],"type":"node"},{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"code","children":["GRUv3Cell"],"type":"node"},"s can be constructed directly by specifying the non-linear function, the ",{"attributes":{},"tag":"code","children":["Wi"],"type":"node"},", ",{"attributes":{},"tag":"code","children":["Wh"],"type":"node"},", and ",{"attributes":{},"tag":"code","children":["Wh_h"],"type":"node"}," internal matrices, a bias vector ",{"attributes":{},"tag":"code","children":["b"],"type":"node"},", and a learnable initial state ",{"attributes":{},"tag":"code","children":["state0"],"type":"node"},". The  ",{"attributes":{},"tag":"code","children":["Wi"],"type":"node"},", ",{"attributes":{},"tag":"code","children":["Wh"],"type":"node"},", and ",{"attributes":{},"tag":"code","children":["Wh_h"],"type":"node"}," matrices do not need to be the same type. See the example in ",{"attributes":{"reftype":"symbol","href":"@ref","title":"","document_id":"Flux@0.13.9/ref/Flux.RNN"},"tag":"reference","children":[{"attributes":{},"tag":"code","children":["RNN"],"type":"node"}],"type":"node"},"."],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}