{"attributes":{"kind":"struct","backlinks":[{"tag":"sourcefile","title":"Flux/optimise/optimisers.jl","docid":"Flux@0.13.9/src/optimise/optimisers.jl"},{"tag":"sourcefile","title":"Flux/Flux.jl","docid":"Flux@0.13.9/src/Flux.jl"},{"tag":"sourcefile","title":"Flux/optimise/Optimise.jl","docid":"Flux@0.13.9/src/optimise/Optimise.jl"}],"methods":[{"symbol_id":"Flux.Optimise.ExpDecay","module_id":"Flux.Optimise","file":"optimise/optimisers.jl","line":660,"signature":"(::Signature)"},{"symbol_id":"Flux.Optimise.ExpDecay","module_id":"Flux.Optimise","file":"optimise/optimisers.jl","line":652,"signature":"(::Signature)"}],"package_id":"Flux@0.13.9","title":"ExpDecay","symbol_id":"Flux.Optimise.ExpDecay","exported":true,"module_id":"Flux.Optimise"},"tag":"documentation","children":[{"attributes":{"symbol":"Flux.Optimise.ExpDecay","line":622,"module":"Flux.Optimise","file":"optimise/optimisers.jl"},"tag":"docstring","children":[{"attributes":{},"tag":"md","children":[{"attributes":{"lang":""},"tag":"codeblock","children":["ExpDecay(η = 0.001, decay = 0.1, decay_step = 1000, clip = 1e-4, start = 1)\n"],"type":"node"},{"attributes":{},"tag":"p","children":["Discount the learning rate ",{"attributes":{},"tag":"code","children":["η"],"type":"node"}," by the factor ",{"attributes":{},"tag":"code","children":["decay"],"type":"node"}," every ",{"attributes":{},"tag":"code","children":["decay_step"],"type":"node"}," steps till a minimum of ",{"attributes":{},"tag":"code","children":["clip"],"type":"node"},"."],"type":"node"},{"attributes":{},"tag":"h1","children":["Parameters"],"type":"node"},{"attributes":{},"tag":"ul","children":[{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":["Learning rate (",{"attributes":{},"tag":"code","children":["η"],"type":"node"},"): Amount by which gradients are discounted before updating the weights."],"type":"node"}],"type":"node"},{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"code","children":["decay"],"type":"node"},": Factor by which the learning rate is discounted."],"type":"node"}],"type":"node"},{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"code","children":["decay_step"],"type":"node"},": Schedule decay operations by setting the number of steps between two decay operations."],"type":"node"}],"type":"node"},{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"code","children":["clip"],"type":"node"},": Minimum value of learning rate."],"type":"node"}],"type":"node"},{"attributes":{},"tag":"li","children":[{"attributes":{},"tag":"p","children":["'start': Step at which the decay starts."],"type":"node"}],"type":"node"}],"type":"node"},{"attributes":{},"tag":"p","children":["See also the ",{"attributes":{"href":"@ref","title":"","document_id":"@ref"},"tag":"reference","children":["Scheduling Optimisers"],"type":"node"}," section of the docs for more general scheduling techniques."],"type":"node"},{"attributes":{},"tag":"h1","children":["Examples"],"type":"node"},{"attributes":{},"tag":"p","children":[{"attributes":{},"tag":"code","children":["ExpDecay"],"type":"node"}," is typically composed  with other optimizers as the last transformation of the gradient:"],"type":"node"},{"attributes":{"lang":"julia"},"tag":"codeblock","children":[{"attributes":{},"tag":"julia","children":[{"attributes":{},"tag":"=","children":[{"attributes":{},"tag":"Identifier","children":["opt"],"type":"node"},{"attributes":{},"tag":"Whitespace","children":[" "],"type":"node"},{"attributes":{},"tag":"=","children":["="],"type":"node"},{"attributes":{},"tag":"Whitespace","children":[" "],"type":"node"},{"attributes":{},"tag":"call","children":[{"attributes":{"reftype":"symbol","document_id":"Flux@0.13.9/ref/Flux.Optimise.Optimiser"},"tag":"reference","children":["Optimiser"],"type":"node"},{"attributes":{},"tag":"(","children":["("],"type":"node"},{"attributes":{},"tag":"call","children":[{"attributes":{"reftype":"symbol","document_id":"Flux@0.13.9/ref/Flux.Optimise.Adam"},"tag":"reference","children":["Adam"],"type":"node"},{"attributes":{},"tag":"(","children":["("],"type":"node"},{"attributes":{},"tag":")","children":[")"],"type":"node"}],"type":"node"},{"attributes":{},"tag":",","children":[","],"type":"node"},{"attributes":{},"tag":"Whitespace","children":[" "],"type":"node"},{"attributes":{},"tag":"call","children":[{"attributes":{"reftype":"symbol","document_id":"Flux@0.13.9/ref/Flux.Optimise.ExpDecay"},"tag":"reference","children":["ExpDecay"],"type":"node"},{"attributes":{},"tag":"(","children":["("],"type":"node"},{"attributes":{},"tag":"Float","children":["1.0"],"type":"node"},{"attributes":{},"tag":")","children":[")"],"type":"node"}],"type":"node"},{"attributes":{},"tag":")","children":[")"],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"},{"attributes":{},"tag":"p","children":["Note: you may want to start with ",{"attributes":{},"tag":"code","children":["η=1"],"type":"node"}," in ",{"attributes":{},"tag":"code","children":["ExpDecay"],"type":"node"}," when combined with other optimizers (",{"attributes":{},"tag":"code","children":["Adam"],"type":"node"}," in this case) that have their own learning rate."],"type":"node"}],"type":"node"}],"type":"node"}],"type":"node"}